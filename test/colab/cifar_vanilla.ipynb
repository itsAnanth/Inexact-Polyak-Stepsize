{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ABgpXQB24Ok",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b5fec1b7-15d5-460c-f657-ed1e9b468bd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from sklearn.metrics import accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.models as models\n",
        "from torchvision.models import resnet18, ResNet18_Weights\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqXnzFmu24Op"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "class IPS(optim.Optimizer):\n",
        "    def __init__(self, model_params, T, lower_bound):\n",
        "\n",
        "        defaults = dict(T=T, lower_bound=lower_bound)\n",
        "\n",
        "        super(IPS, self).__init__(model_params, defaults)\n",
        "\n",
        "        self.best_loss = float('inf')\n",
        "        self.best_params = None\n",
        "\n",
        "\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "\n",
        "\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                rtloss = closure()\n",
        "                loss = rtloss.item()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            T = group['T']\n",
        "            l_star = group['lower_bound']\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "\n",
        "                grad = param.grad.data\n",
        "                grad_norm_sq = torch.sum(grad ** 2) + 1e-8\n",
        "\n",
        "                inexact_step_size = (loss - l_star) / (grad_norm_sq * (T ** 0.5))\n",
        "\n",
        "                # important, since we're using incremental stepsize without clamping gradient explodes\n",
        "                # especially true in earlier steps\n",
        "                inexact_step_size = torch.clamp(inexact_step_size, min=0.0, max=1.0)\n",
        "                param.data.add_(grad, alpha=-inexact_step_size)\n",
        "\n",
        "        # if loss < self.best_loss:\n",
        "        #     self.best_loss = loss\n",
        "        #     self.best_params = [p.clone().detach() for p in self.param_groups[0]['params']]\n",
        "\n",
        "        return rtloss\n",
        "\n",
        "    def load_best_params(self):\n",
        "        if self.best_params:\n",
        "            for param, best_param in zip(self.param_groups[0]['params'], self.best_params):\n",
        "                param.data.copy_(best_param)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JW3HYij124Oq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.optim as optim\n",
        "\n",
        "\"\"\"\n",
        "    logic\n",
        "\n",
        "    from adam, moment estimates are given by\n",
        "\n",
        "\n",
        "\n",
        "    m_t = beta_1 * m_t-1 + (1 - beta_t) * grad\n",
        "    v_t = beta_2 * v_t-1 + (1 - beta_2) * grad ** 2\n",
        "\n",
        "    with vt maximum tracking,\n",
        "\n",
        "    v_t_max = max(v_t, v_t-1_max)\n",
        "    theta_t = theta_t-1 - (alpha * m_t) / (v_t ** 0.5)\n",
        "\n",
        "\n",
        "    ips = (loss - l*) / (grad_norm_square * (T ** 0.5))\n",
        "    theta_t = theta_t-1 - (ips * grad)\n",
        "\n",
        "    with adam,\n",
        "\n",
        "    TODO: currently, doing layer wise ips, probably should do parameter wise ips. verify it later\n",
        "\n",
        "    ips = (loss - l*) / (grad_norm_square * (T ** 0.5) * (v_t_sum ** 0.5))\n",
        "    theta_t = theta_t-1 - (ips * m_t)\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class AdaIPS_S(optim.Optimizer):\n",
        "    def __init__(self, model_params, lower_bound=0, beta_1=0.9, beta_2=0.999, eps=1e-8, per_param=False):\n",
        "        defaults = dict(lower_bound=lower_bound, beta_1=beta_1, beta_2=beta_2, eps=eps)\n",
        "        super().__init__(model_params, defaults)\n",
        "\n",
        "        print(f\"initialized optimizer with per layer learning rate: {per_param}, no T\")\n",
        "        self.best_loss = float('inf')\n",
        "        self.best_params = None  # List of lists for each param group\n",
        "        self.t = 0\n",
        "        self.per_param = per_param\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        self.t += 1\n",
        "\n",
        "        loss = None\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                loss = closure()\n",
        "                loss_value = loss.item()\n",
        "        else:\n",
        "            raise ValueError(\"AdaIPS requires closure for loss value\")\n",
        "\n",
        "        current_params = []\n",
        "        for group in self.param_groups:\n",
        "            current_params.append([p.clone().detach() for p in group['params']])\n",
        "        if loss_value < self.best_loss:\n",
        "            self.best_loss = loss_value\n",
        "            self.best_params = current_params\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            l_star = group['lower_bound']\n",
        "            beta_1 = group['beta_1']\n",
        "            beta_2 = group['beta_2']\n",
        "            eps = group['eps']\n",
        "\n",
        "            for param in group['params']:\n",
        "                if param.grad is None:\n",
        "                    continue\n",
        "                grad = param.grad\n",
        "\n",
        "                state = self.state[param]\n",
        "\n",
        "                # Initialize state\n",
        "                if len(state) == 0:\n",
        "                    state['m_t'] = torch.zeros_like(param)\n",
        "                    state['v_t'] = torch.zeros_like(param)\n",
        "                    state['v_t_max'] = torch.zeros_like(param)\n",
        "\n",
        "\n",
        "                m_t = state['m_t']\n",
        "                v_t = state['v_t']\n",
        "                v_t_max = state['v_t_max']\n",
        "\n",
        "                # Update moments\n",
        "\n",
        "                # Update first moment estimate (momentum)\n",
        "                # first moment estimate holds information regarding gradient trends\n",
        "                # ex, if gradient is a ball rolling down a hill then mt holds information like velocity, direction etc\n",
        "                m_t.mul_(beta_1).add_(grad, alpha=1 - beta_1)\n",
        "\n",
        "                # Calculate parameter-wise adaptive T_t\n",
        "                # high vt means gradient has been fluctuating, move slowly\n",
        "                # v_t_hat shows gradient variance (low vt means high confidence, travelling in this direction reduces loss and vice versa)\n",
        "                # high variance means unstable region\n",
        "                # inverse relation, so to prevent overshooting, for large gradient variance small steps\n",
        "                # ex, if gradient is a ball rolling down a hill then vt represents terrain difficulty\n",
        "                v_t.mul_(beta_2).addcmul_(grad, grad, value=1 - beta_2)\n",
        "\n",
        "                v_t_max = torch.maximum(v_t_max, v_t)\n",
        "                # bias correction\n",
        "                m_t_hat = m_t / (1 - beta_1 ** self.t)\n",
        "                v_t_hat = v_t_max / (1 - beta_2 ** self.t)\n",
        "\n",
        "                sum_v_t_hat = v_t_hat.sum()\n",
        "\n",
        "                grad_norm_sq = grad.pow(2).sum().clamp(min=eps)\n",
        "                # param_t = (T0 ** 0.5) * (torch.sqrt(v_t_hat) + eps) if self.per_param else (torch.sqrt(sum_v_t_hat) + eps)\n",
        "\n",
        "                param_t = (torch.sqrt(v_t_hat) + eps) if self.per_param else (torch.sqrt(sum_v_t_hat) + eps)\n",
        "                # did sqrt param_t here, optimization becomes faster because smaller denominator, but becomes more unstable so keep it as it is\n",
        "                denominator = grad_norm_sq * param_t\n",
        "                denominator = denominator.clamp(min=eps)\n",
        "\n",
        "                step_size = (loss_value - l_star) / denominator\n",
        "                step_size = torch.clamp(step_size, min=0.0, max=0.1)\n",
        "\n",
        "                # param.data.add_(m_t_hat, alpha=-step_size)\n",
        "                param.data.add_((m_t_hat * -step_size))\n",
        "\n",
        "\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def load_best_params(self):\n",
        "        if self.best_params is not None:\n",
        "            for group, best_group_params in zip(self.param_groups, self.best_params):\n",
        "                for param, best_param in zip(group['params'], best_group_params):\n",
        "                    param.data.copy_(best_param)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Znjk9OQ24Os"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "def getData(dataset: str, transform_train, transform_val, batch_size=64):\n",
        "\n",
        "    if not hasattr(torchvision.datasets, dataset):\n",
        "        raise ValueError(\"dataset does not exist\")\n",
        "\n",
        "    data = getattr(torchvision.datasets, dataset)\n",
        "\n",
        "    # Create datasets without transforms initially\n",
        "    full_trainset = data(root='../data', train=True, download=True, transform=None)\n",
        "    testset = data(root='../data', train=False, download=True, transform=transform_val)\n",
        "\n",
        "    train_size = int(0.8 * len(full_trainset))\n",
        "    val_size = len(full_trainset) - train_size\n",
        "\n",
        "    # Split without transforms\n",
        "    train_subset, val_subset = torch.utils.data.random_split(full_trainset, [train_size, val_size])\n",
        "\n",
        "    # Create dataset wrappers that apply transforms\n",
        "    trainset = TransformDataset(train_subset, transform_train)\n",
        "    valset = TransformDataset(val_subset, transform_val)\n",
        "\n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=batch_size, shuffle=False)\n",
        "    validationloader = torch.utils.data.DataLoader(valset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return trainloader, trainset, testloader, testset, validationloader, valset\n",
        "\n",
        "# Helper class to apply transforms to a subset\n",
        "class TransformDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset, transform=None):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JYCcNN0j24Os"
      },
      "outputs": [],
      "source": [
        "# Device setup\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "class CIFAR10_CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CIFAR10_CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
        "        self.fc2 = nn.Linear(512, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "        self.t_losses = []\n",
        "        self.v_losses = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class PretrainedResNet18(nn.Module):\n",
        "    def __init__(self, num_classes=10):\n",
        "        super(PretrainedResNet18, self).__init__()\n",
        "        # Load pretrained ResNet18\n",
        "        self.model = resnet18(weights=None)\n",
        "\n",
        "        # Replace the first conv layer to handle CIFAR-10's 32x32 images\n",
        "        # Original ResNet has 7x7 conv with stride 2 for ImageNet's larger images\n",
        "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "\n",
        "        # Remove the max pooling layer (not needed for smaller images)\n",
        "        self.model.maxpool = nn.Identity()\n",
        "\n",
        "        # Replace the final fully connected layer\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Linear(in_features, num_classes)\n",
        "\n",
        "        self.t_losses = []\n",
        "        self.v_losses = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ResNetBlockWithDropout(nn.Module):\n",
        "    \"\"\"Custom BasicBlock with Dropout\"\"\"\n",
        "    def __init__(self, original_block, dropout_rate=0.2):\n",
        "        super().__init__()\n",
        "        self.conv1 = original_block.conv1\n",
        "        self.bn1 = original_block.bn1\n",
        "        self.relu = original_block.relu\n",
        "        self.conv2 = original_block.conv2\n",
        "        self.bn2 = original_block.bn2\n",
        "        self.downsample = original_block.downsample\n",
        "\n",
        "        # Add dropout\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        # Apply dropout after activation\n",
        "        out = self.dropout(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)\n",
        "\n",
        "        out += identity\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNetWithDropout(nn.Module):\n",
        "    def __init__(self, num_classes=10, dropout_rate=0.3):\n",
        "        super(ResNetWithDropout, self).__init__()\n",
        "        self.model = resnet18(weights=None)  # Or use `weights=models.ResNet18_Weights.DEFAULT`\n",
        "\n",
        "        # Modify the first conv layer to fit CIFAR-10 (optional)\n",
        "        self.model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.model.maxpool = nn.Identity()  # Remove max pooling for small images\n",
        "\n",
        "        # Replace each BasicBlock with ResNetBlockWithDropout\n",
        "        for name, module in self.model.named_children():\n",
        "            if isinstance(module, nn.Sequential):  # Residual blocks are in nn.Sequential\n",
        "                for block_idx, block in enumerate(module):\n",
        "                    if isinstance(block, models.resnet.BasicBlock):\n",
        "                        module[block_idx] = ResNetBlockWithDropout(block, dropout_rate)\n",
        "\n",
        "        # Add dropout before final fully connected layer\n",
        "        in_features = self.model.fc.in_features\n",
        "        self.model.fc = nn.Sequential(\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.Linear(in_features, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5bgd7Ug724Ou"
      },
      "outputs": [],
      "source": [
        "epochs = 100\n",
        "batch_size = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UCaMrnyd24Ou",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "1b4711cb-5801-4cba-9447-21e024a12461"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:03<00:00, 43.7MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "40000"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomCrop(32, padding=4),  # Randomly crop with padding\n",
        "    transforms.RandomHorizontalFlip(),     # Randomly flip horizontally\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2),  # Adjust color properties\n",
        "    transforms.ToTensor(),                 # Convert to tensor\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))  # Normalize with CIFAR-10 mean and std\n",
        "])\n",
        "\n",
        "# Define transforms for validation/testing (no augmentation)\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "])\n",
        "trainloader, trainset, testloader, testset, validationloader, valset = getData('CIFAR10', transform_train=transform_train, transform_val=transform_test, batch_size=batch_size)\n",
        "\n",
        "display(len(trainset), len(testset), len(valset))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(model, optimizer, epoch, loss, accuracy, filename, t_losses, v_losses):\n",
        "    checkpoint = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'loss': loss,\n",
        "        'accuracy': accuracy,\n",
        "        't_losses': t_losses,\n",
        "        'v_losses': v_losses\n",
        "    }\n",
        "    torch.save(checkpoint, filename)\n",
        "    print(f\"Checkpoint saved: {filename}\")"
      ],
      "metadata": {
        "id": "EfHEd1UrO0es"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NSOENtq_24Ov"
      },
      "outputs": [],
      "source": [
        "def train(model, optimizer, epochs=2):\n",
        "    print(optimizer)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    t_losses = []\n",
        "    v_losses = []\n",
        "    num_epochs = epochs\n",
        "    for epoch in range(num_epochs):\n",
        "\n",
        "        model.train()\n",
        "        running_loss = 0\n",
        "        for i, (inputs, labels) in enumerate(trainloader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            def closure():\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                loss.backward()\n",
        "                return loss\n",
        "\n",
        "            loss = optimizer.step(closure)\n",
        "\n",
        "            print(f'Epoch {epoch+1}, Step {i}, Loss: {loss}')\n",
        "            running_loss += loss.item()\n",
        "\n",
        "            del inputs, labels\n",
        "\n",
        "        t_losses.append(running_loss / len(trainloader))\n",
        "        print(f\"Epoch {epoch+1} end, avg train loss: {running_loss / len(trainloader)}\")\n",
        "\n",
        "        model.eval()  # Set model to evaluation mode\n",
        "        val_running_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():  # No need to track gradients during validation\n",
        "            for v_i, (inputs, labels) in enumerate(validationloader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                val_running_loss += loss.item()\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                del inputs, labels, outputs\n",
        "\n",
        "        # Calculate and store validation loss and accuracy\n",
        "        val_loss = val_running_loss / len(validationloader)\n",
        "        v_losses.append(val_loss)\n",
        "        accuracy = 100 * correct / total\n",
        "\n",
        "        print(f\"Epoch {epoch+1} end, avg val loss: {val_loss}, accuracy: {accuracy:.2f}%\")\n",
        "\n",
        "        if (epoch % 10 == 0 and epoch != 0):\n",
        "          save_checkpoint(model, optimizer, epoch+1, loss=val_loss, accuracy=accuracy, filename=f\"checkpoint_{epoch+1}.pth\", t_losses=t_losses, v_losses=v_losses)\n",
        "\n",
        "\n",
        "    model = model.cpu()\n",
        "    model.t_losses = t_losses\n",
        "    model.v_losses = v_losses\n",
        "    torch.cuda.empty_cache()\n",
        "    print(f\"GPU memory allocated: {torch.cuda.memory_allocated() / 1e9:.2f} GB\")\n",
        "    print(f\"GPU memory reserved: {torch.cuda.memory_reserved() / 1e9:.2f} GB\")\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Training complete.\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz9yTMhz24Ov"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "def test(model, testloader):\n",
        "    if testloader is None:\n",
        "        raise ValueError(\"testloader must be provided\")\n",
        "\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Initialize lists to store all predictions and labels\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    # Iterate through batches\n",
        "    for i, (inputs, labels) in enumerate(testloader):\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "        with torch.no_grad():\n",
        "            output = model(inputs)\n",
        "            predictions = torch.argmax(output, dim=1)\n",
        "\n",
        "        # Store batch results\n",
        "        all_predictions.append(predictions.cpu().numpy())\n",
        "        all_labels.append(labels.cpu().numpy())\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_predictions = np.concatenate(all_predictions)\n",
        "    all_labels = np.concatenate(all_labels)\n",
        "\n",
        "    # Calculate metrics on the entire dataset\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    precision, recall, fscore, support = precision_recall_fscore_support(all_labels, all_predictions, average='weighted')\n",
        "\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall: {recall:.4f}\")\n",
        "    print(f\"F1-score: {fscore:.4f}\")\n",
        "    model.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p2TuqNr624Ow",
        "outputId": "054be9e2-9b17-4f40-b02a-7d7423925c25"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 85, Step 44, Loss: 0.16460232436656952\n",
            "Epoch 85, Step 45, Loss: 0.07996023446321487\n",
            "Epoch 85, Step 46, Loss: 0.10982980579137802\n",
            "Epoch 85, Step 47, Loss: 0.12633109092712402\n",
            "Epoch 85, Step 48, Loss: 0.08943552523851395\n",
            "Epoch 85, Step 49, Loss: 0.1318071484565735\n",
            "Epoch 85, Step 50, Loss: 0.07942172884941101\n",
            "Epoch 85, Step 51, Loss: 0.08066412806510925\n",
            "Epoch 85, Step 52, Loss: 0.09670336544513702\n",
            "Epoch 85, Step 53, Loss: 0.1755588948726654\n",
            "Epoch 85, Step 54, Loss: 0.14021101593971252\n",
            "Epoch 85, Step 55, Loss: 0.13673946261405945\n",
            "Epoch 85, Step 56, Loss: 0.15827934443950653\n",
            "Epoch 85, Step 57, Loss: 0.098063625395298\n",
            "Epoch 85, Step 58, Loss: 0.08576233685016632\n",
            "Epoch 85, Step 59, Loss: 0.1453215479850769\n",
            "Epoch 85, Step 60, Loss: 0.14641694724559784\n",
            "Epoch 85, Step 61, Loss: 0.0531480573117733\n",
            "Epoch 85, Step 62, Loss: 0.15515093505382538\n",
            "Epoch 85, Step 63, Loss: 0.061765097081661224\n",
            "Epoch 85, Step 64, Loss: 0.22243496775627136\n",
            "Epoch 85, Step 65, Loss: 0.18530575931072235\n",
            "Epoch 85, Step 66, Loss: 0.10234832763671875\n",
            "Epoch 85, Step 67, Loss: 0.18388471007347107\n",
            "Epoch 85, Step 68, Loss: 0.15621857345104218\n",
            "Epoch 85, Step 69, Loss: 0.15174297988414764\n",
            "Epoch 85, Step 70, Loss: 0.09394523501396179\n",
            "Epoch 85, Step 71, Loss: 0.11062715947628021\n",
            "Epoch 85, Step 72, Loss: 0.154667928814888\n",
            "Epoch 85, Step 73, Loss: 0.12522238492965698\n",
            "Epoch 85, Step 74, Loss: 0.13941940665245056\n",
            "Epoch 85, Step 75, Loss: 0.14652231335639954\n",
            "Epoch 85, Step 76, Loss: 0.13735279440879822\n",
            "Epoch 85, Step 77, Loss: 0.13745644688606262\n",
            "Epoch 85, Step 78, Loss: 0.11589303612709045\n",
            "Epoch 85, Step 79, Loss: 0.12999248504638672\n",
            "Epoch 85, Step 80, Loss: 0.13028056919574738\n",
            "Epoch 85, Step 81, Loss: 0.11562147736549377\n",
            "Epoch 85, Step 82, Loss: 0.12738588452339172\n",
            "Epoch 85, Step 83, Loss: 0.12988874316215515\n",
            "Epoch 85, Step 84, Loss: 0.13831692934036255\n",
            "Epoch 85, Step 85, Loss: 0.3051454424858093\n",
            "Epoch 85, Step 86, Loss: 0.15364237129688263\n",
            "Epoch 85, Step 87, Loss: 0.15117321908473969\n",
            "Epoch 85, Step 88, Loss: 0.12481935322284698\n",
            "Epoch 85, Step 89, Loss: 0.0730564221739769\n",
            "Epoch 85, Step 90, Loss: 0.11513488739728928\n",
            "Epoch 85, Step 91, Loss: 0.0712374597787857\n",
            "Epoch 85, Step 92, Loss: 0.10255493223667145\n",
            "Epoch 85, Step 93, Loss: 0.13077802956104279\n",
            "Epoch 85, Step 94, Loss: 0.11278653889894485\n",
            "Epoch 85, Step 95, Loss: 0.1683967411518097\n",
            "Epoch 85, Step 96, Loss: 0.1750815510749817\n",
            "Epoch 85, Step 97, Loss: 0.2039545178413391\n",
            "Epoch 85, Step 98, Loss: 0.16016587615013123\n",
            "Epoch 85, Step 99, Loss: 0.08633477240800858\n",
            "Epoch 85, Step 100, Loss: 0.12599651515483856\n",
            "Epoch 85, Step 101, Loss: 0.1466844081878662\n",
            "Epoch 85, Step 102, Loss: 0.17106829583644867\n",
            "Epoch 85, Step 103, Loss: 0.10406177490949631\n",
            "Epoch 85, Step 104, Loss: 0.11792910844087601\n",
            "Epoch 85, Step 105, Loss: 0.18370972573757172\n",
            "Epoch 85, Step 106, Loss: 0.059927601367235184\n",
            "Epoch 85, Step 107, Loss: 0.11258560419082642\n",
            "Epoch 85, Step 108, Loss: 0.10397104173898697\n",
            "Epoch 85, Step 109, Loss: 0.09348137676715851\n",
            "Epoch 85, Step 110, Loss: 0.1340983361005783\n",
            "Epoch 85, Step 111, Loss: 0.11019166558980942\n",
            "Epoch 85, Step 112, Loss: 0.060266610234975815\n",
            "Epoch 85, Step 113, Loss: 0.07602663338184357\n",
            "Epoch 85, Step 114, Loss: 0.14694082736968994\n",
            "Epoch 85, Step 115, Loss: 0.0932496041059494\n",
            "Epoch 85, Step 116, Loss: 0.14875298738479614\n",
            "Epoch 85, Step 117, Loss: 0.07294918596744537\n",
            "Epoch 85, Step 118, Loss: 0.15156275033950806\n",
            "Epoch 85, Step 119, Loss: 0.15204781293869019\n",
            "Epoch 85, Step 120, Loss: 0.11447840183973312\n",
            "Epoch 85, Step 121, Loss: 0.2121979296207428\n",
            "Epoch 85, Step 122, Loss: 0.07434513419866562\n",
            "Epoch 85, Step 123, Loss: 0.13093994557857513\n",
            "Epoch 85, Step 124, Loss: 0.10982221364974976\n",
            "Epoch 85, Step 125, Loss: 0.10499197989702225\n",
            "Epoch 85, Step 126, Loss: 0.1261359304189682\n",
            "Epoch 85, Step 127, Loss: 0.10578108578920364\n",
            "Epoch 85, Step 128, Loss: 0.19433000683784485\n",
            "Epoch 85, Step 129, Loss: 0.1343526989221573\n",
            "Epoch 85, Step 130, Loss: 0.07014274597167969\n",
            "Epoch 85, Step 131, Loss: 0.1347256600856781\n",
            "Epoch 85, Step 132, Loss: 0.09917735308408737\n",
            "Epoch 85, Step 133, Loss: 0.11399451643228531\n",
            "Epoch 85, Step 134, Loss: 0.23131918907165527\n",
            "Epoch 85, Step 135, Loss: 0.14078187942504883\n",
            "Epoch 85, Step 136, Loss: 0.09543879330158234\n",
            "Epoch 85, Step 137, Loss: 0.11445558071136475\n",
            "Epoch 85, Step 138, Loss: 0.20640462636947632\n",
            "Epoch 85, Step 139, Loss: 0.17412717640399933\n",
            "Epoch 85, Step 140, Loss: 0.19099295139312744\n",
            "Epoch 85, Step 141, Loss: 0.0756988376379013\n",
            "Epoch 85, Step 142, Loss: 0.1579730212688446\n",
            "Epoch 85, Step 143, Loss: 0.11686953902244568\n",
            "Epoch 85, Step 144, Loss: 0.11824145913124084\n",
            "Epoch 85, Step 145, Loss: 0.08697902411222458\n",
            "Epoch 85, Step 146, Loss: 0.11650539934635162\n",
            "Epoch 85, Step 147, Loss: 0.18579529225826263\n",
            "Epoch 85, Step 148, Loss: 0.046462297439575195\n",
            "Epoch 85, Step 149, Loss: 0.20845633745193481\n",
            "Epoch 85, Step 150, Loss: 0.1361180543899536\n",
            "Epoch 85, Step 151, Loss: 0.10270701348781586\n",
            "Epoch 85, Step 152, Loss: 0.07998661696910858\n",
            "Epoch 85, Step 153, Loss: 0.1727708876132965\n",
            "Epoch 85, Step 154, Loss: 0.133119136095047\n",
            "Epoch 85, Step 155, Loss: 0.20774699747562408\n",
            "Epoch 85, Step 156, Loss: 0.10635915398597717\n",
            "Epoch 85, Step 157, Loss: 0.10447894781827927\n",
            "Epoch 85, Step 158, Loss: 0.16201238334178925\n",
            "Epoch 85, Step 159, Loss: 0.11674398928880692\n",
            "Epoch 85, Step 160, Loss: 0.09786511212587357\n",
            "Epoch 85, Step 161, Loss: 0.053906455636024475\n",
            "Epoch 85, Step 162, Loss: 0.12321104109287262\n",
            "Epoch 85, Step 163, Loss: 0.11083664000034332\n",
            "Epoch 85, Step 164, Loss: 0.15158545970916748\n",
            "Epoch 85, Step 165, Loss: 0.21944774687290192\n",
            "Epoch 85, Step 166, Loss: 0.06578316539525986\n",
            "Epoch 85, Step 167, Loss: 0.09549141675233841\n",
            "Epoch 85, Step 168, Loss: 0.06884091347455978\n",
            "Epoch 85, Step 169, Loss: 0.10425059497356415\n",
            "Epoch 85, Step 170, Loss: 0.10958831012248993\n",
            "Epoch 85, Step 171, Loss: 0.08638843894004822\n",
            "Epoch 85, Step 172, Loss: 0.15005804598331451\n",
            "Epoch 85, Step 173, Loss: 0.12751740217208862\n",
            "Epoch 85, Step 174, Loss: 0.07067107409238815\n",
            "Epoch 85, Step 175, Loss: 0.1385033279657364\n",
            "Epoch 85, Step 176, Loss: 0.1019107773900032\n",
            "Epoch 85, Step 177, Loss: 0.14930330216884613\n",
            "Epoch 85, Step 178, Loss: 0.12715989351272583\n",
            "Epoch 85, Step 179, Loss: 0.14919736981391907\n",
            "Epoch 85, Step 180, Loss: 0.07782185077667236\n",
            "Epoch 85, Step 181, Loss: 0.22698290646076202\n",
            "Epoch 85, Step 182, Loss: 0.14927123486995697\n",
            "Epoch 85, Step 183, Loss: 0.08200734853744507\n",
            "Epoch 85, Step 184, Loss: 0.09255608916282654\n",
            "Epoch 85, Step 185, Loss: 0.1305805742740631\n",
            "Epoch 85, Step 186, Loss: 0.15144164860248566\n",
            "Epoch 85, Step 187, Loss: 0.12295416742563248\n",
            "Epoch 85, Step 188, Loss: 0.15379968285560608\n",
            "Epoch 85, Step 189, Loss: 0.07526236027479172\n",
            "Epoch 85, Step 190, Loss: 0.16087254881858826\n",
            "Epoch 85, Step 191, Loss: 0.16702179610729218\n",
            "Epoch 85, Step 192, Loss: 0.0636182650923729\n",
            "Epoch 85, Step 193, Loss: 0.19952590763568878\n",
            "Epoch 85, Step 194, Loss: 0.180868461728096\n",
            "Epoch 85, Step 195, Loss: 0.09629692137241364\n",
            "Epoch 85, Step 196, Loss: 0.15209917724132538\n",
            "Epoch 85, Step 197, Loss: 0.1361141800880432\n",
            "Epoch 85, Step 198, Loss: 0.08354175090789795\n",
            "Epoch 85, Step 199, Loss: 0.15247605741024017\n",
            "Epoch 85, Step 200, Loss: 0.2240617573261261\n",
            "Epoch 85, Step 201, Loss: 0.1548018455505371\n",
            "Epoch 85, Step 202, Loss: 0.09405017644166946\n",
            "Epoch 85, Step 203, Loss: 0.13673469424247742\n",
            "Epoch 85, Step 204, Loss: 0.12881067395210266\n",
            "Epoch 85, Step 205, Loss: 0.12954704463481903\n",
            "Epoch 85, Step 206, Loss: 0.14625965058803558\n",
            "Epoch 85, Step 207, Loss: 0.06534485518932343\n",
            "Epoch 85, Step 208, Loss: 0.15771903097629547\n",
            "Epoch 85, Step 209, Loss: 0.13566365838050842\n",
            "Epoch 85, Step 210, Loss: 0.09500783681869507\n",
            "Epoch 85, Step 211, Loss: 0.10350577533245087\n",
            "Epoch 85, Step 212, Loss: 0.11599944531917572\n",
            "Epoch 85, Step 213, Loss: 0.09921630471944809\n",
            "Epoch 85, Step 214, Loss: 0.1541631519794464\n",
            "Epoch 85, Step 215, Loss: 0.12245011329650879\n",
            "Epoch 85, Step 216, Loss: 0.1905062049627304\n",
            "Epoch 85, Step 217, Loss: 0.09350663423538208\n",
            "Epoch 85, Step 218, Loss: 0.13842256367206573\n",
            "Epoch 85, Step 219, Loss: 0.14352349936962128\n",
            "Epoch 85, Step 220, Loss: 0.1391211748123169\n",
            "Epoch 85, Step 221, Loss: 0.2320365160703659\n",
            "Epoch 85, Step 222, Loss: 0.0950671061873436\n",
            "Epoch 85, Step 223, Loss: 0.15857049822807312\n",
            "Epoch 85, Step 224, Loss: 0.09306187927722931\n",
            "Epoch 85, Step 225, Loss: 0.22160176932811737\n",
            "Epoch 85, Step 226, Loss: 0.09796062111854553\n",
            "Epoch 85, Step 227, Loss: 0.23713254928588867\n",
            "Epoch 85, Step 228, Loss: 0.07788950204849243\n",
            "Epoch 85, Step 229, Loss: 0.13633029162883759\n",
            "Epoch 85, Step 230, Loss: 0.10763233155012131\n",
            "Epoch 85, Step 231, Loss: 0.09941045939922333\n",
            "Epoch 85, Step 232, Loss: 0.059047259390354156\n",
            "Epoch 85, Step 233, Loss: 0.1102348044514656\n",
            "Epoch 85, Step 234, Loss: 0.11888950318098068\n",
            "Epoch 85, Step 235, Loss: 0.13199937343597412\n",
            "Epoch 85, Step 236, Loss: 0.1561354100704193\n",
            "Epoch 85, Step 237, Loss: 0.08943159878253937\n",
            "Epoch 85, Step 238, Loss: 0.1657744199037552\n",
            "Epoch 85, Step 239, Loss: 0.07677140086889267\n",
            "Epoch 85, Step 240, Loss: 0.08403509110212326\n",
            "Epoch 85, Step 241, Loss: 0.09255351126194\n",
            "Epoch 85, Step 242, Loss: 0.16230380535125732\n",
            "Epoch 85, Step 243, Loss: 0.08879027515649796\n",
            "Epoch 85, Step 244, Loss: 0.1317257434129715\n",
            "Epoch 85, Step 245, Loss: 0.11494356393814087\n",
            "Epoch 85, Step 246, Loss: 0.1087144985795021\n",
            "Epoch 85, Step 247, Loss: 0.21462325751781464\n",
            "Epoch 85, Step 248, Loss: 0.10772476345300674\n",
            "Epoch 85, Step 249, Loss: 0.16805003583431244\n",
            "Epoch 85, Step 250, Loss: 0.06896945834159851\n",
            "Epoch 85, Step 251, Loss: 0.11413519829511642\n",
            "Epoch 85, Step 252, Loss: 0.06439265608787537\n",
            "Epoch 85, Step 253, Loss: 0.04739737883210182\n",
            "Epoch 85, Step 254, Loss: 0.1518172025680542\n",
            "Epoch 85, Step 255, Loss: 0.11198695003986359\n",
            "Epoch 85, Step 256, Loss: 0.17457090318202972\n",
            "Epoch 85, Step 257, Loss: 0.07659810781478882\n",
            "Epoch 85, Step 258, Loss: 0.0999520868062973\n",
            "Epoch 85, Step 259, Loss: 0.17699605226516724\n",
            "Epoch 85, Step 260, Loss: 0.1600423902273178\n",
            "Epoch 85, Step 261, Loss: 0.0789368748664856\n",
            "Epoch 85, Step 262, Loss: 0.13602009415626526\n",
            "Epoch 85, Step 263, Loss: 0.09766809642314911\n",
            "Epoch 85, Step 264, Loss: 0.13613352179527283\n",
            "Epoch 85, Step 265, Loss: 0.16246400773525238\n",
            "Epoch 85, Step 266, Loss: 0.1061851978302002\n",
            "Epoch 85, Step 267, Loss: 0.059783998876810074\n",
            "Epoch 85, Step 268, Loss: 0.09502454102039337\n",
            "Epoch 85, Step 269, Loss: 0.09424268454313278\n",
            "Epoch 85, Step 270, Loss: 0.14611957967281342\n",
            "Epoch 85, Step 271, Loss: 0.15021847188472748\n",
            "Epoch 85, Step 272, Loss: 0.15817897021770477\n",
            "Epoch 85, Step 273, Loss: 0.0749860480427742\n",
            "Epoch 85, Step 274, Loss: 0.08955683559179306\n",
            "Epoch 85, Step 275, Loss: 0.0996086373925209\n",
            "Epoch 85, Step 276, Loss: 0.11454103887081146\n",
            "Epoch 85, Step 277, Loss: 0.06092584878206253\n",
            "Epoch 85, Step 278, Loss: 0.08606305718421936\n",
            "Epoch 85, Step 279, Loss: 0.14540737867355347\n",
            "Epoch 85, Step 280, Loss: 0.17888642847537994\n",
            "Epoch 85, Step 281, Loss: 0.1453004777431488\n",
            "Epoch 85, Step 282, Loss: 0.1822933554649353\n",
            "Epoch 85, Step 283, Loss: 0.18321746587753296\n",
            "Epoch 85, Step 284, Loss: 0.1780063658952713\n",
            "Epoch 85, Step 285, Loss: 0.0706404373049736\n",
            "Epoch 85, Step 286, Loss: 0.06351131945848465\n",
            "Epoch 85, Step 287, Loss: 0.045942146331071854\n",
            "Epoch 85, Step 288, Loss: 0.10817456245422363\n",
            "Epoch 85, Step 289, Loss: 0.06285707652568817\n",
            "Epoch 85, Step 290, Loss: 0.12065286189317703\n",
            "Epoch 85, Step 291, Loss: 0.10807470977306366\n",
            "Epoch 85, Step 292, Loss: 0.07681916654109955\n",
            "Epoch 85, Step 293, Loss: 0.10123707354068756\n",
            "Epoch 85, Step 294, Loss: 0.15170156955718994\n",
            "Epoch 85, Step 295, Loss: 0.07808523625135422\n",
            "Epoch 85, Step 296, Loss: 0.10479889065027237\n",
            "Epoch 85, Step 297, Loss: 0.07154615223407745\n",
            "Epoch 85, Step 298, Loss: 0.1819411665201187\n",
            "Epoch 85, Step 299, Loss: 0.13256341218948364\n",
            "Epoch 85, Step 300, Loss: 0.10669180005788803\n",
            "Epoch 85, Step 301, Loss: 0.09144052118062973\n",
            "Epoch 85, Step 302, Loss: 0.09054296463727951\n",
            "Epoch 85, Step 303, Loss: 0.06988382339477539\n",
            "Epoch 85, Step 304, Loss: 0.12402983009815216\n",
            "Epoch 85, Step 305, Loss: 0.1363459974527359\n",
            "Epoch 85, Step 306, Loss: 0.12079048156738281\n",
            "Epoch 85, Step 307, Loss: 0.15177452564239502\n",
            "Epoch 85, Step 308, Loss: 0.1043463870882988\n",
            "Epoch 85, Step 309, Loss: 0.13115671277046204\n",
            "Epoch 85, Step 310, Loss: 0.08280859142541885\n",
            "Epoch 85, Step 311, Loss: 0.17805704474449158\n",
            "Epoch 85, Step 312, Loss: 0.23324106633663177\n",
            "Epoch 85 end, avg train loss: 0.12388503781189553\n",
            "Epoch 85 end, avg val loss: 0.3688389455805285, accuracy: 90.47%\n",
            "Epoch 86, Step 0, Loss: 0.13503549993038177\n",
            "Epoch 86, Step 1, Loss: 0.11989858001470566\n",
            "Epoch 86, Step 2, Loss: 0.0967920795083046\n",
            "Epoch 86, Step 3, Loss: 0.18276390433311462\n",
            "Epoch 86, Step 4, Loss: 0.11314301192760468\n",
            "Epoch 86, Step 5, Loss: 0.14425502717494965\n",
            "Epoch 86, Step 6, Loss: 0.08316945284605026\n",
            "Epoch 86, Step 7, Loss: 0.15473638474941254\n",
            "Epoch 86, Step 8, Loss: 0.10471431165933609\n",
            "Epoch 86, Step 9, Loss: 0.09703272581100464\n",
            "Epoch 86, Step 10, Loss: 0.05226767808198929\n",
            "Epoch 86, Step 11, Loss: 0.10751545429229736\n",
            "Epoch 86, Step 12, Loss: 0.07342363893985748\n",
            "Epoch 86, Step 13, Loss: 0.08024350553750992\n",
            "Epoch 86, Step 14, Loss: 0.14771482348442078\n",
            "Epoch 86, Step 15, Loss: 0.2023157924413681\n",
            "Epoch 86, Step 16, Loss: 0.10509194433689117\n",
            "Epoch 86, Step 17, Loss: 0.05576131120324135\n",
            "Epoch 86, Step 18, Loss: 0.07041162997484207\n",
            "Epoch 86, Step 19, Loss: 0.08006277680397034\n",
            "Epoch 86, Step 20, Loss: 0.14025799930095673\n",
            "Epoch 86, Step 21, Loss: 0.16130255162715912\n",
            "Epoch 86, Step 22, Loss: 0.11431650817394257\n",
            "Epoch 86, Step 23, Loss: 0.04928974434733391\n",
            "Epoch 86, Step 24, Loss: 0.13493011891841888\n",
            "Epoch 86, Step 25, Loss: 0.09729117900133133\n",
            "Epoch 86, Step 26, Loss: 0.16096614301204681\n",
            "Epoch 86, Step 27, Loss: 0.06472326070070267\n",
            "Epoch 86, Step 28, Loss: 0.2235185205936432\n",
            "Epoch 86, Step 29, Loss: 0.07224708050489426\n",
            "Epoch 86, Step 30, Loss: 0.10419472306966782\n",
            "Epoch 86, Step 31, Loss: 0.09334578365087509\n",
            "Epoch 86, Step 32, Loss: 0.07815045118331909\n",
            "Epoch 86, Step 33, Loss: 0.06375634670257568\n",
            "Epoch 86, Step 34, Loss: 0.08054431527853012\n",
            "Epoch 86, Step 35, Loss: 0.051302723586559296\n",
            "Epoch 86, Step 36, Loss: 0.09576274454593658\n",
            "Epoch 86, Step 37, Loss: 0.17044734954833984\n",
            "Epoch 86, Step 38, Loss: 0.12139234691858292\n",
            "Epoch 86, Step 39, Loss: 0.12192907184362411\n",
            "Epoch 86, Step 40, Loss: 0.10986043512821198\n",
            "Epoch 86, Step 41, Loss: 0.0633547455072403\n",
            "Epoch 86, Step 42, Loss: 0.14723502099514008\n",
            "Epoch 86, Step 43, Loss: 0.11019701510667801\n",
            "Epoch 86, Step 44, Loss: 0.12998263537883759\n",
            "Epoch 86, Step 45, Loss: 0.21048499643802643\n",
            "Epoch 86, Step 46, Loss: 0.10043597221374512\n",
            "Epoch 86, Step 47, Loss: 0.09362499415874481\n",
            "Epoch 86, Step 48, Loss: 0.11851970851421356\n",
            "Epoch 86, Step 49, Loss: 0.08983852714300156\n",
            "Epoch 86, Step 50, Loss: 0.10516185313463211\n",
            "Epoch 86, Step 51, Loss: 0.102003313601017\n",
            "Epoch 86, Step 52, Loss: 0.08741072565317154\n",
            "Epoch 86, Step 53, Loss: 0.14588500559329987\n",
            "Epoch 86, Step 54, Loss: 0.12472711503505707\n",
            "Epoch 86, Step 55, Loss: 0.09605889767408371\n",
            "Epoch 86, Step 56, Loss: 0.15172454714775085\n",
            "Epoch 86, Step 57, Loss: 0.14504696428775787\n",
            "Epoch 86, Step 58, Loss: 0.10617588460445404\n",
            "Epoch 86, Step 59, Loss: 0.08207110315561295\n",
            "Epoch 86, Step 60, Loss: 0.11341740936040878\n",
            "Epoch 86, Step 61, Loss: 0.10478468984365463\n",
            "Epoch 86, Step 62, Loss: 0.13265301287174225\n",
            "Epoch 86, Step 63, Loss: 0.09586088359355927\n",
            "Epoch 86, Step 64, Loss: 0.07535644620656967\n",
            "Epoch 86, Step 65, Loss: 0.07967153191566467\n",
            "Epoch 86, Step 66, Loss: 0.1163630485534668\n",
            "Epoch 86, Step 67, Loss: 0.15211966633796692\n",
            "Epoch 86, Step 68, Loss: 0.14556202292442322\n",
            "Epoch 86, Step 69, Loss: 0.1794363409280777\n",
            "Epoch 86, Step 70, Loss: 0.16748027503490448\n",
            "Epoch 86, Step 71, Loss: 0.07585196197032928\n",
            "Epoch 86, Step 72, Loss: 0.06604664772748947\n",
            "Epoch 86, Step 73, Loss: 0.13358072936534882\n",
            "Epoch 86, Step 74, Loss: 0.1710532009601593\n",
            "Epoch 86, Step 75, Loss: 0.13493473827838898\n",
            "Epoch 86, Step 76, Loss: 0.09650547802448273\n",
            "Epoch 86, Step 77, Loss: 0.08859329670667648\n",
            "Epoch 86, Step 78, Loss: 0.04003322124481201\n",
            "Epoch 86, Step 79, Loss: 0.15198102593421936\n",
            "Epoch 86, Step 80, Loss: 0.12444005906581879\n",
            "Epoch 86, Step 81, Loss: 0.15025220811367035\n",
            "Epoch 86, Step 82, Loss: 0.0809069275856018\n",
            "Epoch 86, Step 83, Loss: 0.1169157400727272\n",
            "Epoch 86, Step 84, Loss: 0.14916622638702393\n",
            "Epoch 86, Step 85, Loss: 0.1660139560699463\n",
            "Epoch 86, Step 86, Loss: 0.20582924783229828\n",
            "Epoch 86, Step 87, Loss: 0.11265520751476288\n",
            "Epoch 86, Step 88, Loss: 0.1636994481086731\n",
            "Epoch 86, Step 89, Loss: 0.1483231782913208\n",
            "Epoch 86, Step 90, Loss: 0.08708224445581436\n",
            "Epoch 86, Step 91, Loss: 0.0950208306312561\n",
            "Epoch 86, Step 92, Loss: 0.0687674731016159\n",
            "Epoch 86, Step 93, Loss: 0.08005720376968384\n",
            "Epoch 86, Step 94, Loss: 0.11657381802797318\n",
            "Epoch 86, Step 95, Loss: 0.10767275094985962\n",
            "Epoch 86, Step 96, Loss: 0.052337199449539185\n",
            "Epoch 86, Step 97, Loss: 0.08739560097455978\n",
            "Epoch 86, Step 98, Loss: 0.08294764906167984\n",
            "Epoch 86, Step 99, Loss: 0.1374260038137436\n",
            "Epoch 86, Step 100, Loss: 0.0871560201048851\n",
            "Epoch 86, Step 101, Loss: 0.12092949450016022\n",
            "Epoch 86, Step 102, Loss: 0.07520889490842819\n",
            "Epoch 86, Step 103, Loss: 0.03668913245201111\n",
            "Epoch 86, Step 104, Loss: 0.164832204580307\n",
            "Epoch 86, Step 105, Loss: 0.19563084840774536\n",
            "Epoch 86, Step 106, Loss: 0.12255207449197769\n",
            "Epoch 86, Step 107, Loss: 0.12417087703943253\n",
            "Epoch 86, Step 108, Loss: 0.16818585991859436\n",
            "Epoch 86, Step 109, Loss: 0.10196372866630554\n",
            "Epoch 86, Step 110, Loss: 0.09381453692913055\n",
            "Epoch 86, Step 111, Loss: 0.1882438212633133\n",
            "Epoch 86, Step 112, Loss: 0.13583171367645264\n",
            "Epoch 86, Step 113, Loss: 0.08913156390190125\n",
            "Epoch 86, Step 114, Loss: 0.1061435341835022\n",
            "Epoch 86, Step 115, Loss: 0.10207786411046982\n",
            "Epoch 86, Step 116, Loss: 0.10907835513353348\n",
            "Epoch 86, Step 117, Loss: 0.09781569987535477\n",
            "Epoch 86, Step 118, Loss: 0.13080234825611115\n",
            "Epoch 86, Step 119, Loss: 0.07173625379800797\n",
            "Epoch 86, Step 120, Loss: 0.080213263630867\n",
            "Epoch 86, Step 121, Loss: 0.06631670147180557\n",
            "Epoch 86, Step 122, Loss: 0.06780461966991425\n",
            "Epoch 86, Step 123, Loss: 0.11921506375074387\n",
            "Epoch 86, Step 124, Loss: 0.14924901723861694\n",
            "Epoch 86, Step 125, Loss: 0.1254463493824005\n",
            "Epoch 86, Step 126, Loss: 0.08642934262752533\n",
            "Epoch 86, Step 127, Loss: 0.11933034658432007\n",
            "Epoch 86, Step 128, Loss: 0.08589951694011688\n",
            "Epoch 86, Step 129, Loss: 0.16919030249118805\n",
            "Epoch 86, Step 130, Loss: 0.19290341436862946\n",
            "Epoch 86, Step 131, Loss: 0.1171271950006485\n",
            "Epoch 86, Step 132, Loss: 0.14039292931556702\n",
            "Epoch 86, Step 133, Loss: 0.08746396005153656\n",
            "Epoch 86, Step 134, Loss: 0.11887019127607346\n",
            "Epoch 86, Step 135, Loss: 0.12858140468597412\n",
            "Epoch 86, Step 136, Loss: 0.2025856375694275\n",
            "Epoch 86, Step 137, Loss: 0.13423103094100952\n",
            "Epoch 86, Step 138, Loss: 0.15341950953006744\n",
            "Epoch 86, Step 139, Loss: 0.10906893759965897\n",
            "Epoch 86, Step 140, Loss: 0.0858028382062912\n",
            "Epoch 86, Step 141, Loss: 0.15137030184268951\n",
            "Epoch 86, Step 142, Loss: 0.1607433706521988\n",
            "Epoch 86, Step 143, Loss: 0.1537041962146759\n",
            "Epoch 86, Step 144, Loss: 0.1004306748509407\n",
            "Epoch 86, Step 145, Loss: 0.12682293355464935\n",
            "Epoch 86, Step 146, Loss: 0.12078621238470078\n",
            "Epoch 86, Step 147, Loss: 0.12255309522151947\n",
            "Epoch 86, Step 148, Loss: 0.16799652576446533\n",
            "Epoch 86, Step 149, Loss: 0.12120550870895386\n",
            "Epoch 86, Step 150, Loss: 0.15721385180950165\n",
            "Epoch 86, Step 151, Loss: 0.15056979656219482\n",
            "Epoch 86, Step 152, Loss: 0.09770243614912033\n",
            "Epoch 86, Step 153, Loss: 0.11385490000247955\n",
            "Epoch 86, Step 154, Loss: 0.12387582659721375\n",
            "Epoch 86, Step 155, Loss: 0.07952761650085449\n",
            "Epoch 86, Step 156, Loss: 0.14949139952659607\n",
            "Epoch 86, Step 157, Loss: 0.14228560030460358\n",
            "Epoch 86, Step 158, Loss: 0.042615365236997604\n",
            "Epoch 86, Step 159, Loss: 0.2085058093070984\n",
            "Epoch 86, Step 160, Loss: 0.18753904104232788\n",
            "Epoch 86, Step 161, Loss: 0.07997535914182663\n",
            "Epoch 86, Step 162, Loss: 0.09763913601636887\n",
            "Epoch 86, Step 163, Loss: 0.060240138322114944\n",
            "Epoch 86, Step 164, Loss: 0.07301604747772217\n",
            "Epoch 86, Step 165, Loss: 0.08302485942840576\n",
            "Epoch 86, Step 166, Loss: 0.08024480938911438\n",
            "Epoch 86, Step 167, Loss: 0.15009579062461853\n",
            "Epoch 86, Step 168, Loss: 0.08009686321020126\n",
            "Epoch 86, Step 169, Loss: 0.09465184062719345\n",
            "Epoch 86, Step 170, Loss: 0.10735289007425308\n",
            "Epoch 86, Step 171, Loss: 0.0915834978222847\n",
            "Epoch 86, Step 172, Loss: 0.17324575781822205\n",
            "Epoch 86, Step 173, Loss: 0.09235262870788574\n",
            "Epoch 86, Step 174, Loss: 0.06706596165895462\n",
            "Epoch 86, Step 175, Loss: 0.09017486870288849\n",
            "Epoch 86, Step 176, Loss: 0.10469958931207657\n",
            "Epoch 86, Step 177, Loss: 0.05804980546236038\n",
            "Epoch 86, Step 178, Loss: 0.10820070654153824\n",
            "Epoch 86, Step 179, Loss: 0.14852482080459595\n",
            "Epoch 86, Step 180, Loss: 0.10833138227462769\n",
            "Epoch 86, Step 181, Loss: 0.09664511680603027\n",
            "Epoch 86, Step 182, Loss: 0.07652929425239563\n",
            "Epoch 86, Step 183, Loss: 0.07202202826738358\n",
            "Epoch 86, Step 184, Loss: 0.105869360268116\n",
            "Epoch 86, Step 185, Loss: 0.1524915099143982\n",
            "Epoch 86, Step 186, Loss: 0.07412508130073547\n",
            "Epoch 86, Step 187, Loss: 0.06665534526109695\n",
            "Epoch 86, Step 188, Loss: 0.06792260706424713\n",
            "Epoch 86, Step 189, Loss: 0.09484906494617462\n",
            "Epoch 86, Step 190, Loss: 0.05123911798000336\n",
            "Epoch 86, Step 191, Loss: 0.19786040484905243\n",
            "Epoch 86, Step 192, Loss: 0.06074511632323265\n",
            "Epoch 86, Step 193, Loss: 0.07492695748806\n",
            "Epoch 86, Step 194, Loss: 0.12383492290973663\n",
            "Epoch 86, Step 195, Loss: 0.08894619345664978\n",
            "Epoch 86, Step 196, Loss: 0.18963320553302765\n",
            "Epoch 86, Step 197, Loss: 0.09181811660528183\n",
            "Epoch 86, Step 198, Loss: 0.09604482352733612\n",
            "Epoch 86, Step 199, Loss: 0.09506102651357651\n",
            "Epoch 86, Step 200, Loss: 0.09277048707008362\n",
            "Epoch 86, Step 201, Loss: 0.05836675316095352\n",
            "Epoch 86, Step 202, Loss: 0.06087719649076462\n",
            "Epoch 86, Step 203, Loss: 0.05532266944646835\n",
            "Epoch 86, Step 204, Loss: 0.14615498483181\n",
            "Epoch 86, Step 205, Loss: 0.07130390405654907\n",
            "Epoch 86, Step 206, Loss: 0.11182819306850433\n",
            "Epoch 86, Step 207, Loss: 0.20070408284664154\n",
            "Epoch 86, Step 208, Loss: 0.047491732984781265\n",
            "Epoch 86, Step 209, Loss: 0.10120704025030136\n",
            "Epoch 86, Step 210, Loss: 0.05915311723947525\n",
            "Epoch 86, Step 211, Loss: 0.12594987452030182\n",
            "Epoch 86, Step 212, Loss: 0.07265068590641022\n",
            "Epoch 86, Step 213, Loss: 0.05704795569181442\n",
            "Epoch 86, Step 214, Loss: 0.12549489736557007\n",
            "Epoch 86, Step 215, Loss: 0.19246791303157806\n",
            "Epoch 86, Step 216, Loss: 0.09869032353162766\n",
            "Epoch 86, Step 217, Loss: 0.1377740502357483\n",
            "Epoch 86, Step 218, Loss: 0.10107842087745667\n",
            "Epoch 86, Step 219, Loss: 0.1367316097021103\n",
            "Epoch 86, Step 220, Loss: 0.07950033247470856\n",
            "Epoch 86, Step 221, Loss: 0.1475709080696106\n",
            "Epoch 86, Step 222, Loss: 0.15621981024742126\n",
            "Epoch 86, Step 223, Loss: 0.0803113505244255\n",
            "Epoch 86, Step 224, Loss: 0.11873975396156311\n",
            "Epoch 86, Step 225, Loss: 0.23464328050613403\n",
            "Epoch 86, Step 226, Loss: 0.06309282034635544\n",
            "Epoch 86, Step 227, Loss: 0.11453008651733398\n",
            "Epoch 86, Step 228, Loss: 0.08269399404525757\n",
            "Epoch 86, Step 229, Loss: 0.09583704173564911\n",
            "Epoch 86, Step 230, Loss: 0.13770462572574615\n",
            "Epoch 86, Step 231, Loss: 0.1544743925333023\n",
            "Epoch 86, Step 232, Loss: 0.18834403157234192\n",
            "Epoch 86, Step 233, Loss: 0.2353460192680359\n",
            "Epoch 86, Step 234, Loss: 0.0830269604921341\n",
            "Epoch 86, Step 235, Loss: 0.12774568796157837\n",
            "Epoch 86, Step 236, Loss: 0.168678879737854\n",
            "Epoch 86, Step 237, Loss: 0.06869713217020035\n",
            "Epoch 86, Step 238, Loss: 0.11671809107065201\n",
            "Epoch 86, Step 239, Loss: 0.06422755122184753\n",
            "Epoch 86, Step 240, Loss: 0.09007897228002548\n",
            "Epoch 86, Step 241, Loss: 0.12198836356401443\n",
            "Epoch 86, Step 242, Loss: 0.15002653002738953\n",
            "Epoch 86, Step 243, Loss: 0.12941548228263855\n",
            "Epoch 86, Step 244, Loss: 0.09032753854990005\n",
            "Epoch 86, Step 245, Loss: 0.07649514824151993\n",
            "Epoch 86, Step 246, Loss: 0.08530611544847488\n",
            "Epoch 86, Step 247, Loss: 0.14699527621269226\n",
            "Epoch 86, Step 248, Loss: 0.12034104019403458\n",
            "Epoch 86, Step 249, Loss: 0.09974271059036255\n",
            "Epoch 86, Step 250, Loss: 0.13680481910705566\n",
            "Epoch 86, Step 251, Loss: 0.06339385360479355\n",
            "Epoch 86, Step 252, Loss: 0.11609393358230591\n",
            "Epoch 86, Step 253, Loss: 0.1355466991662979\n",
            "Epoch 86, Step 254, Loss: 0.12361535429954529\n",
            "Epoch 86, Step 255, Loss: 0.08350424468517303\n",
            "Epoch 86, Step 256, Loss: 0.14234836399555206\n",
            "Epoch 86, Step 257, Loss: 0.1914350688457489\n",
            "Epoch 86, Step 258, Loss: 0.09012366086244583\n",
            "Epoch 86, Step 259, Loss: 0.11377474665641785\n",
            "Epoch 86, Step 260, Loss: 0.10806236416101456\n",
            "Epoch 86, Step 261, Loss: 0.21464498341083527\n",
            "Epoch 86, Step 262, Loss: 0.08329189568758011\n",
            "Epoch 86, Step 263, Loss: 0.12234872579574585\n",
            "Epoch 86, Step 264, Loss: 0.23396234214305878\n",
            "Epoch 86, Step 265, Loss: 0.11295387148857117\n",
            "Epoch 86, Step 266, Loss: 0.16813001036643982\n",
            "Epoch 86, Step 267, Loss: 0.09177396446466446\n",
            "Epoch 86, Step 268, Loss: 0.07821453362703323\n",
            "Epoch 86, Step 269, Loss: 0.14870962500572205\n",
            "Epoch 86, Step 270, Loss: 0.17549031972885132\n",
            "Epoch 86, Step 271, Loss: 0.1745700091123581\n",
            "Epoch 86, Step 272, Loss: 0.08071495592594147\n",
            "Epoch 86, Step 273, Loss: 0.1465981900691986\n",
            "Epoch 86, Step 274, Loss: 0.09983353316783905\n",
            "Epoch 86, Step 275, Loss: 0.15937845408916473\n",
            "Epoch 86, Step 276, Loss: 0.11718577891588211\n",
            "Epoch 86, Step 277, Loss: 0.14637954533100128\n",
            "Epoch 86, Step 278, Loss: 0.14335550367832184\n",
            "Epoch 86, Step 279, Loss: 0.1498933881521225\n",
            "Epoch 86, Step 280, Loss: 0.15413455665111542\n",
            "Epoch 86, Step 281, Loss: 0.20398037135601044\n",
            "Epoch 86, Step 282, Loss: 0.11922105401754379\n",
            "Epoch 86, Step 283, Loss: 0.14564722776412964\n",
            "Epoch 86, Step 284, Loss: 0.0824233740568161\n",
            "Epoch 86, Step 285, Loss: 0.1160813644528389\n",
            "Epoch 86, Step 286, Loss: 0.19226020574569702\n",
            "Epoch 86, Step 287, Loss: 0.11133649945259094\n",
            "Epoch 86, Step 288, Loss: 0.07456175982952118\n",
            "Epoch 86, Step 289, Loss: 0.13504499197006226\n",
            "Epoch 86, Step 290, Loss: 0.076105497777462\n",
            "Epoch 86, Step 291, Loss: 0.19407503306865692\n",
            "Epoch 86, Step 292, Loss: 0.1698887050151825\n",
            "Epoch 86, Step 293, Loss: 0.09557469189167023\n",
            "Epoch 86, Step 294, Loss: 0.16675317287445068\n",
            "Epoch 86, Step 295, Loss: 0.127752423286438\n",
            "Epoch 86, Step 296, Loss: 0.16956394910812378\n",
            "Epoch 86, Step 297, Loss: 0.16911402344703674\n",
            "Epoch 86, Step 298, Loss: 0.10551832616329193\n",
            "Epoch 86, Step 299, Loss: 0.19701899588108063\n",
            "Epoch 86, Step 300, Loss: 0.13071730732917786\n",
            "Epoch 86, Step 301, Loss: 0.0948123186826706\n",
            "Epoch 86, Step 302, Loss: 0.11887016892433167\n",
            "Epoch 86, Step 303, Loss: 0.12590095400810242\n",
            "Epoch 86, Step 304, Loss: 0.12814033031463623\n",
            "Epoch 86, Step 305, Loss: 0.11868275701999664\n",
            "Epoch 86, Step 306, Loss: 0.12912707030773163\n",
            "Epoch 86, Step 307, Loss: 0.09041263163089752\n",
            "Epoch 86, Step 308, Loss: 0.15120643377304077\n",
            "Epoch 86, Step 309, Loss: 0.166282519698143\n",
            "Epoch 86, Step 310, Loss: 0.1720881462097168\n",
            "Epoch 86, Step 311, Loss: 0.046191729605197906\n",
            "Epoch 86, Step 312, Loss: 0.15742848813533783\n",
            "Epoch 86 end, avg train loss: 0.11762504505749327\n",
            "Epoch 86 end, avg val loss: 0.3892243373962237, accuracy: 90.23%\n",
            "Epoch 87, Step 0, Loss: 0.09562962502241135\n",
            "Epoch 87, Step 1, Loss: 0.11262108385562897\n",
            "Epoch 87, Step 2, Loss: 0.0743512213230133\n",
            "Epoch 87, Step 3, Loss: 0.10041692852973938\n",
            "Epoch 87, Step 4, Loss: 0.06612275540828705\n",
            "Epoch 87, Step 5, Loss: 0.10016527026891708\n",
            "Epoch 87, Step 6, Loss: 0.19841301441192627\n",
            "Epoch 87, Step 7, Loss: 0.11037937551736832\n",
            "Epoch 87, Step 8, Loss: 0.14204701781272888\n",
            "Epoch 87, Step 9, Loss: 0.10405807942152023\n",
            "Epoch 87, Step 10, Loss: 0.10137075930833817\n",
            "Epoch 87, Step 11, Loss: 0.10839947313070297\n",
            "Epoch 87, Step 12, Loss: 0.09460671246051788\n",
            "Epoch 87, Step 13, Loss: 0.11481835693120956\n",
            "Epoch 87, Step 14, Loss: 0.26514217257499695\n",
            "Epoch 87, Step 15, Loss: 0.1091323271393776\n",
            "Epoch 87, Step 16, Loss: 0.07508731633424759\n",
            "Epoch 87, Step 17, Loss: 0.14834392070770264\n",
            "Epoch 87, Step 18, Loss: 0.07855603843927383\n",
            "Epoch 87, Step 19, Loss: 0.12355615943670273\n",
            "Epoch 87, Step 20, Loss: 0.11863642930984497\n",
            "Epoch 87, Step 21, Loss: 0.06941472738981247\n",
            "Epoch 87, Step 22, Loss: 0.058447353541851044\n",
            "Epoch 87, Step 23, Loss: 0.08269456773996353\n",
            "Epoch 87, Step 24, Loss: 0.12180831283330917\n",
            "Epoch 87, Step 25, Loss: 0.0831160619854927\n",
            "Epoch 87, Step 26, Loss: 0.0699101984500885\n",
            "Epoch 87, Step 27, Loss: 0.1643480658531189\n",
            "Epoch 87, Step 28, Loss: 0.18502698838710785\n",
            "Epoch 87, Step 29, Loss: 0.06992894411087036\n",
            "Epoch 87, Step 30, Loss: 0.08130583167076111\n",
            "Epoch 87, Step 31, Loss: 0.09173009544610977\n",
            "Epoch 87, Step 32, Loss: 0.09739305078983307\n",
            "Epoch 87, Step 33, Loss: 0.1456846296787262\n",
            "Epoch 87, Step 34, Loss: 0.06241321936249733\n",
            "Epoch 87, Step 35, Loss: 0.12434617429971695\n",
            "Epoch 87, Step 36, Loss: 0.13517560064792633\n",
            "Epoch 87, Step 37, Loss: 0.11018767207860947\n",
            "Epoch 87, Step 38, Loss: 0.17675529420375824\n",
            "Epoch 87, Step 39, Loss: 0.09312570095062256\n",
            "Epoch 87, Step 40, Loss: 0.08672308176755905\n",
            "Epoch 87, Step 41, Loss: 0.07709039747714996\n",
            "Epoch 87, Step 42, Loss: 0.0689704418182373\n",
            "Epoch 87, Step 43, Loss: 0.08685614168643951\n",
            "Epoch 87, Step 44, Loss: 0.05465749278664589\n",
            "Epoch 87, Step 45, Loss: 0.10398580133914948\n",
            "Epoch 87, Step 46, Loss: 0.23389723896980286\n",
            "Epoch 87, Step 47, Loss: 0.12361547350883484\n",
            "Epoch 87, Step 48, Loss: 0.13434429466724396\n",
            "Epoch 87, Step 49, Loss: 0.08375170826911926\n",
            "Epoch 87, Step 50, Loss: 0.14835664629936218\n",
            "Epoch 87, Step 51, Loss: 0.08628840744495392\n",
            "Epoch 87, Step 52, Loss: 0.10431626439094543\n",
            "Epoch 87, Step 53, Loss: 0.09464052319526672\n",
            "Epoch 87, Step 54, Loss: 0.08948143571615219\n",
            "Epoch 87, Step 55, Loss: 0.09907882660627365\n",
            "Epoch 87, Step 56, Loss: 0.08549464493989944\n",
            "Epoch 87, Step 57, Loss: 0.16796813905239105\n",
            "Epoch 87, Step 58, Loss: 0.16997282207012177\n",
            "Epoch 87, Step 59, Loss: 0.08999959379434586\n",
            "Epoch 87, Step 60, Loss: 0.13027097284793854\n",
            "Epoch 87, Step 61, Loss: 0.06355179846286774\n",
            "Epoch 87, Step 62, Loss: 0.10508977621793747\n",
            "Epoch 87, Step 63, Loss: 0.185100719332695\n",
            "Epoch 87, Step 64, Loss: 0.11115299165248871\n",
            "Epoch 87, Step 65, Loss: 0.09751097857952118\n",
            "Epoch 87, Step 66, Loss: 0.07362207770347595\n",
            "Epoch 87, Step 67, Loss: 0.19240564107894897\n",
            "Epoch 87, Step 68, Loss: 0.15129819512367249\n",
            "Epoch 87, Step 69, Loss: 0.10493571311235428\n",
            "Epoch 87, Step 70, Loss: 0.14186863601207733\n",
            "Epoch 87, Step 71, Loss: 0.07241510599851608\n",
            "Epoch 87, Step 72, Loss: 0.06522946804761887\n",
            "Epoch 87, Step 73, Loss: 0.12073313444852829\n",
            "Epoch 87, Step 74, Loss: 0.09458152204751968\n",
            "Epoch 87, Step 75, Loss: 0.07196063548326492\n",
            "Epoch 87, Step 76, Loss: 0.07726183533668518\n",
            "Epoch 87, Step 77, Loss: 0.21197262406349182\n",
            "Epoch 87, Step 78, Loss: 0.11846739053726196\n",
            "Epoch 87, Step 79, Loss: 0.1569926142692566\n",
            "Epoch 87, Step 80, Loss: 0.08575129508972168\n",
            "Epoch 87, Step 81, Loss: 0.1452603042125702\n",
            "Epoch 87, Step 82, Loss: 0.07131166756153107\n",
            "Epoch 87, Step 83, Loss: 0.094245046377182\n",
            "Epoch 87, Step 84, Loss: 0.08612103015184402\n",
            "Epoch 87, Step 85, Loss: 0.14367520809173584\n",
            "Epoch 87, Step 86, Loss: 0.16996124386787415\n",
            "Epoch 87, Step 87, Loss: 0.12477004528045654\n",
            "Epoch 87, Step 88, Loss: 0.08339385688304901\n",
            "Epoch 87, Step 89, Loss: 0.08107534795999527\n",
            "Epoch 87, Step 90, Loss: 0.06655702739953995\n",
            "Epoch 87, Step 91, Loss: 0.1234743744134903\n",
            "Epoch 87, Step 92, Loss: 0.08438855409622192\n",
            "Epoch 87, Step 93, Loss: 0.1543339639902115\n",
            "Epoch 87, Step 94, Loss: 0.07857386022806168\n",
            "Epoch 87, Step 95, Loss: 0.09037230908870697\n",
            "Epoch 87, Step 96, Loss: 0.06832995265722275\n",
            "Epoch 87, Step 97, Loss: 0.1447063833475113\n",
            "Epoch 87, Step 98, Loss: 0.0895782932639122\n",
            "Epoch 87, Step 99, Loss: 0.09929421544075012\n",
            "Epoch 87, Step 100, Loss: 0.07489994913339615\n",
            "Epoch 87, Step 101, Loss: 0.12880918383598328\n",
            "Epoch 87, Step 102, Loss: 0.12612079083919525\n",
            "Epoch 87, Step 103, Loss: 0.15899313986301422\n",
            "Epoch 87, Step 104, Loss: 0.1525791585445404\n",
            "Epoch 87, Step 105, Loss: 0.17245016992092133\n",
            "Epoch 87, Step 106, Loss: 0.1128963977098465\n",
            "Epoch 87, Step 107, Loss: 0.085322804749012\n",
            "Epoch 87, Step 108, Loss: 0.08057505637407303\n",
            "Epoch 87, Step 109, Loss: 0.159312903881073\n",
            "Epoch 87, Step 110, Loss: 0.08812437206506729\n",
            "Epoch 87, Step 111, Loss: 0.05531404912471771\n",
            "Epoch 87, Step 112, Loss: 0.1566726714372635\n",
            "Epoch 87, Step 113, Loss: 0.0802009329199791\n",
            "Epoch 87, Step 114, Loss: 0.1028396263718605\n",
            "Epoch 87, Step 115, Loss: 0.08152008801698685\n",
            "Epoch 87, Step 116, Loss: 0.10259124636650085\n",
            "Epoch 87, Step 117, Loss: 0.08866206556558609\n",
            "Epoch 87, Step 118, Loss: 0.1389666646718979\n",
            "Epoch 87, Step 119, Loss: 0.047502484172582626\n",
            "Epoch 87, Step 120, Loss: 0.05040518194437027\n",
            "Epoch 87, Step 121, Loss: 0.14480124413967133\n",
            "Epoch 87, Step 122, Loss: 0.1634141057729721\n",
            "Epoch 87, Step 123, Loss: 0.09578311443328857\n",
            "Epoch 87, Step 124, Loss: 0.06594180315732956\n",
            "Epoch 87, Step 125, Loss: 0.15343442559242249\n",
            "Epoch 87, Step 126, Loss: 0.09844616800546646\n",
            "Epoch 87, Step 127, Loss: 0.11052726209163666\n",
            "Epoch 87, Step 128, Loss: 0.13083715736865997\n",
            "Epoch 87, Step 129, Loss: 0.16510623693466187\n",
            "Epoch 87, Step 130, Loss: 0.05884633958339691\n",
            "Epoch 87, Step 131, Loss: 0.08663100004196167\n",
            "Epoch 87, Step 132, Loss: 0.046298205852508545\n",
            "Epoch 87, Step 133, Loss: 0.15109169483184814\n",
            "Epoch 87, Step 134, Loss: 0.09306745231151581\n",
            "Epoch 87, Step 135, Loss: 0.13007964193820953\n",
            "Epoch 87, Step 136, Loss: 0.16504333913326263\n",
            "Epoch 87, Step 137, Loss: 0.14366482198238373\n",
            "Epoch 87, Step 138, Loss: 0.0899280309677124\n",
            "Epoch 87, Step 139, Loss: 0.14862041175365448\n",
            "Epoch 87, Step 140, Loss: 0.1139986589550972\n",
            "Epoch 87, Step 141, Loss: 0.09267915785312653\n",
            "Epoch 87, Step 142, Loss: 0.14365458488464355\n",
            "Epoch 87, Step 143, Loss: 0.1978042870759964\n",
            "Epoch 87, Step 144, Loss: 0.07025708258152008\n",
            "Epoch 87, Step 145, Loss: 0.10252738744020462\n",
            "Epoch 87, Step 146, Loss: 0.13317812979221344\n",
            "Epoch 87, Step 147, Loss: 0.08124138414859772\n",
            "Epoch 87, Step 148, Loss: 0.09668385237455368\n",
            "Epoch 87, Step 149, Loss: 0.17776347696781158\n",
            "Epoch 87, Step 150, Loss: 0.1481110155582428\n",
            "Epoch 87, Step 151, Loss: 0.07317967712879181\n",
            "Epoch 87, Step 152, Loss: 0.1489522010087967\n",
            "Epoch 87, Step 153, Loss: 0.11184243857860565\n",
            "Epoch 87, Step 154, Loss: 0.14097358286380768\n",
            "Epoch 87, Step 155, Loss: 0.10666707158088684\n",
            "Epoch 87, Step 156, Loss: 0.10224082320928574\n",
            "Epoch 87, Step 157, Loss: 0.13497310876846313\n",
            "Epoch 87, Step 158, Loss: 0.1812431514263153\n",
            "Epoch 87, Step 159, Loss: 0.08724560588598251\n",
            "Epoch 87, Step 160, Loss: 0.042823854833841324\n",
            "Epoch 87, Step 161, Loss: 0.1629418283700943\n",
            "Epoch 87, Step 162, Loss: 0.09749861061573029\n",
            "Epoch 87, Step 163, Loss: 0.09114374220371246\n",
            "Epoch 87, Step 164, Loss: 0.11231852322816849\n",
            "Epoch 87, Step 165, Loss: 0.03454774245619774\n",
            "Epoch 87, Step 166, Loss: 0.16040734946727753\n",
            "Epoch 87, Step 167, Loss: 0.11734756827354431\n",
            "Epoch 87, Step 168, Loss: 0.07081791013479233\n",
            "Epoch 87, Step 169, Loss: 0.16469822824001312\n",
            "Epoch 87, Step 170, Loss: 0.07327944785356522\n",
            "Epoch 87, Step 171, Loss: 0.08527298271656036\n",
            "Epoch 87, Step 172, Loss: 0.172451913356781\n",
            "Epoch 87, Step 173, Loss: 0.04593765735626221\n",
            "Epoch 87, Step 174, Loss: 0.1065409928560257\n",
            "Epoch 87, Step 175, Loss: 0.13264504075050354\n",
            "Epoch 87, Step 176, Loss: 0.09206109493970871\n",
            "Epoch 87, Step 177, Loss: 0.13798579573631287\n",
            "Epoch 87, Step 178, Loss: 0.1016572117805481\n",
            "Epoch 87, Step 179, Loss: 0.13979177176952362\n",
            "Epoch 87, Step 180, Loss: 0.1632639467716217\n",
            "Epoch 87, Step 181, Loss: 0.08800403028726578\n",
            "Epoch 87, Step 182, Loss: 0.14095640182495117\n",
            "Epoch 87, Step 183, Loss: 0.1131354495882988\n",
            "Epoch 87, Step 184, Loss: 0.11727789044380188\n",
            "Epoch 87, Step 185, Loss: 0.07836967706680298\n",
            "Epoch 87, Step 186, Loss: 0.1543871909379959\n",
            "Epoch 87, Step 187, Loss: 0.16305334866046906\n",
            "Epoch 87, Step 188, Loss: 0.07245852798223495\n",
            "Epoch 87, Step 189, Loss: 0.10880199074745178\n",
            "Epoch 87, Step 190, Loss: 0.06504146754741669\n",
            "Epoch 87, Step 191, Loss: 0.09590597450733185\n",
            "Epoch 87, Step 192, Loss: 0.12139703333377838\n",
            "Epoch 87, Step 193, Loss: 0.12593026459217072\n",
            "Epoch 87, Step 194, Loss: 0.11415202170610428\n",
            "Epoch 87, Step 195, Loss: 0.12169072777032852\n",
            "Epoch 87, Step 196, Loss: 0.07918103784322739\n",
            "Epoch 87, Step 197, Loss: 0.11975637823343277\n",
            "Epoch 87, Step 198, Loss: 0.178053081035614\n",
            "Epoch 87, Step 199, Loss: 0.08237843960523605\n",
            "Epoch 87, Step 200, Loss: 0.09061026573181152\n",
            "Epoch 87, Step 201, Loss: 0.0825018659234047\n",
            "Epoch 87, Step 202, Loss: 0.0692579373717308\n",
            "Epoch 87, Step 203, Loss: 0.19857949018478394\n",
            "Epoch 87, Step 204, Loss: 0.10931617766618729\n",
            "Epoch 87, Step 205, Loss: 0.0885530486702919\n",
            "Epoch 87, Step 206, Loss: 0.1143021434545517\n",
            "Epoch 87, Step 207, Loss: 0.16965265572071075\n",
            "Epoch 87, Step 208, Loss: 0.08362996578216553\n",
            "Epoch 87, Step 209, Loss: 0.08837873488664627\n",
            "Epoch 87, Step 210, Loss: 0.15836356580257416\n",
            "Epoch 87, Step 211, Loss: 0.09844747930765152\n",
            "Epoch 87, Step 212, Loss: 0.08888423442840576\n",
            "Epoch 87, Step 213, Loss: 0.10146675258874893\n",
            "Epoch 87, Step 214, Loss: 0.06531637161970139\n",
            "Epoch 87, Step 215, Loss: 0.09938962012529373\n",
            "Epoch 87, Step 216, Loss: 0.1727723628282547\n",
            "Epoch 87, Step 217, Loss: 0.07416506111621857\n",
            "Epoch 87, Step 218, Loss: 0.11355423927307129\n",
            "Epoch 87, Step 219, Loss: 0.10851605236530304\n",
            "Epoch 87, Step 220, Loss: 0.10891737043857574\n",
            "Epoch 87, Step 221, Loss: 0.08903497457504272\n",
            "Epoch 87, Step 222, Loss: 0.1846190243959427\n",
            "Epoch 87, Step 223, Loss: 0.18170349299907684\n",
            "Epoch 87, Step 224, Loss: 0.15582819283008575\n",
            "Epoch 87, Step 225, Loss: 0.1259460300207138\n",
            "Epoch 87, Step 226, Loss: 0.08566880226135254\n",
            "Epoch 87, Step 227, Loss: 0.08995646983385086\n",
            "Epoch 87, Step 228, Loss: 0.09096205979585648\n",
            "Epoch 87, Step 229, Loss: 0.17378422617912292\n",
            "Epoch 87, Step 230, Loss: 0.15666013956069946\n",
            "Epoch 87, Step 231, Loss: 0.10386167466640472\n",
            "Epoch 87, Step 232, Loss: 0.14277935028076172\n",
            "Epoch 87, Step 233, Loss: 0.24608217179775238\n",
            "Epoch 87, Step 234, Loss: 0.08951433002948761\n",
            "Epoch 87, Step 235, Loss: 0.16433623433113098\n",
            "Epoch 87, Step 236, Loss: 0.042203135788440704\n",
            "Epoch 87, Step 237, Loss: 0.09196816384792328\n",
            "Epoch 87, Step 238, Loss: 0.10564443469047546\n",
            "Epoch 87, Step 239, Loss: 0.1605542004108429\n",
            "Epoch 87, Step 240, Loss: 0.12975814938545227\n",
            "Epoch 87, Step 241, Loss: 0.07354974001646042\n",
            "Epoch 87, Step 242, Loss: 0.1756003350019455\n",
            "Epoch 87, Step 243, Loss: 0.1429378092288971\n",
            "Epoch 87, Step 244, Loss: 0.09021434187889099\n",
            "Epoch 87, Step 245, Loss: 0.13260455429553986\n",
            "Epoch 87, Step 246, Loss: 0.17970044910907745\n",
            "Epoch 87, Step 247, Loss: 0.10845094919204712\n",
            "Epoch 87, Step 248, Loss: 0.09145130217075348\n",
            "Epoch 87, Step 249, Loss: 0.1042240783572197\n",
            "Epoch 87, Step 250, Loss: 0.21280165016651154\n",
            "Epoch 87, Step 251, Loss: 0.08164732903242111\n",
            "Epoch 87, Step 252, Loss: 0.16297951340675354\n",
            "Epoch 87, Step 253, Loss: 0.05986790359020233\n",
            "Epoch 87, Step 254, Loss: 0.12172998487949371\n",
            "Epoch 87, Step 255, Loss: 0.18368856608867645\n",
            "Epoch 87, Step 256, Loss: 0.19891922175884247\n",
            "Epoch 87, Step 257, Loss: 0.08716028928756714\n",
            "Epoch 87, Step 258, Loss: 0.11934324353933334\n",
            "Epoch 87, Step 259, Loss: 0.08754213899374008\n",
            "Epoch 87, Step 260, Loss: 0.055198486894369125\n",
            "Epoch 87, Step 261, Loss: 0.14902958273887634\n",
            "Epoch 87, Step 262, Loss: 0.1920337677001953\n",
            "Epoch 87, Step 263, Loss: 0.0461035892367363\n",
            "Epoch 87, Step 264, Loss: 0.09627749025821686\n",
            "Epoch 87, Step 265, Loss: 0.1435004025697708\n",
            "Epoch 87, Step 266, Loss: 0.12068826705217361\n",
            "Epoch 87, Step 267, Loss: 0.11410053074359894\n",
            "Epoch 87, Step 268, Loss: 0.06493645906448364\n",
            "Epoch 87, Step 269, Loss: 0.14914977550506592\n",
            "Epoch 87, Step 270, Loss: 0.12070094794034958\n",
            "Epoch 87, Step 271, Loss: 0.07219871133565903\n",
            "Epoch 87, Step 272, Loss: 0.1949232965707779\n",
            "Epoch 87, Step 273, Loss: 0.15366949141025543\n",
            "Epoch 87, Step 274, Loss: 0.15593795478343964\n",
            "Epoch 87, Step 275, Loss: 0.15722493827342987\n",
            "Epoch 87, Step 276, Loss: 0.12553521990776062\n",
            "Epoch 87, Step 277, Loss: 0.15259578824043274\n",
            "Epoch 87, Step 278, Loss: 0.13783417642116547\n",
            "Epoch 87, Step 279, Loss: 0.14287616312503815\n",
            "Epoch 87, Step 280, Loss: 0.07835341989994049\n",
            "Epoch 87, Step 281, Loss: 0.1404775083065033\n",
            "Epoch 87, Step 282, Loss: 0.17813755571842194\n",
            "Epoch 87, Step 283, Loss: 0.06815248727798462\n",
            "Epoch 87, Step 284, Loss: 0.09631765633821487\n",
            "Epoch 87, Step 285, Loss: 0.09535221010446548\n",
            "Epoch 87, Step 286, Loss: 0.1188330352306366\n",
            "Epoch 87, Step 287, Loss: 0.15583866834640503\n",
            "Epoch 87, Step 288, Loss: 0.17350585758686066\n",
            "Epoch 87, Step 289, Loss: 0.09997541457414627\n",
            "Epoch 87, Step 290, Loss: 0.15213927626609802\n",
            "Epoch 87, Step 291, Loss: 0.12258978188037872\n",
            "Epoch 87, Step 292, Loss: 0.0823884904384613\n",
            "Epoch 87, Step 293, Loss: 0.19558307528495789\n",
            "Epoch 87, Step 294, Loss: 0.08264179527759552\n",
            "Epoch 87, Step 295, Loss: 0.11588922142982483\n",
            "Epoch 87, Step 296, Loss: 0.21485653519630432\n",
            "Epoch 87, Step 297, Loss: 0.10447229444980621\n",
            "Epoch 87, Step 298, Loss: 0.11714673042297363\n",
            "Epoch 87, Step 299, Loss: 0.12079528719186783\n",
            "Epoch 87, Step 300, Loss: 0.09862425178289413\n",
            "Epoch 87, Step 301, Loss: 0.11462670564651489\n",
            "Epoch 87, Step 302, Loss: 0.10780572146177292\n",
            "Epoch 87, Step 303, Loss: 0.15874382853507996\n",
            "Epoch 87, Step 304, Loss: 0.059615179896354675\n",
            "Epoch 87, Step 305, Loss: 0.10687176138162613\n",
            "Epoch 87, Step 306, Loss: 0.06873933970928192\n",
            "Epoch 87, Step 307, Loss: 0.11111285537481308\n",
            "Epoch 87, Step 308, Loss: 0.166017085313797\n",
            "Epoch 87, Step 309, Loss: 0.16728782653808594\n",
            "Epoch 87, Step 310, Loss: 0.09831519424915314\n",
            "Epoch 87, Step 311, Loss: 0.17632721364498138\n",
            "Epoch 87, Step 312, Loss: 0.06074109300971031\n",
            "Epoch 87 end, avg train loss: 0.11605526320041178\n",
            "Epoch 87 end, avg val loss: 0.3641621615985243, accuracy: 90.45%\n",
            "Epoch 88, Step 0, Loss: 0.09408441931009293\n",
            "Epoch 88, Step 1, Loss: 0.0987919420003891\n",
            "Epoch 88, Step 2, Loss: 0.08809356391429901\n",
            "Epoch 88, Step 3, Loss: 0.05850972235202789\n",
            "Epoch 88, Step 4, Loss: 0.14839379489421844\n",
            "Epoch 88, Step 5, Loss: 0.1259201318025589\n",
            "Epoch 88, Step 6, Loss: 0.07819531857967377\n",
            "Epoch 88, Step 7, Loss: 0.10711728781461716\n",
            "Epoch 88, Step 8, Loss: 0.08765147626399994\n",
            "Epoch 88, Step 9, Loss: 0.08154302090406418\n",
            "Epoch 88, Step 10, Loss: 0.10381579399108887\n",
            "Epoch 88, Step 11, Loss: 0.15351825952529907\n",
            "Epoch 88, Step 12, Loss: 0.08889675885438919\n",
            "Epoch 88, Step 13, Loss: 0.1098172515630722\n",
            "Epoch 88, Step 14, Loss: 0.17810043692588806\n",
            "Epoch 88, Step 15, Loss: 0.101290762424469\n",
            "Epoch 88, Step 16, Loss: 0.1260913759469986\n",
            "Epoch 88, Step 17, Loss: 0.10401738435029984\n",
            "Epoch 88, Step 18, Loss: 0.08079303056001663\n",
            "Epoch 88, Step 19, Loss: 0.11566874384880066\n",
            "Epoch 88, Step 20, Loss: 0.07501264661550522\n",
            "Epoch 88, Step 21, Loss: 0.07504259794950485\n",
            "Epoch 88, Step 22, Loss: 0.14063340425491333\n",
            "Epoch 88, Step 23, Loss: 0.07766032218933105\n",
            "Epoch 88, Step 24, Loss: 0.11364459991455078\n",
            "Epoch 88, Step 25, Loss: 0.15687432885169983\n",
            "Epoch 88, Step 26, Loss: 0.10034390538930893\n",
            "Epoch 88, Step 27, Loss: 0.07840089499950409\n",
            "Epoch 88, Step 28, Loss: 0.13149073719978333\n",
            "Epoch 88, Step 29, Loss: 0.12436313927173615\n",
            "Epoch 88, Step 30, Loss: 0.14928537607192993\n",
            "Epoch 88, Step 31, Loss: 0.0794098824262619\n",
            "Epoch 88, Step 32, Loss: 0.16900084912776947\n",
            "Epoch 88, Step 33, Loss: 0.11704841256141663\n",
            "Epoch 88, Step 34, Loss: 0.11786768585443497\n",
            "Epoch 88, Step 35, Loss: 0.0980914756655693\n",
            "Epoch 88, Step 36, Loss: 0.13328111171722412\n",
            "Epoch 88, Step 37, Loss: 0.09335512667894363\n",
            "Epoch 88, Step 38, Loss: 0.0686444416642189\n",
            "Epoch 88, Step 39, Loss: 0.12113439291715622\n",
            "Epoch 88, Step 40, Loss: 0.1737925261259079\n",
            "Epoch 88, Step 41, Loss: 0.11448559165000916\n",
            "Epoch 88, Step 42, Loss: 0.1203211173415184\n",
            "Epoch 88, Step 43, Loss: 0.13862334191799164\n",
            "Epoch 88, Step 44, Loss: 0.10735872387886047\n",
            "Epoch 88, Step 45, Loss: 0.16669900715351105\n",
            "Epoch 88, Step 46, Loss: 0.20339076220989227\n",
            "Epoch 88, Step 47, Loss: 0.07710259407758713\n",
            "Epoch 88, Step 48, Loss: 0.08757034689188004\n",
            "Epoch 88, Step 49, Loss: 0.07513760775327682\n",
            "Epoch 88, Step 50, Loss: 0.10058045387268066\n",
            "Epoch 88, Step 51, Loss: 0.08955326676368713\n",
            "Epoch 88, Step 52, Loss: 0.059523265808820724\n",
            "Epoch 88, Step 53, Loss: 0.09089280664920807\n",
            "Epoch 88, Step 54, Loss: 0.06889843940734863\n",
            "Epoch 88, Step 55, Loss: 0.20832079648971558\n",
            "Epoch 88, Step 56, Loss: 0.03665900602936745\n",
            "Epoch 88, Step 57, Loss: 0.11549320071935654\n",
            "Epoch 88, Step 58, Loss: 0.08396846801042557\n",
            "Epoch 88, Step 59, Loss: 0.12762388586997986\n",
            "Epoch 88, Step 60, Loss: 0.14280451834201813\n",
            "Epoch 88, Step 61, Loss: 0.09676878899335861\n",
            "Epoch 88, Step 62, Loss: 0.13516710698604584\n",
            "Epoch 88, Step 63, Loss: 0.16564403474330902\n",
            "Epoch 88, Step 64, Loss: 0.11913350969552994\n",
            "Epoch 88, Step 65, Loss: 0.08476264774799347\n",
            "Epoch 88, Step 66, Loss: 0.09674544632434845\n",
            "Epoch 88, Step 67, Loss: 0.1063745841383934\n",
            "Epoch 88, Step 68, Loss: 0.08394181728363037\n",
            "Epoch 88, Step 69, Loss: 0.0963621437549591\n",
            "Epoch 88, Step 70, Loss: 0.11869662255048752\n",
            "Epoch 88, Step 71, Loss: 0.11443597078323364\n",
            "Epoch 88, Step 72, Loss: 0.17701776325702667\n",
            "Epoch 88, Step 73, Loss: 0.16802272200584412\n",
            "Epoch 88, Step 74, Loss: 0.12722688913345337\n",
            "Epoch 88, Step 75, Loss: 0.187491774559021\n",
            "Epoch 88, Step 76, Loss: 0.18297508358955383\n",
            "Epoch 88, Step 77, Loss: 0.13360628485679626\n",
            "Epoch 88, Step 78, Loss: 0.15827246010303497\n",
            "Epoch 88, Step 79, Loss: 0.07853097468614578\n",
            "Epoch 88, Step 80, Loss: 0.06935426592826843\n",
            "Epoch 88, Step 81, Loss: 0.1179569661617279\n",
            "Epoch 88, Step 82, Loss: 0.09686127305030823\n",
            "Epoch 88, Step 83, Loss: 0.14591941237449646\n",
            "Epoch 88, Step 84, Loss: 0.14488111436367035\n",
            "Epoch 88, Step 85, Loss: 0.07392800599336624\n",
            "Epoch 88, Step 86, Loss: 0.07092087715864182\n",
            "Epoch 88, Step 87, Loss: 0.12468528002500534\n",
            "Epoch 88, Step 88, Loss: 0.12426119297742844\n",
            "Epoch 88, Step 89, Loss: 0.12799261510372162\n",
            "Epoch 88, Step 90, Loss: 0.0679817870259285\n",
            "Epoch 88, Step 91, Loss: 0.12567557394504547\n",
            "Epoch 88, Step 92, Loss: 0.13083305954933167\n",
            "Epoch 88, Step 93, Loss: 0.2336864173412323\n",
            "Epoch 88, Step 94, Loss: 0.09309694916009903\n",
            "Epoch 88, Step 95, Loss: 0.10842655599117279\n",
            "Epoch 88, Step 96, Loss: 0.14783838391304016\n",
            "Epoch 88, Step 97, Loss: 0.2492760866880417\n",
            "Epoch 88, Step 98, Loss: 0.09524369984865189\n",
            "Epoch 88, Step 99, Loss: 0.13925568759441376\n",
            "Epoch 88, Step 100, Loss: 0.08776403963565826\n",
            "Epoch 88, Step 101, Loss: 0.06838679313659668\n",
            "Epoch 88, Step 102, Loss: 0.14145657420158386\n",
            "Epoch 88, Step 103, Loss: 0.09773190319538116\n",
            "Epoch 88, Step 104, Loss: 0.06254113465547562\n",
            "Epoch 88, Step 105, Loss: 0.10575667023658752\n",
            "Epoch 88, Step 106, Loss: 0.13196444511413574\n",
            "Epoch 88, Step 107, Loss: 0.1488589644432068\n",
            "Epoch 88, Step 108, Loss: 0.09852228313684464\n",
            "Epoch 88, Step 109, Loss: 0.07375017553567886\n",
            "Epoch 88, Step 110, Loss: 0.11626215279102325\n",
            "Epoch 88, Step 111, Loss: 0.1257774978876114\n",
            "Epoch 88, Step 112, Loss: 0.1382385939359665\n",
            "Epoch 88, Step 113, Loss: 0.12095455080270767\n",
            "Epoch 88, Step 114, Loss: 0.12379429489374161\n",
            "Epoch 88, Step 115, Loss: 0.12526923418045044\n",
            "Epoch 88, Step 116, Loss: 0.08205777406692505\n",
            "Epoch 88, Step 117, Loss: 0.08449745178222656\n",
            "Epoch 88, Step 118, Loss: 0.09897778928279877\n",
            "Epoch 88, Step 119, Loss: 0.18761800229549408\n",
            "Epoch 88, Step 120, Loss: 0.0824611634016037\n",
            "Epoch 88, Step 121, Loss: 0.14540550112724304\n",
            "Epoch 88, Step 122, Loss: 0.11565707623958588\n",
            "Epoch 88, Step 123, Loss: 0.1940344274044037\n",
            "Epoch 88, Step 124, Loss: 0.12498996406793594\n",
            "Epoch 88, Step 125, Loss: 0.17103266716003418\n",
            "Epoch 88, Step 126, Loss: 0.11505410820245743\n",
            "Epoch 88, Step 127, Loss: 0.1683717519044876\n",
            "Epoch 88, Step 128, Loss: 0.17869435250759125\n",
            "Epoch 88, Step 129, Loss: 0.1254434585571289\n",
            "Epoch 88, Step 130, Loss: 0.1317887008190155\n",
            "Epoch 88, Step 131, Loss: 0.14403538405895233\n",
            "Epoch 88, Step 132, Loss: 0.061744339764118195\n",
            "Epoch 88, Step 133, Loss: 0.15632133185863495\n",
            "Epoch 88, Step 134, Loss: 0.0399768091738224\n",
            "Epoch 88, Step 135, Loss: 0.05884825065732002\n",
            "Epoch 88, Step 136, Loss: 0.15598049759864807\n",
            "Epoch 88, Step 137, Loss: 0.15381966531276703\n",
            "Epoch 88, Step 138, Loss: 0.06875617057085037\n",
            "Epoch 88, Step 139, Loss: 0.23014917969703674\n",
            "Epoch 88, Step 140, Loss: 0.16026811301708221\n",
            "Epoch 88, Step 141, Loss: 0.06334935873746872\n",
            "Epoch 88, Step 142, Loss: 0.20160239934921265\n",
            "Epoch 88, Step 143, Loss: 0.09128981828689575\n",
            "Epoch 88, Step 144, Loss: 0.15325959026813507\n",
            "Epoch 88, Step 145, Loss: 0.06801851838827133\n",
            "Epoch 88, Step 146, Loss: 0.08376365154981613\n",
            "Epoch 88, Step 147, Loss: 0.11010785400867462\n",
            "Epoch 88, Step 148, Loss: 0.14333589375019073\n",
            "Epoch 88, Step 149, Loss: 0.0983460545539856\n",
            "Epoch 88, Step 150, Loss: 0.09285550564527512\n",
            "Epoch 88, Step 151, Loss: 0.0951755940914154\n",
            "Epoch 88, Step 152, Loss: 0.11200008541345596\n",
            "Epoch 88, Step 153, Loss: 0.15767790377140045\n",
            "Epoch 88, Step 154, Loss: 0.1370120495557785\n",
            "Epoch 88, Step 155, Loss: 0.09009377658367157\n",
            "Epoch 88, Step 156, Loss: 0.08756718039512634\n",
            "Epoch 88, Step 157, Loss: 0.12969540059566498\n",
            "Epoch 88, Step 158, Loss: 0.13003836572170258\n",
            "Epoch 88, Step 159, Loss: 0.13221095502376556\n",
            "Epoch 88, Step 160, Loss: 0.16998261213302612\n",
            "Epoch 88, Step 161, Loss: 0.16185417771339417\n",
            "Epoch 88, Step 162, Loss: 0.16675908863544464\n",
            "Epoch 88, Step 163, Loss: 0.1715025007724762\n",
            "Epoch 88, Step 164, Loss: 0.07051266729831696\n",
            "Epoch 88, Step 165, Loss: 0.09218771755695343\n",
            "Epoch 88, Step 166, Loss: 0.08050847798585892\n",
            "Epoch 88, Step 167, Loss: 0.12827590107917786\n",
            "Epoch 88, Step 168, Loss: 0.18106593191623688\n",
            "Epoch 88, Step 169, Loss: 0.08528514206409454\n",
            "Epoch 88, Step 170, Loss: 0.12179690599441528\n",
            "Epoch 88, Step 171, Loss: 0.0958314761519432\n",
            "Epoch 88, Step 172, Loss: 0.12980350852012634\n",
            "Epoch 88, Step 173, Loss: 0.07322841137647629\n",
            "Epoch 88, Step 174, Loss: 0.11352203786373138\n",
            "Epoch 88, Step 175, Loss: 0.1908327043056488\n",
            "Epoch 88, Step 176, Loss: 0.19752521812915802\n",
            "Epoch 88, Step 177, Loss: 0.12357424944639206\n",
            "Epoch 88, Step 178, Loss: 0.10756821185350418\n",
            "Epoch 88, Step 179, Loss: 0.18306757509708405\n",
            "Epoch 88, Step 180, Loss: 0.1029864251613617\n",
            "Epoch 88, Step 181, Loss: 0.11856799572706223\n",
            "Epoch 88, Step 182, Loss: 0.06801627576351166\n",
            "Epoch 88, Step 183, Loss: 0.16264192759990692\n",
            "Epoch 88, Step 184, Loss: 0.06515458226203918\n",
            "Epoch 88, Step 185, Loss: 0.05735357478260994\n",
            "Epoch 88, Step 186, Loss: 0.12618082761764526\n",
            "Epoch 88, Step 187, Loss: 0.15127518773078918\n",
            "Epoch 88, Step 188, Loss: 0.08852968364953995\n",
            "Epoch 88, Step 189, Loss: 0.06876662373542786\n",
            "Epoch 88, Step 190, Loss: 0.13535748422145844\n",
            "Epoch 88, Step 191, Loss: 0.10297852754592896\n",
            "Epoch 88, Step 192, Loss: 0.08027245104312897\n",
            "Epoch 88, Step 193, Loss: 0.05631142482161522\n",
            "Epoch 88, Step 194, Loss: 0.10385327786207199\n",
            "Epoch 88, Step 195, Loss: 0.08981446176767349\n",
            "Epoch 88, Step 196, Loss: 0.07182508707046509\n",
            "Epoch 88, Step 197, Loss: 0.19792217016220093\n",
            "Epoch 88, Step 198, Loss: 0.11131681501865387\n",
            "Epoch 88, Step 199, Loss: 0.0979214608669281\n",
            "Epoch 88, Step 200, Loss: 0.07366050779819489\n",
            "Epoch 88, Step 201, Loss: 0.08730316162109375\n",
            "Epoch 88, Step 202, Loss: 0.14144697785377502\n",
            "Epoch 88, Step 203, Loss: 0.10330459475517273\n",
            "Epoch 88, Step 204, Loss: 0.16858279705047607\n",
            "Epoch 88, Step 205, Loss: 0.21302370727062225\n",
            "Epoch 88, Step 206, Loss: 0.07905730605125427\n",
            "Epoch 88, Step 207, Loss: 0.07440667599439621\n",
            "Epoch 88, Step 208, Loss: 0.12314888834953308\n",
            "Epoch 88, Step 209, Loss: 0.05894569680094719\n",
            "Epoch 88, Step 210, Loss: 0.08205869793891907\n",
            "Epoch 88, Step 211, Loss: 0.0936303660273552\n",
            "Epoch 88, Step 212, Loss: 0.08947987854480743\n",
            "Epoch 88, Step 213, Loss: 0.16196036338806152\n",
            "Epoch 88, Step 214, Loss: 0.03996557742357254\n",
            "Epoch 88, Step 215, Loss: 0.16024769842624664\n",
            "Epoch 88, Step 216, Loss: 0.1375398188829422\n",
            "Epoch 88, Step 217, Loss: 0.07856723666191101\n",
            "Epoch 88, Step 218, Loss: 0.12187286466360092\n",
            "Epoch 88, Step 219, Loss: 0.07184655964374542\n",
            "Epoch 88, Step 220, Loss: 0.10430481284856796\n",
            "Epoch 88, Step 221, Loss: 0.12788312137126923\n",
            "Epoch 88, Step 222, Loss: 0.0739068016409874\n",
            "Epoch 88, Step 223, Loss: 0.09591053426265717\n",
            "Epoch 88, Step 224, Loss: 0.16212087869644165\n",
            "Epoch 88, Step 225, Loss: 0.07531299442052841\n",
            "Epoch 88, Step 226, Loss: 0.1578032523393631\n",
            "Epoch 88, Step 227, Loss: 0.12430212646722794\n",
            "Epoch 88, Step 228, Loss: 0.14692322909832\n",
            "Epoch 88, Step 229, Loss: 0.0873083770275116\n",
            "Epoch 88, Step 230, Loss: 0.06088181957602501\n",
            "Epoch 88, Step 231, Loss: 0.08094271272420883\n",
            "Epoch 88, Step 232, Loss: 0.08246307820081711\n",
            "Epoch 88, Step 233, Loss: 0.15028676390647888\n",
            "Epoch 88, Step 234, Loss: 0.09761397540569305\n",
            "Epoch 88, Step 235, Loss: 0.14099308848381042\n",
            "Epoch 88, Step 236, Loss: 0.10213562101125717\n",
            "Epoch 88, Step 237, Loss: 0.14182105660438538\n",
            "Epoch 88, Step 238, Loss: 0.15143607556819916\n",
            "Epoch 88, Step 239, Loss: 0.08544330298900604\n",
            "Epoch 88, Step 240, Loss: 0.17280864715576172\n",
            "Epoch 88, Step 241, Loss: 0.14039762318134308\n",
            "Epoch 88, Step 242, Loss: 0.1087661013007164\n",
            "Epoch 88, Step 243, Loss: 0.06201425567269325\n",
            "Epoch 88, Step 244, Loss: 0.1743764728307724\n",
            "Epoch 88, Step 245, Loss: 0.09492552280426025\n",
            "Epoch 88, Step 246, Loss: 0.06660977751016617\n",
            "Epoch 88, Step 247, Loss: 0.10829892754554749\n",
            "Epoch 88, Step 248, Loss: 0.09253804385662079\n",
            "Epoch 88, Step 249, Loss: 0.07360247522592545\n",
            "Epoch 88, Step 250, Loss: 0.08333954960107803\n",
            "Epoch 88, Step 251, Loss: 0.09448571503162384\n",
            "Epoch 88, Step 252, Loss: 0.07757508754730225\n",
            "Epoch 88, Step 253, Loss: 0.14497005939483643\n",
            "Epoch 88, Step 254, Loss: 0.12862320244312286\n",
            "Epoch 88, Step 255, Loss: 0.11348698288202286\n",
            "Epoch 88, Step 256, Loss: 0.1073174923658371\n",
            "Epoch 88, Step 257, Loss: 0.14287754893302917\n",
            "Epoch 88, Step 258, Loss: 0.10317829251289368\n",
            "Epoch 88, Step 259, Loss: 0.10446231067180634\n",
            "Epoch 88, Step 260, Loss: 0.10299799591302872\n",
            "Epoch 88, Step 261, Loss: 0.2939096987247467\n",
            "Epoch 88, Step 262, Loss: 0.1286810040473938\n",
            "Epoch 88, Step 263, Loss: 0.1407182812690735\n",
            "Epoch 88, Step 264, Loss: 0.049757182598114014\n",
            "Epoch 88, Step 265, Loss: 0.13409137725830078\n",
            "Epoch 88, Step 266, Loss: 0.11735726147890091\n",
            "Epoch 88, Step 267, Loss: 0.056984201073646545\n",
            "Epoch 88, Step 268, Loss: 0.03807942941784859\n",
            "Epoch 88, Step 269, Loss: 0.09468846768140793\n",
            "Epoch 88, Step 270, Loss: 0.17240974307060242\n",
            "Epoch 88, Step 271, Loss: 0.17815625667572021\n",
            "Epoch 88, Step 272, Loss: 0.04264530539512634\n",
            "Epoch 88, Step 273, Loss: 0.06848489493131638\n",
            "Epoch 88, Step 274, Loss: 0.0986994206905365\n",
            "Epoch 88, Step 275, Loss: 0.10406583547592163\n",
            "Epoch 88, Step 276, Loss: 0.10401567071676254\n",
            "Epoch 88, Step 277, Loss: 0.1288224309682846\n",
            "Epoch 88, Step 278, Loss: 0.19691438972949982\n",
            "Epoch 88, Step 279, Loss: 0.1392897516489029\n",
            "Epoch 88, Step 280, Loss: 0.08552033454179764\n",
            "Epoch 88, Step 281, Loss: 0.12544815242290497\n",
            "Epoch 88, Step 282, Loss: 0.1554771512746811\n",
            "Epoch 88, Step 283, Loss: 0.11447005718946457\n",
            "Epoch 88, Step 284, Loss: 0.07424689084291458\n",
            "Epoch 88, Step 285, Loss: 0.09560517966747284\n",
            "Epoch 88, Step 286, Loss: 0.08768422156572342\n",
            "Epoch 88, Step 287, Loss: 0.08447416871786118\n",
            "Epoch 88, Step 288, Loss: 0.1218879446387291\n",
            "Epoch 88, Step 289, Loss: 0.08781366050243378\n",
            "Epoch 88, Step 290, Loss: 0.19138537347316742\n",
            "Epoch 88, Step 291, Loss: 0.10209466516971588\n",
            "Epoch 88, Step 292, Loss: 0.2003915160894394\n",
            "Epoch 88, Step 293, Loss: 0.1341041624546051\n",
            "Epoch 88, Step 294, Loss: 0.06072310358285904\n",
            "Epoch 88, Step 295, Loss: 0.06168246641755104\n",
            "Epoch 88, Step 296, Loss: 0.10803968459367752\n",
            "Epoch 88, Step 297, Loss: 0.11104131489992142\n",
            "Epoch 88, Step 298, Loss: 0.12259296327829361\n",
            "Epoch 88, Step 299, Loss: 0.18694999814033508\n",
            "Epoch 88, Step 300, Loss: 0.06635963171720505\n",
            "Epoch 88, Step 301, Loss: 0.04996361583471298\n",
            "Epoch 88, Step 302, Loss: 0.06697716563940048\n",
            "Epoch 88, Step 303, Loss: 0.24149848520755768\n",
            "Epoch 88, Step 304, Loss: 0.0779837816953659\n",
            "Epoch 88, Step 305, Loss: 0.16627830266952515\n",
            "Epoch 88, Step 306, Loss: 0.11029280722141266\n",
            "Epoch 88, Step 307, Loss: 0.06139235571026802\n",
            "Epoch 88, Step 308, Loss: 0.09783396124839783\n",
            "Epoch 88, Step 309, Loss: 0.12894147634506226\n",
            "Epoch 88, Step 310, Loss: 0.0798749178647995\n",
            "Epoch 88, Step 311, Loss: 0.12304071336984634\n",
            "Epoch 88, Step 312, Loss: 0.07268553972244263\n",
            "Epoch 88 end, avg train loss: 0.11562655842342316\n",
            "Epoch 88 end, avg val loss: 0.37270331358878955, accuracy: 90.53%\n",
            "Epoch 89, Step 0, Loss: 0.07156012207269669\n",
            "Epoch 89, Step 1, Loss: 0.07167067378759384\n",
            "Epoch 89, Step 2, Loss: 0.06127382814884186\n",
            "Epoch 89, Step 3, Loss: 0.12378066778182983\n",
            "Epoch 89, Step 4, Loss: 0.14430412650108337\n",
            "Epoch 89, Step 5, Loss: 0.12121731787919998\n",
            "Epoch 89, Step 6, Loss: 0.07246826589107513\n",
            "Epoch 89, Step 7, Loss: 0.10953488200902939\n",
            "Epoch 89, Step 8, Loss: 0.05443271994590759\n",
            "Epoch 89, Step 9, Loss: 0.0762530118227005\n",
            "Epoch 89, Step 10, Loss: 0.05275142937898636\n",
            "Epoch 89, Step 11, Loss: 0.16023367643356323\n",
            "Epoch 89, Step 12, Loss: 0.07784846425056458\n",
            "Epoch 89, Step 13, Loss: 0.0872315987944603\n",
            "Epoch 89, Step 14, Loss: 0.08460016548633575\n",
            "Epoch 89, Step 15, Loss: 0.08882609009742737\n",
            "Epoch 89, Step 16, Loss: 0.15937747061252594\n",
            "Epoch 89, Step 17, Loss: 0.07150663435459137\n",
            "Epoch 89, Step 18, Loss: 0.10485497117042542\n",
            "Epoch 89, Step 19, Loss: 0.04683639481663704\n",
            "Epoch 89, Step 20, Loss: 0.1127377599477768\n",
            "Epoch 89, Step 21, Loss: 0.09716016054153442\n",
            "Epoch 89, Step 22, Loss: 0.15254487097263336\n",
            "Epoch 89, Step 23, Loss: 0.09774696081876755\n",
            "Epoch 89, Step 24, Loss: 0.06418735533952713\n",
            "Epoch 89, Step 25, Loss: 0.18251076340675354\n",
            "Epoch 89, Step 26, Loss: 0.08260747790336609\n",
            "Epoch 89, Step 27, Loss: 0.10836024582386017\n",
            "Epoch 89, Step 28, Loss: 0.19477927684783936\n",
            "Epoch 89, Step 29, Loss: 0.07891508936882019\n",
            "Epoch 89, Step 30, Loss: 0.0955064445734024\n",
            "Epoch 89, Step 31, Loss: 0.11556857079267502\n",
            "Epoch 89, Step 32, Loss: 0.10392733663320541\n",
            "Epoch 89, Step 33, Loss: 0.02703753672540188\n",
            "Epoch 89, Step 34, Loss: 0.1757594645023346\n",
            "Epoch 89, Step 35, Loss: 0.1841237097978592\n",
            "Epoch 89, Step 36, Loss: 0.14906316995620728\n",
            "Epoch 89, Step 37, Loss: 0.07416169345378876\n",
            "Epoch 89, Step 38, Loss: 0.15907809138298035\n",
            "Epoch 89, Step 39, Loss: 0.08792875707149506\n",
            "Epoch 89, Step 40, Loss: 0.16520605981349945\n",
            "Epoch 89, Step 41, Loss: 0.1328439712524414\n",
            "Epoch 89, Step 42, Loss: 0.1544262021780014\n",
            "Epoch 89, Step 43, Loss: 0.12365487962961197\n",
            "Epoch 89, Step 44, Loss: 0.11428304016590118\n",
            "Epoch 89, Step 45, Loss: 0.15235158801078796\n",
            "Epoch 89, Step 46, Loss: 0.11605727672576904\n",
            "Epoch 89, Step 47, Loss: 0.10714826732873917\n",
            "Epoch 89, Step 48, Loss: 0.07312244921922684\n",
            "Epoch 89, Step 49, Loss: 0.12003056704998016\n",
            "Epoch 89, Step 50, Loss: 0.15269529819488525\n",
            "Epoch 89, Step 51, Loss: 0.094883032143116\n",
            "Epoch 89, Step 52, Loss: 0.10802418738603592\n",
            "Epoch 89, Step 53, Loss: 0.17422515153884888\n",
            "Epoch 89, Step 54, Loss: 0.08412244915962219\n",
            "Epoch 89, Step 55, Loss: 0.1010998860001564\n",
            "Epoch 89, Step 56, Loss: 0.1809040755033493\n",
            "Epoch 89, Step 57, Loss: 0.12913616001605988\n",
            "Epoch 89, Step 58, Loss: 0.1227438896894455\n",
            "Epoch 89, Step 59, Loss: 0.07201564311981201\n",
            "Epoch 89, Step 60, Loss: 0.1452345997095108\n",
            "Epoch 89, Step 61, Loss: 0.16256183385849\n",
            "Epoch 89, Step 62, Loss: 0.06598931550979614\n",
            "Epoch 89, Step 63, Loss: 0.17428763210773468\n",
            "Epoch 89, Step 64, Loss: 0.14881543815135956\n",
            "Epoch 89, Step 65, Loss: 0.056451357901096344\n",
            "Epoch 89, Step 66, Loss: 0.156600221991539\n",
            "Epoch 89, Step 67, Loss: 0.09561748057603836\n",
            "Epoch 89, Step 68, Loss: 0.07153673470020294\n",
            "Epoch 89, Step 69, Loss: 0.12716776132583618\n",
            "Epoch 89, Step 70, Loss: 0.1532687097787857\n",
            "Epoch 89, Step 71, Loss: 0.14269724488258362\n",
            "Epoch 89, Step 72, Loss: 0.0548066571354866\n",
            "Epoch 89, Step 73, Loss: 0.13843677937984467\n",
            "Epoch 89, Step 74, Loss: 0.09692256152629852\n",
            "Epoch 89, Step 75, Loss: 0.0846402645111084\n",
            "Epoch 89, Step 76, Loss: 0.039943791925907135\n",
            "Epoch 89, Step 77, Loss: 0.1520211547613144\n",
            "Epoch 89, Step 78, Loss: 0.20345237851142883\n",
            "Epoch 89, Step 79, Loss: 0.1413336992263794\n",
            "Epoch 89, Step 80, Loss: 0.1439887136220932\n",
            "Epoch 89, Step 81, Loss: 0.08801744878292084\n",
            "Epoch 89, Step 82, Loss: 0.1293504238128662\n",
            "Epoch 89, Step 83, Loss: 0.15526260435581207\n",
            "Epoch 89, Step 84, Loss: 0.0908978208899498\n",
            "Epoch 89, Step 85, Loss: 0.11847364157438278\n",
            "Epoch 89, Step 86, Loss: 0.10618474334478378\n",
            "Epoch 89, Step 87, Loss: 0.08096035569906235\n",
            "Epoch 89, Step 88, Loss: 0.12843497097492218\n",
            "Epoch 89, Step 89, Loss: 0.12925417721271515\n",
            "Epoch 89, Step 90, Loss: 0.14415639638900757\n",
            "Epoch 89, Step 91, Loss: 0.09618224203586578\n",
            "Epoch 89, Step 92, Loss: 0.14630255103111267\n",
            "Epoch 89, Step 93, Loss: 0.1777343600988388\n",
            "Epoch 89, Step 94, Loss: 0.10012555867433548\n",
            "Epoch 89, Step 95, Loss: 0.1125090941786766\n",
            "Epoch 89, Step 96, Loss: 0.10975081473588943\n",
            "Epoch 89, Step 97, Loss: 0.06804609298706055\n",
            "Epoch 89, Step 98, Loss: 0.09452389180660248\n",
            "Epoch 89, Step 99, Loss: 0.052572693675756454\n",
            "Epoch 89, Step 100, Loss: 0.1645870953798294\n",
            "Epoch 89, Step 101, Loss: 0.07930049300193787\n",
            "Epoch 89, Step 102, Loss: 0.07652554661035538\n",
            "Epoch 89, Step 103, Loss: 0.10484375804662704\n",
            "Epoch 89, Step 104, Loss: 0.07797063142061234\n",
            "Epoch 89, Step 105, Loss: 0.12385309487581253\n",
            "Epoch 89, Step 106, Loss: 0.05330997705459595\n",
            "Epoch 89, Step 107, Loss: 0.11737264692783356\n",
            "Epoch 89, Step 108, Loss: 0.1909404993057251\n",
            "Epoch 89, Step 109, Loss: 0.09431793540716171\n",
            "Epoch 89, Step 110, Loss: 0.0724477469921112\n",
            "Epoch 89, Step 111, Loss: 0.09234406054019928\n",
            "Epoch 89, Step 112, Loss: 0.11329569667577744\n",
            "Epoch 89, Step 113, Loss: 0.11927175521850586\n",
            "Epoch 89, Step 114, Loss: 0.11343713104724884\n",
            "Epoch 89, Step 115, Loss: 0.16253431141376495\n",
            "Epoch 89, Step 116, Loss: 0.1593090146780014\n",
            "Epoch 89, Step 117, Loss: 0.0894782543182373\n",
            "Epoch 89, Step 118, Loss: 0.0703539028763771\n",
            "Epoch 89, Step 119, Loss: 0.03918266296386719\n",
            "Epoch 89, Step 120, Loss: 0.21501559019088745\n",
            "Epoch 89, Step 121, Loss: 0.0603155642747879\n",
            "Epoch 89, Step 122, Loss: 0.0974801778793335\n",
            "Epoch 89, Step 123, Loss: 0.10068711638450623\n",
            "Epoch 89, Step 124, Loss: 0.12875451147556305\n",
            "Epoch 89, Step 125, Loss: 0.10686077177524567\n",
            "Epoch 89, Step 126, Loss: 0.08491159975528717\n",
            "Epoch 89, Step 127, Loss: 0.0937873125076294\n",
            "Epoch 89, Step 128, Loss: 0.05732369422912598\n",
            "Epoch 89, Step 129, Loss: 0.1309869885444641\n",
            "Epoch 89, Step 130, Loss: 0.10506255179643631\n",
            "Epoch 89, Step 131, Loss: 0.05642327666282654\n",
            "Epoch 89, Step 132, Loss: 0.12662218511104584\n",
            "Epoch 89, Step 133, Loss: 0.08010189980268478\n",
            "Epoch 89, Step 134, Loss: 0.13792648911476135\n",
            "Epoch 89, Step 135, Loss: 0.08717411011457443\n",
            "Epoch 89, Step 136, Loss: 0.13360561430454254\n",
            "Epoch 89, Step 137, Loss: 0.09551667422056198\n",
            "Epoch 89, Step 138, Loss: 0.3036704361438751\n",
            "Epoch 89, Step 139, Loss: 0.16285225749015808\n",
            "Epoch 89, Step 140, Loss: 0.10537920892238617\n",
            "Epoch 89, Step 141, Loss: 0.040603265166282654\n",
            "Epoch 89, Step 142, Loss: 0.1371837556362152\n",
            "Epoch 89, Step 143, Loss: 0.12408571690320969\n",
            "Epoch 89, Step 144, Loss: 0.11697468161582947\n",
            "Epoch 89, Step 145, Loss: 0.1664908081293106\n",
            "Epoch 89, Step 146, Loss: 0.06926966458559036\n",
            "Epoch 89, Step 147, Loss: 0.11729814857244492\n",
            "Epoch 89, Step 148, Loss: 0.1528719663619995\n",
            "Epoch 89, Step 149, Loss: 0.07818799465894699\n",
            "Epoch 89, Step 150, Loss: 0.05843678489327431\n",
            "Epoch 89, Step 151, Loss: 0.09415782243013382\n",
            "Epoch 89, Step 152, Loss: 0.08106723427772522\n",
            "Epoch 89, Step 153, Loss: 0.1583947390317917\n",
            "Epoch 89, Step 154, Loss: 0.158624529838562\n",
            "Epoch 89, Step 155, Loss: 0.14444608986377716\n",
            "Epoch 89, Step 156, Loss: 0.14330850541591644\n",
            "Epoch 89, Step 157, Loss: 0.08403962850570679\n",
            "Epoch 89, Step 158, Loss: 0.0759173259139061\n",
            "Epoch 89, Step 159, Loss: 0.07074323296546936\n",
            "Epoch 89, Step 160, Loss: 0.10933724045753479\n",
            "Epoch 89, Step 161, Loss: 0.0937395915389061\n",
            "Epoch 89, Step 162, Loss: 0.10020392388105392\n",
            "Epoch 89, Step 163, Loss: 0.12815403938293457\n",
            "Epoch 89, Step 164, Loss: 0.17328715324401855\n",
            "Epoch 89, Step 165, Loss: 0.08302880823612213\n",
            "Epoch 89, Step 166, Loss: 0.11290419846773148\n",
            "Epoch 89, Step 167, Loss: 0.14344893395900726\n",
            "Epoch 89, Step 168, Loss: 0.0732731968164444\n",
            "Epoch 89, Step 169, Loss: 0.09199827164411545\n",
            "Epoch 89, Step 170, Loss: 0.09426715970039368\n",
            "Epoch 89, Step 171, Loss: 0.09959340840578079\n",
            "Epoch 89, Step 172, Loss: 0.2043989598751068\n",
            "Epoch 89, Step 173, Loss: 0.11438605189323425\n",
            "Epoch 89, Step 174, Loss: 0.16215777397155762\n",
            "Epoch 89, Step 175, Loss: 0.06039055436849594\n",
            "Epoch 89, Step 176, Loss: 0.11292919516563416\n",
            "Epoch 89, Step 177, Loss: 0.03692825511097908\n",
            "Epoch 89, Step 178, Loss: 0.060418751090765\n",
            "Epoch 89, Step 179, Loss: 0.13155712187290192\n",
            "Epoch 89, Step 180, Loss: 0.1707097589969635\n",
            "Epoch 89, Step 181, Loss: 0.09020090848207474\n",
            "Epoch 89, Step 182, Loss: 0.1026589497923851\n",
            "Epoch 89, Step 183, Loss: 0.14012828469276428\n",
            "Epoch 89, Step 184, Loss: 0.13598337769508362\n",
            "Epoch 89, Step 185, Loss: 0.16039808094501495\n",
            "Epoch 89, Step 186, Loss: 0.052300866693258286\n",
            "Epoch 89, Step 187, Loss: 0.09821414202451706\n",
            "Epoch 89, Step 188, Loss: 0.1059872955083847\n",
            "Epoch 89, Step 189, Loss: 0.16691921651363373\n",
            "Epoch 89, Step 190, Loss: 0.09718777984380722\n",
            "Epoch 89, Step 191, Loss: 0.11740609258413315\n",
            "Epoch 89, Step 192, Loss: 0.07273515313863754\n",
            "Epoch 89, Step 193, Loss: 0.11493553966283798\n",
            "Epoch 89, Step 194, Loss: 0.10603467375040054\n",
            "Epoch 89, Step 195, Loss: 0.14605997502803802\n",
            "Epoch 89, Step 196, Loss: 0.14094576239585876\n",
            "Epoch 89, Step 197, Loss: 0.13440260291099548\n",
            "Epoch 89, Step 198, Loss: 0.10087040811777115\n",
            "Epoch 89, Step 199, Loss: 0.13268621265888214\n",
            "Epoch 89, Step 200, Loss: 0.20975464582443237\n",
            "Epoch 89, Step 201, Loss: 0.15272751450538635\n",
            "Epoch 89, Step 202, Loss: 0.0793764740228653\n",
            "Epoch 89, Step 203, Loss: 0.15875263512134552\n",
            "Epoch 89, Step 204, Loss: 0.05893028527498245\n",
            "Epoch 89, Step 205, Loss: 0.06967681646347046\n",
            "Epoch 89, Step 206, Loss: 0.05400574952363968\n",
            "Epoch 89, Step 207, Loss: 0.08244731277227402\n",
            "Epoch 89, Step 208, Loss: 0.153384268283844\n",
            "Epoch 89, Step 209, Loss: 0.11237647384405136\n",
            "Epoch 89, Step 210, Loss: 0.10273446142673492\n",
            "Epoch 89, Step 211, Loss: 0.11628642678260803\n",
            "Epoch 89, Step 212, Loss: 0.10794858634471893\n",
            "Epoch 89, Step 213, Loss: 0.0375170074403286\n",
            "Epoch 89, Step 214, Loss: 0.0663919597864151\n",
            "Epoch 89, Step 215, Loss: 0.13091689348220825\n",
            "Epoch 89, Step 216, Loss: 0.10786481946706772\n",
            "Epoch 89, Step 217, Loss: 0.14390358328819275\n",
            "Epoch 89, Step 218, Loss: 0.06917557865381241\n",
            "Epoch 89, Step 219, Loss: 0.035613592714071274\n",
            "Epoch 89, Step 220, Loss: 0.09190364927053452\n",
            "Epoch 89, Step 221, Loss: 0.06756521761417389\n",
            "Epoch 89, Step 222, Loss: 0.08164992183446884\n",
            "Epoch 89, Step 223, Loss: 0.12894830107688904\n",
            "Epoch 89, Step 224, Loss: 0.09856098145246506\n",
            "Epoch 89, Step 225, Loss: 0.1028548926115036\n",
            "Epoch 89, Step 226, Loss: 0.15214432775974274\n",
            "Epoch 89, Step 227, Loss: 0.13633845746517181\n",
            "Epoch 89, Step 228, Loss: 0.06444326043128967\n",
            "Epoch 89, Step 229, Loss: 0.16449908912181854\n",
            "Epoch 89, Step 230, Loss: 0.10077575594186783\n",
            "Epoch 89, Step 231, Loss: 0.08352570980787277\n",
            "Epoch 89, Step 232, Loss: 0.14054958522319794\n",
            "Epoch 89, Step 233, Loss: 0.07477939873933792\n",
            "Epoch 89, Step 234, Loss: 0.07769687473773956\n",
            "Epoch 89, Step 235, Loss: 0.16011634469032288\n",
            "Epoch 89, Step 236, Loss: 0.14767789840698242\n",
            "Epoch 89, Step 237, Loss: 0.050698067992925644\n",
            "Epoch 89, Step 238, Loss: 0.10886938869953156\n",
            "Epoch 89, Step 239, Loss: 0.13669978082180023\n",
            "Epoch 89, Step 240, Loss: 0.09598249942064285\n",
            "Epoch 89, Step 241, Loss: 0.15530245006084442\n",
            "Epoch 89, Step 242, Loss: 0.12763187289237976\n",
            "Epoch 89, Step 243, Loss: 0.17443561553955078\n",
            "Epoch 89, Step 244, Loss: 0.1067579984664917\n",
            "Epoch 89, Step 245, Loss: 0.16980724036693573\n",
            "Epoch 89, Step 246, Loss: 0.06706149131059647\n",
            "Epoch 89, Step 247, Loss: 0.1419912874698639\n",
            "Epoch 89, Step 248, Loss: 0.08920834213495255\n",
            "Epoch 89, Step 249, Loss: 0.1049831435084343\n",
            "Epoch 89, Step 250, Loss: 0.11235054582357407\n",
            "Epoch 89, Step 251, Loss: 0.16313436627388\n",
            "Epoch 89, Step 252, Loss: 0.20224136114120483\n",
            "Epoch 89, Step 253, Loss: 0.1854788064956665\n",
            "Epoch 89, Step 254, Loss: 0.1498822420835495\n",
            "Epoch 89, Step 255, Loss: 0.08435971289873123\n",
            "Epoch 89, Step 256, Loss: 0.10183733701705933\n",
            "Epoch 89, Step 257, Loss: 0.11020918190479279\n",
            "Epoch 89, Step 258, Loss: 0.09210497885942459\n",
            "Epoch 89, Step 259, Loss: 0.1232849508523941\n",
            "Epoch 89, Step 260, Loss: 0.13659384846687317\n",
            "Epoch 89, Step 261, Loss: 0.09163587540388107\n",
            "Epoch 89, Step 262, Loss: 0.0800459086894989\n",
            "Epoch 89, Step 263, Loss: 0.16772887110710144\n",
            "Epoch 89, Step 264, Loss: 0.12263313680887222\n",
            "Epoch 89, Step 265, Loss: 0.0660671666264534\n",
            "Epoch 89, Step 266, Loss: 0.11235859990119934\n",
            "Epoch 89, Step 267, Loss: 0.1102585643529892\n",
            "Epoch 89, Step 268, Loss: 0.14211641252040863\n",
            "Epoch 89, Step 269, Loss: 0.17886264622211456\n",
            "Epoch 89, Step 270, Loss: 0.11989953368902206\n",
            "Epoch 89, Step 271, Loss: 0.14886221289634705\n",
            "Epoch 89, Step 272, Loss: 0.16558624804019928\n",
            "Epoch 89, Step 273, Loss: 0.08652821183204651\n",
            "Epoch 89, Step 274, Loss: 0.1309940218925476\n",
            "Epoch 89, Step 275, Loss: 0.11491912603378296\n",
            "Epoch 89, Step 276, Loss: 0.12506145238876343\n",
            "Epoch 89, Step 277, Loss: 0.08069301396608353\n",
            "Epoch 89, Step 278, Loss: 0.1210809201002121\n",
            "Epoch 89, Step 279, Loss: 0.17265598475933075\n",
            "Epoch 89, Step 280, Loss: 0.105596624314785\n",
            "Epoch 89, Step 281, Loss: 0.14465846121311188\n",
            "Epoch 89, Step 282, Loss: 0.12437016516923904\n",
            "Epoch 89, Step 283, Loss: 0.0653601884841919\n",
            "Epoch 89, Step 284, Loss: 0.19983309507369995\n",
            "Epoch 89, Step 285, Loss: 0.14128488302230835\n",
            "Epoch 89, Step 286, Loss: 0.09252426028251648\n",
            "Epoch 89, Step 287, Loss: 0.1437918245792389\n",
            "Epoch 89, Step 288, Loss: 0.10556013882160187\n",
            "Epoch 89, Step 289, Loss: 0.15961016714572906\n",
            "Epoch 89, Step 290, Loss: 0.04408381134271622\n",
            "Epoch 89, Step 291, Loss: 0.1098320409655571\n",
            "Epoch 89, Step 292, Loss: 0.06806053221225739\n",
            "Epoch 89, Step 293, Loss: 0.06660261005163193\n",
            "Epoch 89, Step 294, Loss: 0.13559089601039886\n",
            "Epoch 89, Step 295, Loss: 0.0808657705783844\n",
            "Epoch 89, Step 296, Loss: 0.06772461533546448\n",
            "Epoch 89, Step 297, Loss: 0.10959997773170471\n",
            "Epoch 89, Step 298, Loss: 0.10621639341115952\n",
            "Epoch 89, Step 299, Loss: 0.130457803606987\n",
            "Epoch 89, Step 300, Loss: 0.10864705592393875\n",
            "Epoch 89, Step 301, Loss: 0.08661151677370071\n",
            "Epoch 89, Step 302, Loss: 0.12132052332162857\n",
            "Epoch 89, Step 303, Loss: 0.19561554491519928\n",
            "Epoch 89, Step 304, Loss: 0.17300432920455933\n",
            "Epoch 89, Step 305, Loss: 0.05019880458712578\n",
            "Epoch 89, Step 306, Loss: 0.17847631871700287\n",
            "Epoch 89, Step 307, Loss: 0.12463127076625824\n",
            "Epoch 89, Step 308, Loss: 0.12573686242103577\n",
            "Epoch 89, Step 309, Loss: 0.14028966426849365\n",
            "Epoch 89, Step 310, Loss: 0.09682030975818634\n",
            "Epoch 89, Step 311, Loss: 0.05506983771920204\n",
            "Epoch 89, Step 312, Loss: 0.14953914284706116\n",
            "Epoch 89 end, avg train loss: 0.11386087525505037\n",
            "Epoch 89 end, avg val loss: 0.36639152525458485, accuracy: 90.51%\n",
            "Epoch 90, Step 0, Loss: 0.09991630166769028\n",
            "Epoch 90, Step 1, Loss: 0.04174797236919403\n",
            "Epoch 90, Step 2, Loss: 0.10089453309774399\n",
            "Epoch 90, Step 3, Loss: 0.10562700033187866\n",
            "Epoch 90, Step 4, Loss: 0.0919976681470871\n",
            "Epoch 90, Step 5, Loss: 0.18200549483299255\n",
            "Epoch 90, Step 6, Loss: 0.0699680969119072\n",
            "Epoch 90, Step 7, Loss: 0.13305562734603882\n",
            "Epoch 90, Step 8, Loss: 0.09275950491428375\n",
            "Epoch 90, Step 9, Loss: 0.15582440793514252\n",
            "Epoch 90, Step 10, Loss: 0.07154353708028793\n",
            "Epoch 90, Step 11, Loss: 0.06442392617464066\n",
            "Epoch 90, Step 12, Loss: 0.09616531431674957\n",
            "Epoch 90, Step 13, Loss: 0.06231432780623436\n",
            "Epoch 90, Step 14, Loss: 0.0647205337882042\n",
            "Epoch 90, Step 15, Loss: 0.08417869359254837\n",
            "Epoch 90, Step 16, Loss: 0.10324739664793015\n",
            "Epoch 90, Step 17, Loss: 0.11553491652011871\n",
            "Epoch 90, Step 18, Loss: 0.09770012646913528\n",
            "Epoch 90, Step 19, Loss: 0.09761542081832886\n",
            "Epoch 90, Step 20, Loss: 0.10745740681886673\n",
            "Epoch 90, Step 21, Loss: 0.05451980605721474\n",
            "Epoch 90, Step 22, Loss: 0.10230051726102829\n",
            "Epoch 90, Step 23, Loss: 0.08844080567359924\n",
            "Epoch 90, Step 24, Loss: 0.0840139389038086\n",
            "Epoch 90, Step 25, Loss: 0.13022369146347046\n",
            "Epoch 90, Step 26, Loss: 0.11866732686758041\n",
            "Epoch 90, Step 27, Loss: 0.10577710717916489\n",
            "Epoch 90, Step 28, Loss: 0.10180190950632095\n",
            "Epoch 90, Step 29, Loss: 0.09793592989444733\n",
            "Epoch 90, Step 30, Loss: 0.10389255732297897\n",
            "Epoch 90, Step 31, Loss: 0.10065963119268417\n",
            "Epoch 90, Step 32, Loss: 0.13794736564159393\n",
            "Epoch 90, Step 33, Loss: 0.09463469684123993\n",
            "Epoch 90, Step 34, Loss: 0.15242037177085876\n",
            "Epoch 90, Step 35, Loss: 0.13864658772945404\n",
            "Epoch 90, Step 36, Loss: 0.08889783173799515\n",
            "Epoch 90, Step 37, Loss: 0.15846453607082367\n",
            "Epoch 90, Step 38, Loss: 0.09287693351507187\n",
            "Epoch 90, Step 39, Loss: 0.12432417273521423\n",
            "Epoch 90, Step 40, Loss: 0.1482754349708557\n",
            "Epoch 90, Step 41, Loss: 0.14884966611862183\n",
            "Epoch 90, Step 42, Loss: 0.08853719383478165\n",
            "Epoch 90, Step 43, Loss: 0.06168028712272644\n",
            "Epoch 90, Step 44, Loss: 0.17178332805633545\n",
            "Epoch 90, Step 45, Loss: 0.08120618760585785\n",
            "Epoch 90, Step 46, Loss: 0.09807797521352768\n",
            "Epoch 90, Step 47, Loss: 0.10776599496603012\n",
            "Epoch 90, Step 48, Loss: 0.12574736773967743\n",
            "Epoch 90, Step 49, Loss: 0.11656498163938522\n",
            "Epoch 90, Step 50, Loss: 0.09151820093393326\n",
            "Epoch 90, Step 51, Loss: 0.09360204637050629\n",
            "Epoch 90, Step 52, Loss: 0.10302740335464478\n",
            "Epoch 90, Step 53, Loss: 0.13658460974693298\n",
            "Epoch 90, Step 54, Loss: 0.11948570609092712\n",
            "Epoch 90, Step 55, Loss: 0.18056322634220123\n",
            "Epoch 90, Step 56, Loss: 0.08412449806928635\n",
            "Epoch 90, Step 57, Loss: 0.07366494089365005\n",
            "Epoch 90, Step 58, Loss: 0.20618364214897156\n",
            "Epoch 90, Step 59, Loss: 0.18900950253009796\n",
            "Epoch 90, Step 60, Loss: 0.1825076788663864\n",
            "Epoch 90, Step 61, Loss: 0.16499920189380646\n",
            "Epoch 90, Step 62, Loss: 0.05830983445048332\n",
            "Epoch 90, Step 63, Loss: 0.06909240782260895\n",
            "Epoch 90, Step 64, Loss: 0.10428601503372192\n",
            "Epoch 90, Step 65, Loss: 0.11023517698049545\n",
            "Epoch 90, Step 66, Loss: 0.04783270135521889\n",
            "Epoch 90, Step 67, Loss: 0.10329154133796692\n",
            "Epoch 90, Step 68, Loss: 0.07407736778259277\n",
            "Epoch 90, Step 69, Loss: 0.08135915547609329\n",
            "Epoch 90, Step 70, Loss: 0.14499424397945404\n",
            "Epoch 90, Step 71, Loss: 0.10317149758338928\n",
            "Epoch 90, Step 72, Loss: 0.1398037075996399\n",
            "Epoch 90, Step 73, Loss: 0.05771208554506302\n",
            "Epoch 90, Step 74, Loss: 0.06133555248379707\n",
            "Epoch 90, Step 75, Loss: 0.05621469020843506\n",
            "Epoch 90, Step 76, Loss: 0.08231242746114731\n",
            "Epoch 90, Step 77, Loss: 0.1201290488243103\n",
            "Epoch 90, Step 78, Loss: 0.06964384764432907\n",
            "Epoch 90, Step 79, Loss: 0.08240021765232086\n",
            "Epoch 90, Step 80, Loss: 0.1511494517326355\n",
            "Epoch 90, Step 81, Loss: 0.0623759999871254\n",
            "Epoch 90, Step 82, Loss: 0.11512259393930435\n",
            "Epoch 90, Step 83, Loss: 0.11930274963378906\n",
            "Epoch 90, Step 84, Loss: 0.10804906487464905\n",
            "Epoch 90, Step 85, Loss: 0.09529166668653488\n",
            "Epoch 90, Step 86, Loss: 0.07290972769260406\n",
            "Epoch 90, Step 87, Loss: 0.11852235347032547\n",
            "Epoch 90, Step 88, Loss: 0.09579239785671234\n",
            "Epoch 90, Step 89, Loss: 0.146295428276062\n",
            "Epoch 90, Step 90, Loss: 0.07133182883262634\n",
            "Epoch 90, Step 91, Loss: 0.2175906002521515\n",
            "Epoch 90, Step 92, Loss: 0.06991592049598694\n",
            "Epoch 90, Step 93, Loss: 0.12087313830852509\n",
            "Epoch 90, Step 94, Loss: 0.08068614453077316\n",
            "Epoch 90, Step 95, Loss: 0.1126643568277359\n",
            "Epoch 90, Step 96, Loss: 0.0797441229224205\n",
            "Epoch 90, Step 97, Loss: 0.10116171091794968\n",
            "Epoch 90, Step 98, Loss: 0.046339426189661026\n",
            "Epoch 90, Step 99, Loss: 0.14882351458072662\n",
            "Epoch 90, Step 100, Loss: 0.13407845795154572\n",
            "Epoch 90, Step 101, Loss: 0.10648889094591141\n",
            "Epoch 90, Step 102, Loss: 0.15015187859535217\n",
            "Epoch 90, Step 103, Loss: 0.0871405377984047\n",
            "Epoch 90, Step 104, Loss: 0.198499396443367\n",
            "Epoch 90, Step 105, Loss: 0.11827978491783142\n",
            "Epoch 90, Step 106, Loss: 0.20368678867816925\n",
            "Epoch 90, Step 107, Loss: 0.04431163892149925\n",
            "Epoch 90, Step 108, Loss: 0.05977831035852432\n",
            "Epoch 90, Step 109, Loss: 0.15475524961948395\n",
            "Epoch 90, Step 110, Loss: 0.09289981424808502\n",
            "Epoch 90, Step 111, Loss: 0.06632905453443527\n",
            "Epoch 90, Step 112, Loss: 0.08922021836042404\n",
            "Epoch 90, Step 113, Loss: 0.20229032635688782\n",
            "Epoch 90, Step 114, Loss: 0.06688455492258072\n",
            "Epoch 90, Step 115, Loss: 0.1483858972787857\n",
            "Epoch 90, Step 116, Loss: 0.10200673341751099\n",
            "Epoch 90, Step 117, Loss: 0.050150640308856964\n",
            "Epoch 90, Step 118, Loss: 0.10781953483819962\n",
            "Epoch 90, Step 119, Loss: 0.14508581161499023\n",
            "Epoch 90, Step 120, Loss: 0.22970883548259735\n",
            "Epoch 90, Step 121, Loss: 0.15608634054660797\n",
            "Epoch 90, Step 122, Loss: 0.07878948003053665\n",
            "Epoch 90, Step 123, Loss: 0.09246975183486938\n",
            "Epoch 90, Step 124, Loss: 0.08087710291147232\n",
            "Epoch 90, Step 125, Loss: 0.1294458657503128\n",
            "Epoch 90, Step 126, Loss: 0.15608015656471252\n",
            "Epoch 90, Step 127, Loss: 0.0711645781993866\n",
            "Epoch 90, Step 128, Loss: 0.14585138857364655\n",
            "Epoch 90, Step 129, Loss: 0.13906323909759521\n",
            "Epoch 90, Step 130, Loss: 0.17737515270709991\n",
            "Epoch 90, Step 131, Loss: 0.1194540411233902\n",
            "Epoch 90, Step 132, Loss: 0.1195489913225174\n",
            "Epoch 90, Step 133, Loss: 0.09071111679077148\n",
            "Epoch 90, Step 134, Loss: 0.10370761901140213\n",
            "Epoch 90, Step 135, Loss: 0.08148426562547684\n",
            "Epoch 90, Step 136, Loss: 0.11958443373441696\n",
            "Epoch 90, Step 137, Loss: 0.09329833835363388\n",
            "Epoch 90, Step 138, Loss: 0.19999416172504425\n",
            "Epoch 90, Step 139, Loss: 0.0883101373910904\n",
            "Epoch 90, Step 140, Loss: 0.06863756477832794\n",
            "Epoch 90, Step 141, Loss: 0.10869631171226501\n",
            "Epoch 90, Step 142, Loss: 0.1436460167169571\n",
            "Epoch 90, Step 143, Loss: 0.0938686802983284\n",
            "Epoch 90, Step 144, Loss: 0.07228820770978928\n",
            "Epoch 90, Step 145, Loss: 0.11851462721824646\n",
            "Epoch 90, Step 146, Loss: 0.07769265025854111\n",
            "Epoch 90, Step 147, Loss: 0.08644517511129379\n",
            "Epoch 90, Step 148, Loss: 0.16516482830047607\n",
            "Epoch 90, Step 149, Loss: 0.10061246901750565\n",
            "Epoch 90, Step 150, Loss: 0.19022254645824432\n",
            "Epoch 90, Step 151, Loss: 0.034560300409793854\n",
            "Epoch 90, Step 152, Loss: 0.12936967611312866\n",
            "Epoch 90, Step 153, Loss: 0.12249705940485\n",
            "Epoch 90, Step 154, Loss: 0.10640951991081238\n",
            "Epoch 90, Step 155, Loss: 0.15677109360694885\n",
            "Epoch 90, Step 156, Loss: 0.11184342205524445\n",
            "Epoch 90, Step 157, Loss: 0.10529344528913498\n",
            "Epoch 90, Step 158, Loss: 0.062191665172576904\n",
            "Epoch 90, Step 159, Loss: 0.12142691016197205\n",
            "Epoch 90, Step 160, Loss: 0.10544086992740631\n",
            "Epoch 90, Step 161, Loss: 0.10947758704423904\n",
            "Epoch 90, Step 162, Loss: 0.17348919808864594\n",
            "Epoch 90, Step 163, Loss: 0.12956389784812927\n",
            "Epoch 90, Step 164, Loss: 0.04766539856791496\n",
            "Epoch 90, Step 165, Loss: 0.12195196747779846\n",
            "Epoch 90, Step 166, Loss: 0.1307893842458725\n",
            "Epoch 90, Step 167, Loss: 0.04937689006328583\n",
            "Epoch 90, Step 168, Loss: 0.12583395838737488\n",
            "Epoch 90, Step 169, Loss: 0.10221070051193237\n",
            "Epoch 90, Step 170, Loss: 0.08786290138959885\n",
            "Epoch 90, Step 171, Loss: 0.11094308644533157\n",
            "Epoch 90, Step 172, Loss: 0.14108233153820038\n",
            "Epoch 90, Step 173, Loss: 0.07005967944860458\n",
            "Epoch 90, Step 174, Loss: 0.08035477250814438\n",
            "Epoch 90, Step 175, Loss: 0.07497148215770721\n",
            "Epoch 90, Step 176, Loss: 0.08848148584365845\n",
            "Epoch 90, Step 177, Loss: 0.1695077270269394\n",
            "Epoch 90, Step 178, Loss: 0.05037699267268181\n",
            "Epoch 90, Step 179, Loss: 0.12983882427215576\n",
            "Epoch 90, Step 180, Loss: 0.20616719126701355\n",
            "Epoch 90, Step 181, Loss: 0.12554699182510376\n",
            "Epoch 90, Step 182, Loss: 0.13099908828735352\n",
            "Epoch 90, Step 183, Loss: 0.07810819149017334\n",
            "Epoch 90, Step 184, Loss: 0.2338045984506607\n",
            "Epoch 90, Step 185, Loss: 0.08809014409780502\n",
            "Epoch 90, Step 186, Loss: 0.057424597442150116\n",
            "Epoch 90, Step 187, Loss: 0.1071099117398262\n",
            "Epoch 90, Step 188, Loss: 0.22343875467777252\n",
            "Epoch 90, Step 189, Loss: 0.05135510489344597\n",
            "Epoch 90, Step 190, Loss: 0.13138894736766815\n",
            "Epoch 90, Step 191, Loss: 0.09124626219272614\n",
            "Epoch 90, Step 192, Loss: 0.11301371455192566\n",
            "Epoch 90, Step 193, Loss: 0.1315128356218338\n",
            "Epoch 90, Step 194, Loss: 0.06736324727535248\n",
            "Epoch 90, Step 195, Loss: 0.10895967483520508\n",
            "Epoch 90, Step 196, Loss: 0.08932776004076004\n",
            "Epoch 90, Step 197, Loss: 0.07928189635276794\n",
            "Epoch 90, Step 198, Loss: 0.14100904762744904\n",
            "Epoch 90, Step 199, Loss: 0.1056264340877533\n",
            "Epoch 90, Step 200, Loss: 0.10994432121515274\n",
            "Epoch 90, Step 201, Loss: 0.06010010465979576\n",
            "Epoch 90, Step 202, Loss: 0.08061744272708893\n",
            "Epoch 90, Step 203, Loss: 0.08630428463220596\n",
            "Epoch 90, Step 204, Loss: 0.2267378568649292\n",
            "Epoch 90, Step 205, Loss: 0.157603457570076\n",
            "Epoch 90, Step 206, Loss: 0.131033793091774\n",
            "Epoch 90, Step 207, Loss: 0.140936940908432\n",
            "Epoch 90, Step 208, Loss: 0.07973841577768326\n",
            "Epoch 90, Step 209, Loss: 0.12805525958538055\n",
            "Epoch 90, Step 210, Loss: 0.07660246640443802\n",
            "Epoch 90, Step 211, Loss: 0.11500324308872223\n",
            "Epoch 90, Step 212, Loss: 0.06865309923887253\n",
            "Epoch 90, Step 213, Loss: 0.09008962661027908\n",
            "Epoch 90, Step 214, Loss: 0.13574308156967163\n",
            "Epoch 90, Step 215, Loss: 0.10040026903152466\n",
            "Epoch 90, Step 216, Loss: 0.14951889216899872\n",
            "Epoch 90, Step 217, Loss: 0.13710233569145203\n",
            "Epoch 90, Step 218, Loss: 0.11640731990337372\n",
            "Epoch 90, Step 219, Loss: 0.15197843313217163\n",
            "Epoch 90, Step 220, Loss: 0.15680931508541107\n",
            "Epoch 90, Step 221, Loss: 0.08244404196739197\n",
            "Epoch 90, Step 222, Loss: 0.16016703844070435\n",
            "Epoch 90, Step 223, Loss: 0.05370482802391052\n",
            "Epoch 90, Step 224, Loss: 0.10366812348365784\n",
            "Epoch 90, Step 225, Loss: 0.08167874068021774\n",
            "Epoch 90, Step 226, Loss: 0.1172211617231369\n",
            "Epoch 90, Step 227, Loss: 0.0407569594681263\n",
            "Epoch 90, Step 228, Loss: 0.04864281788468361\n",
            "Epoch 90, Step 229, Loss: 0.11209212988615036\n",
            "Epoch 90, Step 230, Loss: 0.10000576078891754\n",
            "Epoch 90, Step 231, Loss: 0.12424983084201813\n",
            "Epoch 90, Step 232, Loss: 0.12755437195301056\n",
            "Epoch 90, Step 233, Loss: 0.14754906296730042\n",
            "Epoch 90, Step 234, Loss: 0.1390390247106552\n",
            "Epoch 90, Step 235, Loss: 0.06146937608718872\n",
            "Epoch 90, Step 236, Loss: 0.12753647565841675\n",
            "Epoch 90, Step 237, Loss: 0.10002736747264862\n",
            "Epoch 90, Step 238, Loss: 0.07224135100841522\n",
            "Epoch 90, Step 239, Loss: 0.12387566268444061\n",
            "Epoch 90, Step 240, Loss: 0.18308040499687195\n",
            "Epoch 90, Step 241, Loss: 0.14720571041107178\n",
            "Epoch 90, Step 242, Loss: 0.16386209428310394\n",
            "Epoch 90, Step 243, Loss: 0.14825260639190674\n",
            "Epoch 90, Step 244, Loss: 0.1043517217040062\n",
            "Epoch 90, Step 245, Loss: 0.08925601094961166\n",
            "Epoch 90, Step 246, Loss: 0.054324302822351456\n",
            "Epoch 90, Step 247, Loss: 0.2123892903327942\n",
            "Epoch 90, Step 248, Loss: 0.10025079548358917\n",
            "Epoch 90, Step 249, Loss: 0.04938359931111336\n",
            "Epoch 90, Step 250, Loss: 0.10177077353000641\n",
            "Epoch 90, Step 251, Loss: 0.19945096969604492\n",
            "Epoch 90, Step 252, Loss: 0.1170390173792839\n",
            "Epoch 90, Step 253, Loss: 0.09800630062818527\n",
            "Epoch 90, Step 254, Loss: 0.15104658901691437\n",
            "Epoch 90, Step 255, Loss: 0.11088143289089203\n",
            "Epoch 90, Step 256, Loss: 0.07618186622858047\n",
            "Epoch 90, Step 257, Loss: 0.10472827404737473\n",
            "Epoch 90, Step 258, Loss: 0.09484945237636566\n",
            "Epoch 90, Step 259, Loss: 0.09271123260259628\n",
            "Epoch 90, Step 260, Loss: 0.05940156802535057\n",
            "Epoch 90, Step 261, Loss: 0.14240963757038116\n",
            "Epoch 90, Step 262, Loss: 0.07890407741069794\n",
            "Epoch 90, Step 263, Loss: 0.050112295895814896\n",
            "Epoch 90, Step 264, Loss: 0.08523881435394287\n",
            "Epoch 90, Step 265, Loss: 0.14095579087734222\n",
            "Epoch 90, Step 266, Loss: 0.11677192151546478\n",
            "Epoch 90, Step 267, Loss: 0.0870317742228508\n",
            "Epoch 90, Step 268, Loss: 0.0966171994805336\n",
            "Epoch 90, Step 269, Loss: 0.11558737605810165\n",
            "Epoch 90, Step 270, Loss: 0.0798727348446846\n",
            "Epoch 90, Step 271, Loss: 0.07522924989461899\n",
            "Epoch 90, Step 272, Loss: 0.10938582569360733\n",
            "Epoch 90, Step 273, Loss: 0.13586658239364624\n",
            "Epoch 90, Step 274, Loss: 0.17593567073345184\n",
            "Epoch 90, Step 275, Loss: 0.10730662196874619\n",
            "Epoch 90, Step 276, Loss: 0.06523017585277557\n",
            "Epoch 90, Step 277, Loss: 0.0980909913778305\n",
            "Epoch 90, Step 278, Loss: 0.17110183835029602\n",
            "Epoch 90, Step 279, Loss: 0.1891261339187622\n",
            "Epoch 90, Step 280, Loss: 0.05629347637295723\n",
            "Epoch 90, Step 281, Loss: 0.16280891001224518\n",
            "Epoch 90, Step 282, Loss: 0.11170707643032074\n",
            "Epoch 90, Step 283, Loss: 0.1303614228963852\n",
            "Epoch 90, Step 284, Loss: 0.12425605207681656\n",
            "Epoch 90, Step 285, Loss: 0.12987728416919708\n",
            "Epoch 90, Step 286, Loss: 0.11036134511232376\n",
            "Epoch 90, Step 287, Loss: 0.13401439785957336\n",
            "Epoch 90, Step 288, Loss: 0.23701141774654388\n",
            "Epoch 90, Step 289, Loss: 0.03395802527666092\n",
            "Epoch 90, Step 290, Loss: 0.12810727953910828\n",
            "Epoch 90, Step 291, Loss: 0.07238425314426422\n",
            "Epoch 90, Step 292, Loss: 0.13932207226753235\n",
            "Epoch 90, Step 293, Loss: 0.06099827587604523\n",
            "Epoch 90, Step 294, Loss: 0.11077003926038742\n",
            "Epoch 90, Step 295, Loss: 0.12208713591098785\n",
            "Epoch 90, Step 296, Loss: 0.0892522931098938\n",
            "Epoch 90, Step 297, Loss: 0.06689508259296417\n",
            "Epoch 90, Step 298, Loss: 0.07407964020967484\n",
            "Epoch 90, Step 299, Loss: 0.038351550698280334\n",
            "Epoch 90, Step 300, Loss: 0.07070617377758026\n",
            "Epoch 90, Step 301, Loss: 0.0649852305650711\n",
            "Epoch 90, Step 302, Loss: 0.16928935050964355\n",
            "Epoch 90, Step 303, Loss: 0.1172284483909607\n",
            "Epoch 90, Step 304, Loss: 0.10190024226903915\n",
            "Epoch 90, Step 305, Loss: 0.12071307003498077\n",
            "Epoch 90, Step 306, Loss: 0.11892686784267426\n",
            "Epoch 90, Step 307, Loss: 0.12672127783298492\n",
            "Epoch 90, Step 308, Loss: 0.06468765437602997\n",
            "Epoch 90, Step 309, Loss: 0.20946986973285675\n",
            "Epoch 90, Step 310, Loss: 0.1148478090763092\n",
            "Epoch 90, Step 311, Loss: 0.17313875257968903\n",
            "Epoch 90, Step 312, Loss: 0.189932718873024\n",
            "Epoch 90 end, avg train loss: 0.11119632025401051\n",
            "Epoch 90 end, avg val loss: 0.3726514269784977, accuracy: 90.24%\n",
            "Epoch 91, Step 0, Loss: 0.1701877862215042\n",
            "Epoch 91, Step 1, Loss: 0.08548588305711746\n",
            "Epoch 91, Step 2, Loss: 0.1719212532043457\n",
            "Epoch 91, Step 3, Loss: 0.08304406702518463\n",
            "Epoch 91, Step 4, Loss: 0.06650795042514801\n",
            "Epoch 91, Step 5, Loss: 0.10143832862377167\n",
            "Epoch 91, Step 6, Loss: 0.15038138628005981\n",
            "Epoch 91, Step 7, Loss: 0.1282881796360016\n",
            "Epoch 91, Step 8, Loss: 0.1344873160123825\n",
            "Epoch 91, Step 9, Loss: 0.07253866642713547\n",
            "Epoch 91, Step 10, Loss: 0.183119535446167\n",
            "Epoch 91, Step 11, Loss: 0.11853832006454468\n",
            "Epoch 91, Step 12, Loss: 0.10714297741651535\n",
            "Epoch 91, Step 13, Loss: 0.11872521042823792\n",
            "Epoch 91, Step 14, Loss: 0.09859245270490646\n",
            "Epoch 91, Step 15, Loss: 0.1254381686449051\n",
            "Epoch 91, Step 16, Loss: 0.09541795402765274\n",
            "Epoch 91, Step 17, Loss: 0.13332512974739075\n",
            "Epoch 91, Step 18, Loss: 0.15842470526695251\n",
            "Epoch 91, Step 19, Loss: 0.14399294555187225\n",
            "Epoch 91, Step 20, Loss: 0.08842542767524719\n",
            "Epoch 91, Step 21, Loss: 0.10611096769571304\n",
            "Epoch 91, Step 22, Loss: 0.07454442232847214\n",
            "Epoch 91, Step 23, Loss: 0.07470089197158813\n",
            "Epoch 91, Step 24, Loss: 0.0962143987417221\n",
            "Epoch 91, Step 25, Loss: 0.13132475316524506\n",
            "Epoch 91, Step 26, Loss: 0.13358984887599945\n",
            "Epoch 91, Step 27, Loss: 0.18300054967403412\n",
            "Epoch 91, Step 28, Loss: 0.09096785634756088\n",
            "Epoch 91, Step 29, Loss: 0.15915879607200623\n",
            "Epoch 91, Step 30, Loss: 0.12443284690380096\n",
            "Epoch 91, Step 31, Loss: 0.0798272117972374\n",
            "Epoch 91, Step 32, Loss: 0.1001191958785057\n",
            "Epoch 91, Step 33, Loss: 0.11990352720022202\n",
            "Epoch 91, Step 34, Loss: 0.12217879295349121\n",
            "Epoch 91, Step 35, Loss: 0.1709502637386322\n",
            "Epoch 91, Step 36, Loss: 0.13217833638191223\n",
            "Epoch 91, Step 37, Loss: 0.07962430268526077\n",
            "Epoch 91, Step 38, Loss: 0.08411967009305954\n",
            "Epoch 91, Step 39, Loss: 0.0783701092004776\n",
            "Epoch 91, Step 40, Loss: 0.10523293167352676\n",
            "Epoch 91, Step 41, Loss: 0.1300446093082428\n",
            "Epoch 91, Step 42, Loss: 0.10792180895805359\n",
            "Epoch 91, Step 43, Loss: 0.06703860312700272\n",
            "Epoch 91, Step 44, Loss: 0.057182151824235916\n",
            "Epoch 91, Step 45, Loss: 0.09960804879665375\n",
            "Epoch 91, Step 46, Loss: 0.08169155567884445\n",
            "Epoch 91, Step 47, Loss: 0.07371605932712555\n",
            "Epoch 91, Step 48, Loss: 0.09162522852420807\n",
            "Epoch 91, Step 49, Loss: 0.1086731106042862\n",
            "Epoch 91, Step 50, Loss: 0.17839106917381287\n",
            "Epoch 91, Step 51, Loss: 0.11289718002080917\n",
            "Epoch 91, Step 52, Loss: 0.12197186052799225\n",
            "Epoch 91, Step 53, Loss: 0.12696407735347748\n",
            "Epoch 91, Step 54, Loss: 0.06125124171376228\n",
            "Epoch 91, Step 55, Loss: 0.1733357161283493\n",
            "Epoch 91, Step 56, Loss: 0.06016894802451134\n",
            "Epoch 91, Step 57, Loss: 0.10417955368757248\n",
            "Epoch 91, Step 58, Loss: 0.124158576130867\n",
            "Epoch 91, Step 59, Loss: 0.06835082918405533\n",
            "Epoch 91, Step 60, Loss: 0.11111770570278168\n",
            "Epoch 91, Step 61, Loss: 0.14767010509967804\n",
            "Epoch 91, Step 62, Loss: 0.15880879759788513\n",
            "Epoch 91, Step 63, Loss: 0.14999321103096008\n",
            "Epoch 91, Step 64, Loss: 0.06222246214747429\n",
            "Epoch 91, Step 65, Loss: 0.1498216986656189\n",
            "Epoch 91, Step 66, Loss: 0.13998925685882568\n",
            "Epoch 91, Step 67, Loss: 0.053542520850896835\n",
            "Epoch 91, Step 68, Loss: 0.09319579601287842\n",
            "Epoch 91, Step 69, Loss: 0.13197797536849976\n",
            "Epoch 91, Step 70, Loss: 0.10665334761142731\n",
            "Epoch 91, Step 71, Loss: 0.15842533111572266\n",
            "Epoch 91, Step 72, Loss: 0.08646862208843231\n",
            "Epoch 91, Step 73, Loss: 0.09049875289201736\n",
            "Epoch 91, Step 74, Loss: 0.2214270681142807\n",
            "Epoch 91, Step 75, Loss: 0.08784069865942001\n",
            "Epoch 91, Step 76, Loss: 0.07290807366371155\n",
            "Epoch 91, Step 77, Loss: 0.052748918533325195\n",
            "Epoch 91, Step 78, Loss: 0.143314391374588\n",
            "Epoch 91, Step 79, Loss: 0.09967364370822906\n",
            "Epoch 91, Step 80, Loss: 0.08017677068710327\n",
            "Epoch 91, Step 81, Loss: 0.2194000780582428\n",
            "Epoch 91, Step 82, Loss: 0.09893975406885147\n",
            "Epoch 91, Step 83, Loss: 0.06426899880170822\n",
            "Epoch 91, Step 84, Loss: 0.12812285125255585\n",
            "Epoch 91, Step 85, Loss: 0.18147121369838715\n",
            "Epoch 91, Step 86, Loss: 0.06697949022054672\n",
            "Epoch 91, Step 87, Loss: 0.18428662419319153\n",
            "Epoch 91, Step 88, Loss: 0.10827536135911942\n",
            "Epoch 91, Step 89, Loss: 0.10079978406429291\n",
            "Epoch 91, Step 90, Loss: 0.050616249442100525\n",
            "Epoch 91, Step 91, Loss: 0.07098257541656494\n",
            "Epoch 91, Step 92, Loss: 0.10619931668043137\n",
            "Epoch 91, Step 93, Loss: 0.17475736141204834\n",
            "Epoch 91, Step 94, Loss: 0.19848354160785675\n",
            "Epoch 91, Step 95, Loss: 0.16300706565380096\n",
            "Epoch 91, Step 96, Loss: 0.06598300486803055\n",
            "Epoch 91, Step 97, Loss: 0.20321206748485565\n",
            "Epoch 91, Step 98, Loss: 0.10887246578931808\n",
            "Epoch 91, Step 99, Loss: 0.1263989508152008\n",
            "Epoch 91, Step 100, Loss: 0.14008952677249908\n",
            "Epoch 91, Step 101, Loss: 0.11394436657428741\n",
            "Epoch 91, Step 102, Loss: 0.08318746089935303\n",
            "Epoch 91, Step 103, Loss: 0.1049310564994812\n",
            "Epoch 91, Step 104, Loss: 0.20234739780426025\n",
            "Epoch 91, Step 105, Loss: 0.15501949191093445\n",
            "Epoch 91, Step 106, Loss: 0.12156131118535995\n",
            "Epoch 91, Step 107, Loss: 0.06980989128351212\n",
            "Epoch 91, Step 108, Loss: 0.10769877582788467\n",
            "Epoch 91, Step 109, Loss: 0.19055357575416565\n",
            "Epoch 91, Step 110, Loss: 0.11191599816083908\n",
            "Epoch 91, Step 111, Loss: 0.11541379988193512\n",
            "Epoch 91, Step 112, Loss: 0.18997886776924133\n",
            "Epoch 91, Step 113, Loss: 0.07711994647979736\n",
            "Epoch 91, Step 114, Loss: 0.10463862866163254\n",
            "Epoch 91, Step 115, Loss: 0.08287602663040161\n",
            "Epoch 91, Step 116, Loss: 0.12885436415672302\n",
            "Epoch 91, Step 117, Loss: 0.11888845264911652\n",
            "Epoch 91, Step 118, Loss: 0.08183996379375458\n",
            "Epoch 91, Step 119, Loss: 0.10797096788883209\n",
            "Epoch 91, Step 120, Loss: 0.1302599012851715\n",
            "Epoch 91, Step 121, Loss: 0.031556542962789536\n",
            "Epoch 91, Step 122, Loss: 0.19342917203903198\n",
            "Epoch 91, Step 123, Loss: 0.11826029419898987\n",
            "Epoch 91, Step 124, Loss: 0.07400816679000854\n",
            "Epoch 91, Step 125, Loss: 0.08632112294435501\n",
            "Epoch 91, Step 126, Loss: 0.13419003784656525\n",
            "Epoch 91, Step 127, Loss: 0.12272044271230698\n",
            "Epoch 91, Step 128, Loss: 0.07866106182336807\n",
            "Epoch 91, Step 129, Loss: 0.17560483515262604\n",
            "Epoch 91, Step 130, Loss: 0.07884268462657928\n",
            "Epoch 91, Step 131, Loss: 0.11597826331853867\n",
            "Epoch 91, Step 132, Loss: 0.15116600692272186\n",
            "Epoch 91, Step 133, Loss: 0.14635930955410004\n",
            "Epoch 91, Step 134, Loss: 0.13486820459365845\n",
            "Epoch 91, Step 135, Loss: 0.04553475230932236\n",
            "Epoch 91, Step 136, Loss: 0.1328209489583969\n",
            "Epoch 91, Step 137, Loss: 0.14499054849147797\n",
            "Epoch 91, Step 138, Loss: 0.15488670766353607\n",
            "Epoch 91, Step 139, Loss: 0.12453750520944595\n",
            "Epoch 91, Step 140, Loss: 0.14361120760440826\n",
            "Epoch 91, Step 141, Loss: 0.15435294806957245\n",
            "Epoch 91, Step 142, Loss: 0.13018904626369476\n",
            "Epoch 91, Step 143, Loss: 0.09929180145263672\n",
            "Epoch 91, Step 144, Loss: 0.09457076340913773\n",
            "Epoch 91, Step 145, Loss: 0.14744703471660614\n",
            "Epoch 91, Step 146, Loss: 0.11041240394115448\n",
            "Epoch 91, Step 147, Loss: 0.10866670310497284\n",
            "Epoch 91, Step 148, Loss: 0.06318620592355728\n",
            "Epoch 91, Step 149, Loss: 0.07987912744283676\n",
            "Epoch 91, Step 150, Loss: 0.10437211394309998\n",
            "Epoch 91, Step 151, Loss: 0.07950272411108017\n",
            "Epoch 91, Step 152, Loss: 0.1991214156150818\n",
            "Epoch 91, Step 153, Loss: 0.04455015808343887\n",
            "Epoch 91, Step 154, Loss: 0.11349402368068695\n",
            "Epoch 91, Step 155, Loss: 0.13334894180297852\n",
            "Epoch 91, Step 156, Loss: 0.11938270926475525\n",
            "Epoch 91, Step 157, Loss: 0.08808476477861404\n",
            "Epoch 91, Step 158, Loss: 0.14101247489452362\n",
            "Epoch 91, Step 159, Loss: 0.10978512465953827\n",
            "Epoch 91, Step 160, Loss: 0.09373515844345093\n",
            "Epoch 91, Step 161, Loss: 0.1210482120513916\n",
            "Epoch 91, Step 162, Loss: 0.0843031033873558\n",
            "Epoch 91, Step 163, Loss: 0.1283113807439804\n",
            "Epoch 91, Step 164, Loss: 0.08818980306386948\n",
            "Epoch 91, Step 165, Loss: 0.06340329349040985\n",
            "Epoch 91, Step 166, Loss: 0.05688867345452309\n",
            "Epoch 91, Step 167, Loss: 0.18007943034172058\n",
            "Epoch 91, Step 168, Loss: 0.07145506143569946\n",
            "Epoch 91, Step 169, Loss: 0.13480903208255768\n",
            "Epoch 91, Step 170, Loss: 0.14916355907917023\n",
            "Epoch 91, Step 171, Loss: 0.1264951080083847\n",
            "Epoch 91, Step 172, Loss: 0.11986732482910156\n",
            "Epoch 91, Step 173, Loss: 0.1659558266401291\n",
            "Epoch 91, Step 174, Loss: 0.08532222360372543\n",
            "Epoch 91, Step 175, Loss: 0.08769505470991135\n",
            "Epoch 91, Step 176, Loss: 0.06151364743709564\n",
            "Epoch 91, Step 177, Loss: 0.09897898882627487\n",
            "Epoch 91, Step 178, Loss: 0.08475705236196518\n",
            "Epoch 91, Step 179, Loss: 0.06660144031047821\n",
            "Epoch 91, Step 180, Loss: 0.13561375439167023\n",
            "Epoch 91, Step 181, Loss: 0.051481179893016815\n",
            "Epoch 91, Step 182, Loss: 0.10214526206254959\n",
            "Epoch 91, Step 183, Loss: 0.09064678102731705\n",
            "Epoch 91, Step 184, Loss: 0.09479715675115585\n",
            "Epoch 91, Step 185, Loss: 0.19066010415554047\n",
            "Epoch 91, Step 186, Loss: 0.09628810733556747\n",
            "Epoch 91, Step 187, Loss: 0.13660059869289398\n",
            "Epoch 91, Step 188, Loss: 0.155988872051239\n",
            "Epoch 91, Step 189, Loss: 0.13270340859889984\n",
            "Epoch 91, Step 190, Loss: 0.1871475726366043\n",
            "Epoch 91, Step 191, Loss: 0.19038458168506622\n",
            "Epoch 91, Step 192, Loss: 0.15509183704853058\n",
            "Epoch 91, Step 193, Loss: 0.09145869314670563\n",
            "Epoch 91, Step 194, Loss: 0.09256943315267563\n",
            "Epoch 91, Step 195, Loss: 0.19452834129333496\n",
            "Epoch 91, Step 196, Loss: 0.08775002509355545\n",
            "Epoch 91, Step 197, Loss: 0.07292414456605911\n",
            "Epoch 91, Step 198, Loss: 0.10961265116930008\n",
            "Epoch 91, Step 199, Loss: 0.04767641797661781\n",
            "Epoch 91, Step 200, Loss: 0.07102731615304947\n",
            "Epoch 91, Step 201, Loss: 0.07867530733346939\n",
            "Epoch 91, Step 202, Loss: 0.1111522912979126\n",
            "Epoch 91, Step 203, Loss: 0.15211640298366547\n",
            "Epoch 91, Step 204, Loss: 0.14022132754325867\n",
            "Epoch 91, Step 205, Loss: 0.1357877552509308\n",
            "Epoch 91, Step 206, Loss: 0.05863838642835617\n",
            "Epoch 91, Step 207, Loss: 0.13732542097568512\n",
            "Epoch 91, Step 208, Loss: 0.11782827973365784\n",
            "Epoch 91, Step 209, Loss: 0.16667595505714417\n",
            "Epoch 91, Step 210, Loss: 0.06972339004278183\n",
            "Epoch 91, Step 211, Loss: 0.12898457050323486\n",
            "Epoch 91, Step 212, Loss: 0.05134283006191254\n",
            "Epoch 91, Step 213, Loss: 0.1427288055419922\n",
            "Epoch 91, Step 214, Loss: 0.11989335715770721\n",
            "Epoch 91, Step 215, Loss: 0.1095743477344513\n",
            "Epoch 91, Step 216, Loss: 0.08903878927230835\n",
            "Epoch 91, Step 217, Loss: 0.05227021500468254\n",
            "Epoch 91, Step 218, Loss: 0.12478235363960266\n",
            "Epoch 91, Step 219, Loss: 0.1445823609828949\n",
            "Epoch 91, Step 220, Loss: 0.0855306088924408\n",
            "Epoch 91, Step 221, Loss: 0.052548255771398544\n",
            "Epoch 91, Step 222, Loss: 0.07705163955688477\n",
            "Epoch 91, Step 223, Loss: 0.07486862689256668\n",
            "Epoch 91, Step 224, Loss: 0.057402171194553375\n",
            "Epoch 91, Step 225, Loss: 0.12530885636806488\n",
            "Epoch 91, Step 226, Loss: 0.044215232133865356\n",
            "Epoch 91, Step 227, Loss: 0.0727885365486145\n",
            "Epoch 91, Step 228, Loss: 0.13633383810520172\n",
            "Epoch 91, Step 229, Loss: 0.1021079272031784\n",
            "Epoch 91, Step 230, Loss: 0.2084544152021408\n",
            "Epoch 91, Step 231, Loss: 0.11728666722774506\n",
            "Epoch 91, Step 232, Loss: 0.09363286197185516\n",
            "Epoch 91, Step 233, Loss: 0.11160030215978622\n",
            "Epoch 91, Step 234, Loss: 0.11697791516780853\n",
            "Epoch 91, Step 235, Loss: 0.06568408757448196\n",
            "Epoch 91, Step 236, Loss: 0.11856736987829208\n",
            "Epoch 91, Step 237, Loss: 0.08244088292121887\n",
            "Epoch 91, Step 238, Loss: 0.06630562990903854\n",
            "Epoch 91, Step 239, Loss: 0.09808719903230667\n",
            "Epoch 91, Step 240, Loss: 0.09923648089170456\n",
            "Epoch 91, Step 241, Loss: 0.1741156131029129\n",
            "Epoch 91, Step 242, Loss: 0.14846864342689514\n",
            "Epoch 91, Step 243, Loss: 0.1080016940832138\n",
            "Epoch 91, Step 244, Loss: 0.09201215207576752\n",
            "Epoch 91, Step 245, Loss: 0.05571147799491882\n",
            "Epoch 91, Step 246, Loss: 0.16213493049144745\n",
            "Epoch 91, Step 247, Loss: 0.17549212276935577\n",
            "Epoch 91, Step 248, Loss: 0.09165917336940765\n",
            "Epoch 91, Step 249, Loss: 0.044978633522987366\n",
            "Epoch 91, Step 250, Loss: 0.06254732608795166\n",
            "Epoch 91, Step 251, Loss: 0.15893013775348663\n",
            "Epoch 91, Step 252, Loss: 0.10633969306945801\n",
            "Epoch 91, Step 253, Loss: 0.09245236963033676\n",
            "Epoch 91, Step 254, Loss: 0.09609062224626541\n",
            "Epoch 91, Step 255, Loss: 0.11427534371614456\n",
            "Epoch 91, Step 256, Loss: 0.09092729538679123\n",
            "Epoch 91, Step 257, Loss: 0.1455981284379959\n",
            "Epoch 91, Step 258, Loss: 0.11541633307933807\n",
            "Epoch 91, Step 259, Loss: 0.17149591445922852\n",
            "Epoch 91, Step 260, Loss: 0.05596991255879402\n",
            "Epoch 91, Step 261, Loss: 0.10170716047286987\n",
            "Epoch 91, Step 262, Loss: 0.05987507104873657\n",
            "Epoch 91, Step 263, Loss: 0.05512790009379387\n",
            "Epoch 91, Step 264, Loss: 0.09031649678945541\n",
            "Epoch 91, Step 265, Loss: 0.1148013100028038\n",
            "Epoch 91, Step 266, Loss: 0.0850822702050209\n",
            "Epoch 91, Step 267, Loss: 0.1951209306716919\n",
            "Epoch 91, Step 268, Loss: 0.12285402417182922\n",
            "Epoch 91, Step 269, Loss: 0.17018088698387146\n",
            "Epoch 91, Step 270, Loss: 0.17513832449913025\n",
            "Epoch 91, Step 271, Loss: 0.19702059030532837\n",
            "Epoch 91, Step 272, Loss: 0.07021931558847427\n",
            "Epoch 91, Step 273, Loss: 0.13750168681144714\n",
            "Epoch 91, Step 274, Loss: 0.14766159653663635\n",
            "Epoch 91, Step 275, Loss: 0.16075582802295685\n",
            "Epoch 91, Step 276, Loss: 0.16754062473773956\n",
            "Epoch 91, Step 277, Loss: 0.19372069835662842\n",
            "Epoch 91, Step 278, Loss: 0.24102061986923218\n",
            "Epoch 91, Step 279, Loss: 0.14954064786434174\n",
            "Epoch 91, Step 280, Loss: 0.09135851263999939\n",
            "Epoch 91, Step 281, Loss: 0.11723539978265762\n",
            "Epoch 91, Step 282, Loss: 0.15961450338363647\n",
            "Epoch 91, Step 283, Loss: 0.09520062804222107\n",
            "Epoch 91, Step 284, Loss: 0.1035686805844307\n",
            "Epoch 91, Step 285, Loss: 0.14199313521385193\n",
            "Epoch 91, Step 286, Loss: 0.11856859177350998\n",
            "Epoch 91, Step 287, Loss: 0.07863160222768784\n",
            "Epoch 91, Step 288, Loss: 0.1198883056640625\n",
            "Epoch 91, Step 289, Loss: 0.08894424140453339\n",
            "Epoch 91, Step 290, Loss: 0.19170601665973663\n",
            "Epoch 91, Step 291, Loss: 0.1463494598865509\n",
            "Epoch 91, Step 292, Loss: 0.17212818562984467\n",
            "Epoch 91, Step 293, Loss: 0.05509123578667641\n",
            "Epoch 91, Step 294, Loss: 0.1607891321182251\n",
            "Epoch 91, Step 295, Loss: 0.05776645243167877\n",
            "Epoch 91, Step 296, Loss: 0.06362146139144897\n",
            "Epoch 91, Step 297, Loss: 0.1425476223230362\n",
            "Epoch 91, Step 298, Loss: 0.056974247097969055\n",
            "Epoch 91, Step 299, Loss: 0.10512864589691162\n",
            "Epoch 91, Step 300, Loss: 0.10758250951766968\n",
            "Epoch 91, Step 301, Loss: 0.06755710393190384\n",
            "Epoch 91, Step 302, Loss: 0.15429629385471344\n",
            "Epoch 91, Step 303, Loss: 0.09626001864671707\n",
            "Epoch 91, Step 304, Loss: 0.06097031757235527\n",
            "Epoch 91, Step 305, Loss: 0.1279345601797104\n",
            "Epoch 91, Step 306, Loss: 0.11063489317893982\n",
            "Epoch 91, Step 307, Loss: 0.05798279494047165\n",
            "Epoch 91, Step 308, Loss: 0.09143716096878052\n",
            "Epoch 91, Step 309, Loss: 0.09322754293680191\n",
            "Epoch 91, Step 310, Loss: 0.15190832316875458\n",
            "Epoch 91, Step 311, Loss: 0.17002664506435394\n",
            "Epoch 91, Step 312, Loss: 0.05381838604807854\n",
            "Epoch 91 end, avg train loss: 0.1143787888030465\n",
            "Epoch 91 end, avg val loss: 0.3605396124177695, accuracy: 90.78%\n",
            "Checkpoint saved: checkpoint_91.pth\n",
            "Epoch 92, Step 0, Loss: 0.06985900551080704\n",
            "Epoch 92, Step 1, Loss: 0.05320637673139572\n",
            "Epoch 92, Step 2, Loss: 0.0506267249584198\n",
            "Epoch 92, Step 3, Loss: 0.077382393181324\n",
            "Epoch 92, Step 4, Loss: 0.15268734097480774\n",
            "Epoch 92, Step 5, Loss: 0.1146567091345787\n",
            "Epoch 92, Step 6, Loss: 0.07852144539356232\n",
            "Epoch 92, Step 7, Loss: 0.15896177291870117\n",
            "Epoch 92, Step 8, Loss: 0.059164006263017654\n",
            "Epoch 92, Step 9, Loss: 0.10876742005348206\n",
            "Epoch 92, Step 10, Loss: 0.049066875129938126\n",
            "Epoch 92, Step 11, Loss: 0.13715358078479767\n",
            "Epoch 92, Step 12, Loss: 0.1171814426779747\n",
            "Epoch 92, Step 13, Loss: 0.0378161184489727\n",
            "Epoch 92, Step 14, Loss: 0.1475691944360733\n",
            "Epoch 92, Step 15, Loss: 0.16265760362148285\n",
            "Epoch 92, Step 16, Loss: 0.07771274447441101\n",
            "Epoch 92, Step 17, Loss: 0.07222823798656464\n",
            "Epoch 92, Step 18, Loss: 0.06348079442977905\n",
            "Epoch 92, Step 19, Loss: 0.0856296718120575\n",
            "Epoch 92, Step 20, Loss: 0.12320824712514877\n",
            "Epoch 92, Step 21, Loss: 0.06559525430202484\n",
            "Epoch 92, Step 22, Loss: 0.11536303162574768\n",
            "Epoch 92, Step 23, Loss: 0.08813951909542084\n",
            "Epoch 92, Step 24, Loss: 0.12544497847557068\n",
            "Epoch 92, Step 25, Loss: 0.06475819647312164\n",
            "Epoch 92, Step 26, Loss: 0.0736033245921135\n",
            "Epoch 92, Step 27, Loss: 0.12163451313972473\n",
            "Epoch 92, Step 28, Loss: 0.12356817722320557\n",
            "Epoch 92, Step 29, Loss: 0.08838588744401932\n",
            "Epoch 92, Step 30, Loss: 0.09222452342510223\n",
            "Epoch 92, Step 31, Loss: 0.08077095448970795\n",
            "Epoch 92, Step 32, Loss: 0.13005927205085754\n",
            "Epoch 92, Step 33, Loss: 0.1262405514717102\n",
            "Epoch 92, Step 34, Loss: 0.1486607789993286\n",
            "Epoch 92, Step 35, Loss: 0.117770254611969\n",
            "Epoch 92, Step 36, Loss: 0.15459947288036346\n",
            "Epoch 92, Step 37, Loss: 0.0622774101793766\n",
            "Epoch 92, Step 38, Loss: 0.12580198049545288\n",
            "Epoch 92, Step 39, Loss: 0.09957220405340195\n",
            "Epoch 92, Step 40, Loss: 0.18672549724578857\n",
            "Epoch 92, Step 41, Loss: 0.060817230492830276\n",
            "Epoch 92, Step 42, Loss: 0.05100550130009651\n",
            "Epoch 92, Step 43, Loss: 0.05426507815718651\n",
            "Epoch 92, Step 44, Loss: 0.0571322999894619\n",
            "Epoch 92, Step 45, Loss: 0.12081443518400192\n",
            "Epoch 92, Step 46, Loss: 0.09447965770959854\n",
            "Epoch 92, Step 47, Loss: 0.10690074414014816\n",
            "Epoch 92, Step 48, Loss: 0.06277298182249069\n",
            "Epoch 92, Step 49, Loss: 0.10124063491821289\n",
            "Epoch 92, Step 50, Loss: 0.1272379755973816\n",
            "Epoch 92, Step 51, Loss: 0.09636352956295013\n",
            "Epoch 92, Step 52, Loss: 0.07260386645793915\n",
            "Epoch 92, Step 53, Loss: 0.08680808544158936\n",
            "Epoch 92, Step 54, Loss: 0.04236631467938423\n",
            "Epoch 92, Step 55, Loss: 0.04800353944301605\n",
            "Epoch 92, Step 56, Loss: 0.09302365779876709\n",
            "Epoch 92, Step 57, Loss: 0.14822892844676971\n",
            "Epoch 92, Step 58, Loss: 0.11579235643148422\n",
            "Epoch 92, Step 59, Loss: 0.12454891204833984\n",
            "Epoch 92, Step 60, Loss: 0.0522255077958107\n",
            "Epoch 92, Step 61, Loss: 0.09751629084348679\n",
            "Epoch 92, Step 62, Loss: 0.1399318128824234\n",
            "Epoch 92, Step 63, Loss: 0.06701397895812988\n",
            "Epoch 92, Step 64, Loss: 0.14240722358226776\n",
            "Epoch 92, Step 65, Loss: 0.12110112607479095\n",
            "Epoch 92, Step 66, Loss: 0.09768988937139511\n",
            "Epoch 92, Step 67, Loss: 0.0914825052022934\n",
            "Epoch 92, Step 68, Loss: 0.10700520873069763\n",
            "Epoch 92, Step 69, Loss: 0.11592284590005875\n",
            "Epoch 92, Step 70, Loss: 0.12500058114528656\n",
            "Epoch 92, Step 71, Loss: 0.07002640515565872\n",
            "Epoch 92, Step 72, Loss: 0.10045628249645233\n",
            "Epoch 92, Step 73, Loss: 0.0935194194316864\n",
            "Epoch 92, Step 74, Loss: 0.09218014776706696\n",
            "Epoch 92, Step 75, Loss: 0.0812622532248497\n",
            "Epoch 92, Step 76, Loss: 0.11393103003501892\n",
            "Epoch 92, Step 77, Loss: 0.19782572984695435\n",
            "Epoch 92, Step 78, Loss: 0.06944494694471359\n",
            "Epoch 92, Step 79, Loss: 0.11133422702550888\n",
            "Epoch 92, Step 80, Loss: 0.11580263078212738\n",
            "Epoch 92, Step 81, Loss: 0.1877170205116272\n",
            "Epoch 92, Step 82, Loss: 0.09400420635938644\n",
            "Epoch 92, Step 83, Loss: 0.11131437867879868\n",
            "Epoch 92, Step 84, Loss: 0.09692391753196716\n",
            "Epoch 92, Step 85, Loss: 0.10668160766363144\n",
            "Epoch 92, Step 86, Loss: 0.13076408207416534\n",
            "Epoch 92, Step 87, Loss: 0.12525130808353424\n",
            "Epoch 92, Step 88, Loss: 0.10736694931983948\n",
            "Epoch 92, Step 89, Loss: 0.08228457719087601\n",
            "Epoch 92, Step 90, Loss: 0.1123325526714325\n",
            "Epoch 92, Step 91, Loss: 0.11210556328296661\n",
            "Epoch 92, Step 92, Loss: 0.08333174884319305\n",
            "Epoch 92, Step 93, Loss: 0.11097574979066849\n",
            "Epoch 92, Step 94, Loss: 0.0543062761425972\n",
            "Epoch 92, Step 95, Loss: 0.14741237461566925\n",
            "Epoch 92, Step 96, Loss: 0.12597249448299408\n",
            "Epoch 92, Step 97, Loss: 0.0726541131734848\n",
            "Epoch 92, Step 98, Loss: 0.13674844801425934\n",
            "Epoch 92, Step 99, Loss: 0.13238567113876343\n",
            "Epoch 92, Step 100, Loss: 0.12783688306808472\n",
            "Epoch 92, Step 101, Loss: 0.1273321807384491\n",
            "Epoch 92, Step 102, Loss: 0.11649976670742035\n",
            "Epoch 92, Step 103, Loss: 0.1495593637228012\n",
            "Epoch 92, Step 104, Loss: 0.04830517992377281\n",
            "Epoch 92, Step 105, Loss: 0.07743531465530396\n",
            "Epoch 92, Step 106, Loss: 0.08530130237340927\n",
            "Epoch 92, Step 107, Loss: 0.1602858603000641\n",
            "Epoch 92, Step 108, Loss: 0.09383852034807205\n",
            "Epoch 92, Step 109, Loss: 0.10415791720151901\n",
            "Epoch 92, Step 110, Loss: 0.12810437381267548\n",
            "Epoch 92, Step 111, Loss: 0.07024454325437546\n",
            "Epoch 92, Step 112, Loss: 0.17073200643062592\n",
            "Epoch 92, Step 113, Loss: 0.07581865787506104\n",
            "Epoch 92, Step 114, Loss: 0.053405798971652985\n",
            "Epoch 92, Step 115, Loss: 0.24364463984966278\n",
            "Epoch 92, Step 116, Loss: 0.09142991900444031\n",
            "Epoch 92, Step 117, Loss: 0.08777874708175659\n",
            "Epoch 92, Step 118, Loss: 0.034068334847688675\n",
            "Epoch 92, Step 119, Loss: 0.0931553840637207\n",
            "Epoch 92, Step 120, Loss: 0.09484552592039108\n",
            "Epoch 92, Step 121, Loss: 0.13792528212070465\n",
            "Epoch 92, Step 122, Loss: 0.09134462475776672\n",
            "Epoch 92, Step 123, Loss: 0.10999119281768799\n",
            "Epoch 92, Step 124, Loss: 0.09099052101373672\n",
            "Epoch 92, Step 125, Loss: 0.13337695598602295\n",
            "Epoch 92, Step 126, Loss: 0.14464490115642548\n",
            "Epoch 92, Step 127, Loss: 0.0543503500521183\n",
            "Epoch 92, Step 128, Loss: 0.12718607485294342\n",
            "Epoch 92, Step 129, Loss: 0.16349603235721588\n",
            "Epoch 92, Step 130, Loss: 0.11865226924419403\n",
            "Epoch 92, Step 131, Loss: 0.04943808168172836\n",
            "Epoch 92, Step 132, Loss: 0.1033908873796463\n",
            "Epoch 92, Step 133, Loss: 0.09829752892255783\n",
            "Epoch 92, Step 134, Loss: 0.22625748813152313\n",
            "Epoch 92, Step 135, Loss: 0.13921621441841125\n",
            "Epoch 92, Step 136, Loss: 0.11469397693872452\n",
            "Epoch 92, Step 137, Loss: 0.10912462323904037\n",
            "Epoch 92, Step 138, Loss: 0.09932760894298553\n",
            "Epoch 92, Step 139, Loss: 0.06830374896526337\n",
            "Epoch 92, Step 140, Loss: 0.07667127996683121\n",
            "Epoch 92, Step 141, Loss: 0.05872204527258873\n",
            "Epoch 92, Step 142, Loss: 0.09112665802240372\n",
            "Epoch 92, Step 143, Loss: 0.09668385982513428\n",
            "Epoch 92, Step 144, Loss: 0.1455100178718567\n",
            "Epoch 92, Step 145, Loss: 0.1338432878255844\n",
            "Epoch 92, Step 146, Loss: 0.05237632617354393\n",
            "Epoch 92, Step 147, Loss: 0.08520784229040146\n",
            "Epoch 92, Step 148, Loss: 0.05224047973752022\n",
            "Epoch 92, Step 149, Loss: 0.0744008794426918\n",
            "Epoch 92, Step 150, Loss: 0.12686863541603088\n",
            "Epoch 92, Step 151, Loss: 0.10359948873519897\n",
            "Epoch 92, Step 152, Loss: 0.16616342961788177\n",
            "Epoch 92, Step 153, Loss: 0.1864616870880127\n",
            "Epoch 92, Step 154, Loss: 0.1614716500043869\n",
            "Epoch 92, Step 155, Loss: 0.10976025462150574\n",
            "Epoch 92, Step 156, Loss: 0.14018116891384125\n",
            "Epoch 92, Step 157, Loss: 0.09513811767101288\n",
            "Epoch 92, Step 158, Loss: 0.08961738646030426\n",
            "Epoch 92, Step 159, Loss: 0.13414472341537476\n",
            "Epoch 92, Step 160, Loss: 0.09659770876169205\n",
            "Epoch 92, Step 161, Loss: 0.07386936992406845\n",
            "Epoch 92, Step 162, Loss: 0.16091656684875488\n",
            "Epoch 92, Step 163, Loss: 0.04634106159210205\n",
            "Epoch 92, Step 164, Loss: 0.11750168353319168\n",
            "Epoch 92, Step 165, Loss: 0.14903059601783752\n",
            "Epoch 92, Step 166, Loss: 0.07327494770288467\n",
            "Epoch 92, Step 167, Loss: 0.18450088798999786\n",
            "Epoch 92, Step 168, Loss: 0.16320522129535675\n",
            "Epoch 92, Step 169, Loss: 0.10318848490715027\n",
            "Epoch 92, Step 170, Loss: 0.1312493532896042\n",
            "Epoch 92, Step 171, Loss: 0.13777047395706177\n",
            "Epoch 92, Step 172, Loss: 0.20364977419376373\n",
            "Epoch 92, Step 173, Loss: 0.10729723423719406\n",
            "Epoch 92, Step 174, Loss: 0.16988146305084229\n",
            "Epoch 92, Step 175, Loss: 0.08902431279420853\n",
            "Epoch 92, Step 176, Loss: 0.13767899572849274\n",
            "Epoch 92, Step 177, Loss: 0.072390615940094\n",
            "Epoch 92, Step 178, Loss: 0.07313960045576096\n",
            "Epoch 92, Step 179, Loss: 0.08391687273979187\n",
            "Epoch 92, Step 180, Loss: 0.14028368890285492\n",
            "Epoch 92, Step 181, Loss: 0.08533825725317001\n",
            "Epoch 92, Step 182, Loss: 0.09619173407554626\n",
            "Epoch 92, Step 183, Loss: 0.13072876632213593\n",
            "Epoch 92, Step 184, Loss: 0.07148818671703339\n",
            "Epoch 92, Step 185, Loss: 0.17090260982513428\n",
            "Epoch 92, Step 186, Loss: 0.07795743644237518\n",
            "Epoch 92, Step 187, Loss: 0.08478830754756927\n",
            "Epoch 92, Step 188, Loss: 0.13492709398269653\n",
            "Epoch 92, Step 189, Loss: 0.08447666466236115\n",
            "Epoch 92, Step 190, Loss: 0.06709080189466476\n",
            "Epoch 92, Step 191, Loss: 0.08633105456829071\n",
            "Epoch 92, Step 192, Loss: 0.11888118088245392\n",
            "Epoch 92, Step 193, Loss: 0.13321039080619812\n",
            "Epoch 92, Step 194, Loss: 0.08711991459131241\n",
            "Epoch 92, Step 195, Loss: 0.13823015987873077\n",
            "Epoch 92, Step 196, Loss: 0.09012826532125473\n",
            "Epoch 92, Step 197, Loss: 0.08375166356563568\n",
            "Epoch 92, Step 198, Loss: 0.1278124451637268\n",
            "Epoch 92, Step 199, Loss: 0.13396860659122467\n",
            "Epoch 92, Step 200, Loss: 0.11799713969230652\n",
            "Epoch 92, Step 201, Loss: 0.26712802052497864\n",
            "Epoch 92, Step 202, Loss: 0.03981062397360802\n",
            "Epoch 92, Step 203, Loss: 0.10979747027158737\n",
            "Epoch 92, Step 204, Loss: 0.0914708599448204\n",
            "Epoch 92, Step 205, Loss: 0.10255400836467743\n",
            "Epoch 92, Step 206, Loss: 0.08921731263399124\n",
            "Epoch 92, Step 207, Loss: 0.13587655127048492\n",
            "Epoch 92, Step 208, Loss: 0.12223425507545471\n",
            "Epoch 92, Step 209, Loss: 0.14812594652175903\n",
            "Epoch 92, Step 210, Loss: 0.15996576845645905\n",
            "Epoch 92, Step 211, Loss: 0.18455132842063904\n",
            "Epoch 92, Step 212, Loss: 0.172930508852005\n",
            "Epoch 92, Step 213, Loss: 0.07163400202989578\n",
            "Epoch 92, Step 214, Loss: 0.16011743247509003\n",
            "Epoch 92, Step 215, Loss: 0.11376699060201645\n",
            "Epoch 92, Step 216, Loss: 0.13740941882133484\n",
            "Epoch 92, Step 217, Loss: 0.19115419685840607\n",
            "Epoch 92, Step 218, Loss: 0.2035149484872818\n",
            "Epoch 92, Step 219, Loss: 0.11108808219432831\n",
            "Epoch 92, Step 220, Loss: 0.10858054459095001\n",
            "Epoch 92, Step 221, Loss: 0.11684924364089966\n",
            "Epoch 92, Step 222, Loss: 0.16597327589988708\n",
            "Epoch 92, Step 223, Loss: 0.05263421684503555\n",
            "Epoch 92, Step 224, Loss: 0.07533012330532074\n",
            "Epoch 92, Step 225, Loss: 0.09023669362068176\n",
            "Epoch 92, Step 226, Loss: 0.0972471833229065\n",
            "Epoch 92, Step 227, Loss: 0.09298130869865417\n",
            "Epoch 92, Step 228, Loss: 0.12138648331165314\n",
            "Epoch 92, Step 229, Loss: 0.09857653826475143\n",
            "Epoch 92, Step 230, Loss: 0.1017097756266594\n",
            "Epoch 92, Step 231, Loss: 0.11735288053750992\n",
            "Epoch 92, Step 232, Loss: 0.12601087987422943\n",
            "Epoch 92, Step 233, Loss: 0.08723106980323792\n",
            "Epoch 92, Step 234, Loss: 0.140833780169487\n",
            "Epoch 92, Step 235, Loss: 0.08903643488883972\n",
            "Epoch 92, Step 236, Loss: 0.1409975290298462\n",
            "Epoch 92, Step 237, Loss: 0.0978693887591362\n",
            "Epoch 92, Step 238, Loss: 0.09640763700008392\n",
            "Epoch 92, Step 239, Loss: 0.056259289383888245\n",
            "Epoch 92, Step 240, Loss: 0.14089518785476685\n",
            "Epoch 92, Step 241, Loss: 0.16424371302127838\n",
            "Epoch 92, Step 242, Loss: 0.21691863238811493\n",
            "Epoch 92, Step 243, Loss: 0.08289429545402527\n",
            "Epoch 92, Step 244, Loss: 0.1343058943748474\n",
            "Epoch 92, Step 245, Loss: 0.06944066286087036\n",
            "Epoch 92, Step 246, Loss: 0.087933249771595\n",
            "Epoch 92, Step 247, Loss: 0.07657566666603088\n",
            "Epoch 92, Step 248, Loss: 0.12727688252925873\n",
            "Epoch 92, Step 249, Loss: 0.20448046922683716\n",
            "Epoch 92, Step 250, Loss: 0.1195530816912651\n",
            "Epoch 92, Step 251, Loss: 0.08565115183591843\n",
            "Epoch 92, Step 252, Loss: 0.1581391543149948\n",
            "Epoch 92, Step 253, Loss: 0.1401013284921646\n",
            "Epoch 92, Step 254, Loss: 0.15114998817443848\n",
            "Epoch 92, Step 255, Loss: 0.0683012306690216\n",
            "Epoch 92, Step 256, Loss: 0.1090429425239563\n",
            "Epoch 92, Step 257, Loss: 0.11249521374702454\n",
            "Epoch 92, Step 258, Loss: 0.11245285719633102\n",
            "Epoch 92, Step 259, Loss: 0.10593824088573456\n",
            "Epoch 92, Step 260, Loss: 0.07110622525215149\n",
            "Epoch 92, Step 261, Loss: 0.136568084359169\n",
            "Epoch 92, Step 262, Loss: 0.08172274380922318\n",
            "Epoch 92, Step 263, Loss: 0.06591611355543137\n",
            "Epoch 92, Step 264, Loss: 0.07996256649494171\n",
            "Epoch 92, Step 265, Loss: 0.1887636035680771\n",
            "Epoch 92, Step 266, Loss: 0.1556488424539566\n",
            "Epoch 92, Step 267, Loss: 0.15283557772636414\n",
            "Epoch 92, Step 268, Loss: 0.0826844722032547\n",
            "Epoch 92, Step 269, Loss: 0.08301403373479843\n",
            "Epoch 92, Step 270, Loss: 0.07301060855388641\n",
            "Epoch 92, Step 271, Loss: 0.05750100687146187\n",
            "Epoch 92, Step 272, Loss: 0.0867207869887352\n",
            "Epoch 92, Step 273, Loss: 0.10307949036359787\n",
            "Epoch 92, Step 274, Loss: 0.0850176140666008\n",
            "Epoch 92, Step 275, Loss: 0.11453467607498169\n",
            "Epoch 92, Step 276, Loss: 0.14302967488765717\n",
            "Epoch 92, Step 277, Loss: 0.057398874312639236\n",
            "Epoch 92, Step 278, Loss: 0.1909821629524231\n",
            "Epoch 92, Step 279, Loss: 0.08774903416633606\n",
            "Epoch 92, Step 280, Loss: 0.09780070185661316\n",
            "Epoch 92, Step 281, Loss: 0.09487587213516235\n",
            "Epoch 92, Step 282, Loss: 0.12479652464389801\n",
            "Epoch 92, Step 283, Loss: 0.18844375014305115\n",
            "Epoch 92, Step 284, Loss: 0.14040498435497284\n",
            "Epoch 92, Step 285, Loss: 0.12014380097389221\n",
            "Epoch 92, Step 286, Loss: 0.0720500722527504\n",
            "Epoch 92, Step 287, Loss: 0.1012871116399765\n",
            "Epoch 92, Step 288, Loss: 0.11711777746677399\n",
            "Epoch 92, Step 289, Loss: 0.08923674374818802\n",
            "Epoch 92, Step 290, Loss: 0.08318208158016205\n",
            "Epoch 92, Step 291, Loss: 0.09947539865970612\n",
            "Epoch 92, Step 292, Loss: 0.15682870149612427\n",
            "Epoch 92, Step 293, Loss: 0.054036226123571396\n",
            "Epoch 92, Step 294, Loss: 0.08533916622400284\n",
            "Epoch 92, Step 295, Loss: 0.10818725824356079\n",
            "Epoch 92, Step 296, Loss: 0.1500219702720642\n",
            "Epoch 92, Step 297, Loss: 0.1381407082080841\n",
            "Epoch 92, Step 298, Loss: 0.053446751087903976\n",
            "Epoch 92, Step 299, Loss: 0.15099288523197174\n",
            "Epoch 92, Step 300, Loss: 0.07418345659971237\n",
            "Epoch 92, Step 301, Loss: 0.16297641396522522\n",
            "Epoch 92, Step 302, Loss: 0.11856363713741302\n",
            "Epoch 92, Step 303, Loss: 0.07087676972150803\n",
            "Epoch 92, Step 304, Loss: 0.09056233614683151\n",
            "Epoch 92, Step 305, Loss: 0.12519244849681854\n",
            "Epoch 92, Step 306, Loss: 0.10008853673934937\n",
            "Epoch 92, Step 307, Loss: 0.14798477292060852\n",
            "Epoch 92, Step 308, Loss: 0.11213517189025879\n",
            "Epoch 92, Step 309, Loss: 0.06706570833921432\n",
            "Epoch 92, Step 310, Loss: 0.15040156245231628\n",
            "Epoch 92, Step 311, Loss: 0.1847347617149353\n",
            "Epoch 92, Step 312, Loss: 0.11511169373989105\n",
            "Epoch 92 end, avg train loss: 0.10994675236578566\n",
            "Epoch 92 end, avg val loss: 0.37148193725608764, accuracy: 90.25%\n",
            "Epoch 93, Step 0, Loss: 0.06710536777973175\n",
            "Epoch 93, Step 1, Loss: 0.11043066531419754\n",
            "Epoch 93, Step 2, Loss: 0.1535944789648056\n",
            "Epoch 93, Step 3, Loss: 0.07049024105072021\n",
            "Epoch 93, Step 4, Loss: 0.06958337873220444\n",
            "Epoch 93, Step 5, Loss: 0.04881938919425011\n",
            "Epoch 93, Step 6, Loss: 0.11108417809009552\n",
            "Epoch 93, Step 7, Loss: 0.08636917918920517\n",
            "Epoch 93, Step 8, Loss: 0.07235635817050934\n",
            "Epoch 93, Step 9, Loss: 0.12094875425100327\n",
            "Epoch 93, Step 10, Loss: 0.12450803071260452\n",
            "Epoch 93, Step 11, Loss: 0.16364213824272156\n",
            "Epoch 93, Step 12, Loss: 0.041986096650362015\n",
            "Epoch 93, Step 13, Loss: 0.09040527790784836\n",
            "Epoch 93, Step 14, Loss: 0.1079787090420723\n",
            "Epoch 93, Step 15, Loss: 0.18895943462848663\n",
            "Epoch 93, Step 16, Loss: 0.08940232545137405\n",
            "Epoch 93, Step 17, Loss: 0.07207845896482468\n",
            "Epoch 93, Step 18, Loss: 0.08908673375844955\n",
            "Epoch 93, Step 19, Loss: 0.1186118945479393\n",
            "Epoch 93, Step 20, Loss: 0.131278857588768\n",
            "Epoch 93, Step 21, Loss: 0.04228054732084274\n",
            "Epoch 93, Step 22, Loss: 0.0816425159573555\n",
            "Epoch 93, Step 23, Loss: 0.08545899391174316\n",
            "Epoch 93, Step 24, Loss: 0.247122123837471\n",
            "Epoch 93, Step 25, Loss: 0.08844156563282013\n",
            "Epoch 93, Step 26, Loss: 0.08221957832574844\n",
            "Epoch 93, Step 27, Loss: 0.10940296202898026\n",
            "Epoch 93, Step 28, Loss: 0.07788627594709396\n",
            "Epoch 93, Step 29, Loss: 0.14369520545005798\n",
            "Epoch 93, Step 30, Loss: 0.15605366230010986\n",
            "Epoch 93, Step 31, Loss: 0.07530482858419418\n",
            "Epoch 93, Step 32, Loss: 0.05491231754422188\n",
            "Epoch 93, Step 33, Loss: 0.09715273976325989\n",
            "Epoch 93, Step 34, Loss: 0.09658347070217133\n",
            "Epoch 93, Step 35, Loss: 0.10084810853004456\n",
            "Epoch 93, Step 36, Loss: 0.10176157206296921\n",
            "Epoch 93, Step 37, Loss: 0.11463616788387299\n",
            "Epoch 93, Step 38, Loss: 0.043292030692100525\n",
            "Epoch 93, Step 39, Loss: 0.1229463517665863\n",
            "Epoch 93, Step 40, Loss: 0.08384151756763458\n",
            "Epoch 93, Step 41, Loss: 0.08397766202688217\n",
            "Epoch 93, Step 42, Loss: 0.14070940017700195\n",
            "Epoch 93, Step 43, Loss: 0.06053031235933304\n",
            "Epoch 93, Step 44, Loss: 0.11784861981868744\n",
            "Epoch 93, Step 45, Loss: 0.08046240359544754\n",
            "Epoch 93, Step 46, Loss: 0.06591695547103882\n",
            "Epoch 93, Step 47, Loss: 0.05111660063266754\n",
            "Epoch 93, Step 48, Loss: 0.11522480100393295\n",
            "Epoch 93, Step 49, Loss: 0.12154647707939148\n",
            "Epoch 93, Step 50, Loss: 0.09601147472858429\n",
            "Epoch 93, Step 51, Loss: 0.06274160742759705\n",
            "Epoch 93, Step 52, Loss: 0.056676823645830154\n",
            "Epoch 93, Step 53, Loss: 0.12442775070667267\n",
            "Epoch 93, Step 54, Loss: 0.14412030577659607\n",
            "Epoch 93, Step 55, Loss: 0.054447807371616364\n",
            "Epoch 93, Step 56, Loss: 0.09084851294755936\n",
            "Epoch 93, Step 57, Loss: 0.1534540057182312\n",
            "Epoch 93, Step 58, Loss: 0.14930200576782227\n",
            "Epoch 93, Step 59, Loss: 0.0531911626458168\n",
            "Epoch 93, Step 60, Loss: 0.17569077014923096\n",
            "Epoch 93, Step 61, Loss: 0.03123864345252514\n",
            "Epoch 93, Step 62, Loss: 0.10538472980260849\n",
            "Epoch 93, Step 63, Loss: 0.0927359089255333\n",
            "Epoch 93, Step 64, Loss: 0.061687543988227844\n",
            "Epoch 93, Step 65, Loss: 0.10518213361501694\n",
            "Epoch 93, Step 66, Loss: 0.11295907199382782\n",
            "Epoch 93, Step 67, Loss: 0.06223536282777786\n",
            "Epoch 93, Step 68, Loss: 0.06936975568532944\n",
            "Epoch 93, Step 69, Loss: 0.09655293822288513\n",
            "Epoch 93, Step 70, Loss: 0.09402365237474442\n",
            "Epoch 93, Step 71, Loss: 0.10702306777238846\n",
            "Epoch 93, Step 72, Loss: 0.16038744151592255\n",
            "Epoch 93, Step 73, Loss: 0.12214697152376175\n",
            "Epoch 93, Step 74, Loss: 0.05541941896080971\n",
            "Epoch 93, Step 75, Loss: 0.1243930384516716\n",
            "Epoch 93, Step 76, Loss: 0.14888939261436462\n",
            "Epoch 93, Step 77, Loss: 0.07761097699403763\n",
            "Epoch 93, Step 78, Loss: 0.08863407373428345\n",
            "Epoch 93, Step 79, Loss: 0.11927320808172226\n",
            "Epoch 93, Step 80, Loss: 0.13431711494922638\n",
            "Epoch 93, Step 81, Loss: 0.08889202028512955\n",
            "Epoch 93, Step 82, Loss: 0.08197683840990067\n",
            "Epoch 93, Step 83, Loss: 0.032980091869831085\n",
            "Epoch 93, Step 84, Loss: 0.07138018310070038\n",
            "Epoch 93, Step 85, Loss: 0.11519786715507507\n",
            "Epoch 93, Step 86, Loss: 0.08972232043743134\n",
            "Epoch 93, Step 87, Loss: 0.08055268973112106\n",
            "Epoch 93, Step 88, Loss: 0.14630275964736938\n",
            "Epoch 93, Step 89, Loss: 0.10116304457187653\n",
            "Epoch 93, Step 90, Loss: 0.07091520726680756\n",
            "Epoch 93, Step 91, Loss: 0.17550216615200043\n",
            "Epoch 93, Step 92, Loss: 0.08784503489732742\n",
            "Epoch 93, Step 93, Loss: 0.07692206650972366\n",
            "Epoch 93, Step 94, Loss: 0.12319808453321457\n",
            "Epoch 93, Step 95, Loss: 0.1515418291091919\n",
            "Epoch 93, Step 96, Loss: 0.1071229800581932\n",
            "Epoch 93, Step 97, Loss: 0.09808310121297836\n",
            "Epoch 93, Step 98, Loss: 0.05182017758488655\n",
            "Epoch 93, Step 99, Loss: 0.08625150471925735\n",
            "Epoch 93, Step 100, Loss: 0.07703294605016708\n",
            "Epoch 93, Step 101, Loss: 0.0908498540520668\n",
            "Epoch 93, Step 102, Loss: 0.056652043014764786\n",
            "Epoch 93, Step 103, Loss: 0.1714571714401245\n",
            "Epoch 93, Step 104, Loss: 0.09147998690605164\n",
            "Epoch 93, Step 105, Loss: 0.08715375512838364\n",
            "Epoch 93, Step 106, Loss: 0.13560643792152405\n",
            "Epoch 93, Step 107, Loss: 0.152115136384964\n",
            "Epoch 93, Step 108, Loss: 0.11058153957128525\n",
            "Epoch 93, Step 109, Loss: 0.08510491251945496\n",
            "Epoch 93, Step 110, Loss: 0.11490371823310852\n",
            "Epoch 93, Step 111, Loss: 0.12476426362991333\n",
            "Epoch 93, Step 112, Loss: 0.08846953511238098\n",
            "Epoch 93, Step 113, Loss: 0.06427881121635437\n",
            "Epoch 93, Step 114, Loss: 0.1383468061685562\n",
            "Epoch 93, Step 115, Loss: 0.12318189442157745\n",
            "Epoch 93, Step 116, Loss: 0.12054278701543808\n",
            "Epoch 93, Step 117, Loss: 0.10307145118713379\n",
            "Epoch 93, Step 118, Loss: 0.08221972733736038\n",
            "Epoch 93, Step 119, Loss: 0.07260479778051376\n",
            "Epoch 93, Step 120, Loss: 0.1239846795797348\n",
            "Epoch 93, Step 121, Loss: 0.1723456233739853\n",
            "Epoch 93, Step 122, Loss: 0.10004585236310959\n",
            "Epoch 93, Step 123, Loss: 0.058328136801719666\n",
            "Epoch 93, Step 124, Loss: 0.12444634735584259\n",
            "Epoch 93, Step 125, Loss: 0.1143278032541275\n",
            "Epoch 93, Step 126, Loss: 0.05715765058994293\n",
            "Epoch 93, Step 127, Loss: 0.08131378144025803\n",
            "Epoch 93, Step 128, Loss: 0.1257544606924057\n",
            "Epoch 93, Step 129, Loss: 0.06875832378864288\n",
            "Epoch 93, Step 130, Loss: 0.14911098778247833\n",
            "Epoch 93, Step 131, Loss: 0.11281851679086685\n",
            "Epoch 93, Step 132, Loss: 0.051696013659238815\n",
            "Epoch 93, Step 133, Loss: 0.10888924449682236\n",
            "Epoch 93, Step 134, Loss: 0.05640954151749611\n",
            "Epoch 93, Step 135, Loss: 0.07251905649900436\n",
            "Epoch 93, Step 136, Loss: 0.1294921189546585\n",
            "Epoch 93, Step 137, Loss: 0.05075578764081001\n",
            "Epoch 93, Step 138, Loss: 0.11205150932073593\n",
            "Epoch 93, Step 139, Loss: 0.0488937683403492\n",
            "Epoch 93, Step 140, Loss: 0.06036403030157089\n",
            "Epoch 93, Step 141, Loss: 0.09569133818149567\n",
            "Epoch 93, Step 142, Loss: 0.10480426996946335\n",
            "Epoch 93, Step 143, Loss: 0.17150622606277466\n",
            "Epoch 93, Step 144, Loss: 0.06296345591545105\n",
            "Epoch 93, Step 145, Loss: 0.09305854141712189\n",
            "Epoch 93, Step 146, Loss: 0.09854325652122498\n",
            "Epoch 93, Step 147, Loss: 0.07616159319877625\n",
            "Epoch 93, Step 148, Loss: 0.09560969471931458\n",
            "Epoch 93, Step 149, Loss: 0.0829717367887497\n",
            "Epoch 93, Step 150, Loss: 0.11772047728300095\n",
            "Epoch 93, Step 151, Loss: 0.08548543602228165\n",
            "Epoch 93, Step 152, Loss: 0.1096208393573761\n",
            "Epoch 93, Step 153, Loss: 0.0689333826303482\n",
            "Epoch 93, Step 154, Loss: 0.11755865067243576\n",
            "Epoch 93, Step 155, Loss: 0.0833195373415947\n",
            "Epoch 93, Step 156, Loss: 0.13869315385818481\n",
            "Epoch 93, Step 157, Loss: 0.06957091391086578\n",
            "Epoch 93, Step 158, Loss: 0.24633754789829254\n",
            "Epoch 93, Step 159, Loss: 0.07381515949964523\n",
            "Epoch 93, Step 160, Loss: 0.24897949397563934\n",
            "Epoch 93, Step 161, Loss: 0.1495659351348877\n",
            "Epoch 93, Step 162, Loss: 0.07497483491897583\n",
            "Epoch 93, Step 163, Loss: 0.09504859894514084\n",
            "Epoch 93, Step 164, Loss: 0.07405104488134384\n",
            "Epoch 93, Step 165, Loss: 0.06552158296108246\n",
            "Epoch 93, Step 166, Loss: 0.0555078350007534\n",
            "Epoch 93, Step 167, Loss: 0.10620318353176117\n",
            "Epoch 93, Step 168, Loss: 0.21280211210250854\n",
            "Epoch 93, Step 169, Loss: 0.06391627341508865\n",
            "Epoch 93, Step 170, Loss: 0.06034991517663002\n",
            "Epoch 93, Step 171, Loss: 0.10198821127414703\n",
            "Epoch 93, Step 172, Loss: 0.10187080502510071\n",
            "Epoch 93, Step 173, Loss: 0.0356520339846611\n",
            "Epoch 93, Step 174, Loss: 0.08009836822748184\n",
            "Epoch 93, Step 175, Loss: 0.13354313373565674\n",
            "Epoch 93, Step 176, Loss: 0.13622938096523285\n",
            "Epoch 93, Step 177, Loss: 0.03588327392935753\n",
            "Epoch 93, Step 178, Loss: 0.15003950893878937\n",
            "Epoch 93, Step 179, Loss: 0.07826471328735352\n",
            "Epoch 93, Step 180, Loss: 0.08203191310167313\n",
            "Epoch 93, Step 181, Loss: 0.20816011726856232\n",
            "Epoch 93, Step 182, Loss: 0.08404617011547089\n",
            "Epoch 93, Step 183, Loss: 0.07946742326021194\n",
            "Epoch 93, Step 184, Loss: 0.12566734850406647\n",
            "Epoch 93, Step 185, Loss: 0.13471859693527222\n",
            "Epoch 93, Step 186, Loss: 0.2680639326572418\n",
            "Epoch 93, Step 187, Loss: 0.10711008310317993\n",
            "Epoch 93, Step 188, Loss: 0.040158458054065704\n",
            "Epoch 93, Step 189, Loss: 0.07259367406368256\n",
            "Epoch 93, Step 190, Loss: 0.16724619269371033\n",
            "Epoch 93, Step 191, Loss: 0.11189454793930054\n",
            "Epoch 93, Step 192, Loss: 0.10243024677038193\n",
            "Epoch 93, Step 193, Loss: 0.08542979508638382\n",
            "Epoch 93, Step 194, Loss: 0.1327902227640152\n",
            "Epoch 93, Step 195, Loss: 0.08301547914743423\n",
            "Epoch 93, Step 196, Loss: 0.07312841713428497\n",
            "Epoch 93, Step 197, Loss: 0.20997537672519684\n",
            "Epoch 93, Step 198, Loss: 0.02687581442296505\n",
            "Epoch 93, Step 199, Loss: 0.12125395983457565\n",
            "Epoch 93, Step 200, Loss: 0.12390922009944916\n",
            "Epoch 93, Step 201, Loss: 0.057250604033470154\n",
            "Epoch 93, Step 202, Loss: 0.14884565770626068\n",
            "Epoch 93, Step 203, Loss: 0.20277656614780426\n",
            "Epoch 93, Step 204, Loss: 0.11598037928342819\n",
            "Epoch 93, Step 205, Loss: 0.11866477131843567\n",
            "Epoch 93, Step 206, Loss: 0.13378825783729553\n",
            "Epoch 93, Step 207, Loss: 0.10473842918872833\n",
            "Epoch 93, Step 208, Loss: 0.0951257050037384\n",
            "Epoch 93, Step 209, Loss: 0.1826259344816208\n",
            "Epoch 93, Step 210, Loss: 0.12846511602401733\n",
            "Epoch 93, Step 211, Loss: 0.06529980897903442\n",
            "Epoch 93, Step 212, Loss: 0.10526589304208755\n",
            "Epoch 93, Step 213, Loss: 0.06276047974824905\n",
            "Epoch 93, Step 214, Loss: 0.13103394210338593\n",
            "Epoch 93, Step 215, Loss: 0.11507518589496613\n",
            "Epoch 93, Step 216, Loss: 0.057048358023166656\n",
            "Epoch 93, Step 217, Loss: 0.16384534537792206\n",
            "Epoch 93, Step 218, Loss: 0.09908975660800934\n",
            "Epoch 93, Step 219, Loss: 0.13102447986602783\n",
            "Epoch 93, Step 220, Loss: 0.19424980878829956\n",
            "Epoch 93, Step 221, Loss: 0.09607905149459839\n",
            "Epoch 93, Step 222, Loss: 0.07691394537687302\n",
            "Epoch 93, Step 223, Loss: 0.15020278096199036\n",
            "Epoch 93, Step 224, Loss: 0.11349022388458252\n",
            "Epoch 93, Step 225, Loss: 0.13553132116794586\n",
            "Epoch 93, Step 226, Loss: 0.1710364669561386\n",
            "Epoch 93, Step 227, Loss: 0.1420028656721115\n",
            "Epoch 93, Step 228, Loss: 0.03389601409435272\n",
            "Epoch 93, Step 229, Loss: 0.08063223958015442\n",
            "Epoch 93, Step 230, Loss: 0.12071295827627182\n",
            "Epoch 93, Step 231, Loss: 0.13751114904880524\n",
            "Epoch 93, Step 232, Loss: 0.12152950465679169\n",
            "Epoch 93, Step 233, Loss: 0.04756280407309532\n",
            "Epoch 93, Step 234, Loss: 0.1954469382762909\n",
            "Epoch 93, Step 235, Loss: 0.09608186036348343\n",
            "Epoch 93, Step 236, Loss: 0.13533726334571838\n",
            "Epoch 93, Step 237, Loss: 0.12847833335399628\n",
            "Epoch 93, Step 238, Loss: 0.05646152049303055\n",
            "Epoch 93, Step 239, Loss: 0.11832860857248306\n",
            "Epoch 93, Step 240, Loss: 0.05649756267666817\n",
            "Epoch 93, Step 241, Loss: 0.09992830455303192\n",
            "Epoch 93, Step 242, Loss: 0.14008162915706635\n",
            "Epoch 93, Step 243, Loss: 0.08292021602392197\n",
            "Epoch 93, Step 244, Loss: 0.09758562594652176\n",
            "Epoch 93, Step 245, Loss: 0.22803428769111633\n",
            "Epoch 93, Step 246, Loss: 0.10732162743806839\n",
            "Epoch 93, Step 247, Loss: 0.1003156378865242\n",
            "Epoch 93, Step 248, Loss: 0.09910966455936432\n",
            "Epoch 93, Step 249, Loss: 0.1593240350484848\n",
            "Epoch 93, Step 250, Loss: 0.09572771936655045\n",
            "Epoch 93, Step 251, Loss: 0.16431623697280884\n",
            "Epoch 93, Step 252, Loss: 0.09853586554527283\n",
            "Epoch 93, Step 253, Loss: 0.13918782770633698\n",
            "Epoch 93, Step 254, Loss: 0.1321057379245758\n",
            "Epoch 93, Step 255, Loss: 0.1169222965836525\n",
            "Epoch 93, Step 256, Loss: 0.136668398976326\n",
            "Epoch 93, Step 257, Loss: 0.09629197418689728\n",
            "Epoch 93, Step 258, Loss: 0.08887722343206406\n",
            "Epoch 93, Step 259, Loss: 0.05227081850171089\n",
            "Epoch 93, Step 260, Loss: 0.07528983801603317\n",
            "Epoch 93, Step 261, Loss: 0.09929630905389786\n",
            "Epoch 93, Step 262, Loss: 0.020368818193674088\n",
            "Epoch 93, Step 263, Loss: 0.07544996589422226\n",
            "Epoch 93, Step 264, Loss: 0.09883978217840195\n",
            "Epoch 93, Step 265, Loss: 0.16787491738796234\n",
            "Epoch 93, Step 266, Loss: 0.11682230979204178\n",
            "Epoch 93, Step 267, Loss: 0.12299326062202454\n",
            "Epoch 93, Step 268, Loss: 0.07158725708723068\n",
            "Epoch 93, Step 269, Loss: 0.0387413315474987\n",
            "Epoch 93, Step 270, Loss: 0.05262851342558861\n",
            "Epoch 93, Step 271, Loss: 0.1629403680562973\n",
            "Epoch 93, Step 272, Loss: 0.2184843271970749\n",
            "Epoch 93, Step 273, Loss: 0.1339222639799118\n",
            "Epoch 93, Step 274, Loss: 0.07916180789470673\n",
            "Epoch 93, Step 275, Loss: 0.06636162847280502\n",
            "Epoch 93, Step 276, Loss: 0.12225886434316635\n",
            "Epoch 93, Step 277, Loss: 0.14974470436573029\n",
            "Epoch 93, Step 278, Loss: 0.08220742642879486\n",
            "Epoch 93, Step 279, Loss: 0.22152434289455414\n",
            "Epoch 93, Step 280, Loss: 0.13312514126300812\n",
            "Epoch 93, Step 281, Loss: 0.0637255385518074\n",
            "Epoch 93, Step 282, Loss: 0.09872642159461975\n",
            "Epoch 93, Step 283, Loss: 0.09719904512166977\n",
            "Epoch 93, Step 284, Loss: 0.1523347944021225\n",
            "Epoch 93, Step 285, Loss: 0.0689912736415863\n",
            "Epoch 93, Step 286, Loss: 0.1481620818376541\n",
            "Epoch 93, Step 287, Loss: 0.09885717183351517\n",
            "Epoch 93, Step 288, Loss: 0.0962240919470787\n",
            "Epoch 93, Step 289, Loss: 0.12049873173236847\n",
            "Epoch 93, Step 290, Loss: 0.15340468287467957\n",
            "Epoch 93, Step 291, Loss: 0.1059870719909668\n",
            "Epoch 93, Step 292, Loss: 0.13347049057483673\n",
            "Epoch 93, Step 293, Loss: 0.1604854017496109\n",
            "Epoch 93, Step 294, Loss: 0.09614695608615875\n",
            "Epoch 93, Step 295, Loss: 0.16241611540317535\n",
            "Epoch 93, Step 296, Loss: 0.09954563528299332\n",
            "Epoch 93, Step 297, Loss: 0.1342017948627472\n",
            "Epoch 93, Step 298, Loss: 0.0831364095211029\n",
            "Epoch 93, Step 299, Loss: 0.06276774406433105\n",
            "Epoch 93, Step 300, Loss: 0.09973227232694626\n",
            "Epoch 93, Step 301, Loss: 0.047405119985342026\n",
            "Epoch 93, Step 302, Loss: 0.09256183356046677\n",
            "Epoch 93, Step 303, Loss: 0.1094447523355484\n",
            "Epoch 93, Step 304, Loss: 0.1449662744998932\n",
            "Epoch 93, Step 305, Loss: 0.04857839643955231\n",
            "Epoch 93, Step 306, Loss: 0.1358046680688858\n",
            "Epoch 93, Step 307, Loss: 0.08199094235897064\n",
            "Epoch 93, Step 308, Loss: 0.08396294713020325\n",
            "Epoch 93, Step 309, Loss: 0.13729225099086761\n",
            "Epoch 93, Step 310, Loss: 0.08898863196372986\n",
            "Epoch 93, Step 311, Loss: 0.14269310235977173\n",
            "Epoch 93, Step 312, Loss: 0.10332752764225006\n",
            "Epoch 93 end, avg train loss: 0.10583658778248503\n",
            "Epoch 93 end, avg val loss: 0.37327590297253305, accuracy: 90.76%\n",
            "Epoch 94, Step 0, Loss: 0.18875177204608917\n",
            "Epoch 94, Step 1, Loss: 0.13551482558250427\n",
            "Epoch 94, Step 2, Loss: 0.07230280339717865\n",
            "Epoch 94, Step 3, Loss: 0.0517832413315773\n",
            "Epoch 94, Step 4, Loss: 0.04550506919622421\n",
            "Epoch 94, Step 5, Loss: 0.09476437419652939\n",
            "Epoch 94, Step 6, Loss: 0.0554683692753315\n",
            "Epoch 94, Step 7, Loss: 0.14024969935417175\n",
            "Epoch 94, Step 8, Loss: 0.08158557862043381\n",
            "Epoch 94, Step 9, Loss: 0.11937571316957474\n",
            "Epoch 94, Step 10, Loss: 0.03903884440660477\n",
            "Epoch 94, Step 11, Loss: 0.1334685981273651\n",
            "Epoch 94, Step 12, Loss: 0.16178637742996216\n",
            "Epoch 94, Step 13, Loss: 0.08173960447311401\n",
            "Epoch 94, Step 14, Loss: 0.12303563207387924\n",
            "Epoch 94, Step 15, Loss: 0.13897360861301422\n",
            "Epoch 94, Step 16, Loss: 0.13069939613342285\n",
            "Epoch 94, Step 17, Loss: 0.07873223721981049\n",
            "Epoch 94, Step 18, Loss: 0.05806541070342064\n",
            "Epoch 94, Step 19, Loss: 0.09464626014232635\n",
            "Epoch 94, Step 20, Loss: 0.09060338139533997\n",
            "Epoch 94, Step 21, Loss: 0.14072974026203156\n",
            "Epoch 94, Step 22, Loss: 0.09926974773406982\n",
            "Epoch 94, Step 23, Loss: 0.15805025398731232\n",
            "Epoch 94, Step 24, Loss: 0.08024810254573822\n",
            "Epoch 94, Step 25, Loss: 0.11391375213861465\n",
            "Epoch 94, Step 26, Loss: 0.060007065534591675\n",
            "Epoch 94, Step 27, Loss: 0.09147735685110092\n",
            "Epoch 94, Step 28, Loss: 0.09777501970529556\n",
            "Epoch 94, Step 29, Loss: 0.053973663598299026\n",
            "Epoch 94, Step 30, Loss: 0.07954270392656326\n",
            "Epoch 94, Step 31, Loss: 0.09459488838911057\n",
            "Epoch 94, Step 32, Loss: 0.06863942742347717\n",
            "Epoch 94, Step 33, Loss: 0.16493183374404907\n",
            "Epoch 94, Step 34, Loss: 0.16951356828212738\n",
            "Epoch 94, Step 35, Loss: 0.039810724556446075\n",
            "Epoch 94, Step 36, Loss: 0.10319389402866364\n",
            "Epoch 94, Step 37, Loss: 0.13787813484668732\n",
            "Epoch 94, Step 38, Loss: 0.08814746141433716\n",
            "Epoch 94, Step 39, Loss: 0.1250305026769638\n",
            "Epoch 94, Step 40, Loss: 0.07717498391866684\n",
            "Epoch 94, Step 41, Loss: 0.09388487040996552\n",
            "Epoch 94, Step 42, Loss: 0.1023651733994484\n",
            "Epoch 94, Step 43, Loss: 0.08391004055738449\n",
            "Epoch 94, Step 44, Loss: 0.16594845056533813\n",
            "Epoch 94, Step 45, Loss: 0.09886099398136139\n",
            "Epoch 94, Step 46, Loss: 0.05651436746120453\n",
            "Epoch 94, Step 47, Loss: 0.04053403064608574\n",
            "Epoch 94, Step 48, Loss: 0.0752386823296547\n",
            "Epoch 94, Step 49, Loss: 0.07859019190073013\n",
            "Epoch 94, Step 50, Loss: 0.08837541937828064\n",
            "Epoch 94, Step 51, Loss: 0.05437520891427994\n",
            "Epoch 94, Step 52, Loss: 0.16556374728679657\n",
            "Epoch 94, Step 53, Loss: 0.06682407855987549\n",
            "Epoch 94, Step 54, Loss: 0.11343331634998322\n",
            "Epoch 94, Step 55, Loss: 0.07941638678312302\n",
            "Epoch 94, Step 56, Loss: 0.08196095377206802\n",
            "Epoch 94, Step 57, Loss: 0.08317351341247559\n",
            "Epoch 94, Step 58, Loss: 0.0854329764842987\n",
            "Epoch 94, Step 59, Loss: 0.0958833247423172\n",
            "Epoch 94, Step 60, Loss: 0.1014765128493309\n",
            "Epoch 94, Step 61, Loss: 0.10505230724811554\n",
            "Epoch 94, Step 62, Loss: 0.09614550322294235\n",
            "Epoch 94, Step 63, Loss: 0.08653812855482101\n",
            "Epoch 94, Step 64, Loss: 0.04711140692234039\n",
            "Epoch 94, Step 65, Loss: 0.17880742251873016\n",
            "Epoch 94, Step 66, Loss: 0.08149543404579163\n",
            "Epoch 94, Step 67, Loss: 0.13109195232391357\n",
            "Epoch 94, Step 68, Loss: 0.10201966017484665\n",
            "Epoch 94, Step 69, Loss: 0.09298110753297806\n",
            "Epoch 94, Step 70, Loss: 0.08291972428560257\n",
            "Epoch 94, Step 71, Loss: 0.06790493428707123\n",
            "Epoch 94, Step 72, Loss: 0.07705795019865036\n",
            "Epoch 94, Step 73, Loss: 0.06254541873931885\n",
            "Epoch 94, Step 74, Loss: 0.12805669009685516\n",
            "Epoch 94, Step 75, Loss: 0.11257952451705933\n",
            "Epoch 94, Step 76, Loss: 0.08299446851015091\n",
            "Epoch 94, Step 77, Loss: 0.07206959277391434\n",
            "Epoch 94, Step 78, Loss: 0.041371725499629974\n",
            "Epoch 94, Step 79, Loss: 0.18907314538955688\n",
            "Epoch 94, Step 80, Loss: 0.20248614251613617\n",
            "Epoch 94, Step 81, Loss: 0.07844866812229156\n",
            "Epoch 94, Step 82, Loss: 0.061979588121175766\n",
            "Epoch 94, Step 83, Loss: 0.08288372308015823\n",
            "Epoch 94, Step 84, Loss: 0.08234987407922745\n",
            "Epoch 94, Step 85, Loss: 0.10318935662508011\n",
            "Epoch 94, Step 86, Loss: 0.08799895644187927\n",
            "Epoch 94, Step 87, Loss: 0.17654402554035187\n",
            "Epoch 94, Step 88, Loss: 0.1146470159292221\n",
            "Epoch 94, Step 89, Loss: 0.06300671398639679\n",
            "Epoch 94, Step 90, Loss: 0.0803951546549797\n",
            "Epoch 94, Step 91, Loss: 0.05143940448760986\n",
            "Epoch 94, Step 92, Loss: 0.05583387240767479\n",
            "Epoch 94, Step 93, Loss: 0.04040103778243065\n",
            "Epoch 94, Step 94, Loss: 0.11033814400434494\n",
            "Epoch 94, Step 95, Loss: 0.07905961573123932\n",
            "Epoch 94, Step 96, Loss: 0.10896731168031693\n",
            "Epoch 94, Step 97, Loss: 0.1233895868062973\n",
            "Epoch 94, Step 98, Loss: 0.154061958193779\n",
            "Epoch 94, Step 99, Loss: 0.11687902361154556\n",
            "Epoch 94, Step 100, Loss: 0.05351753160357475\n",
            "Epoch 94, Step 101, Loss: 0.07555532455444336\n",
            "Epoch 94, Step 102, Loss: 0.13857732713222504\n",
            "Epoch 94, Step 103, Loss: 0.08256980031728745\n",
            "Epoch 94, Step 104, Loss: 0.043816227465867996\n",
            "Epoch 94, Step 105, Loss: 0.07937480509281158\n",
            "Epoch 94, Step 106, Loss: 0.03555016592144966\n",
            "Epoch 94, Step 107, Loss: 0.12769636511802673\n",
            "Epoch 94, Step 108, Loss: 0.06112339720129967\n",
            "Epoch 94, Step 109, Loss: 0.06753531098365784\n",
            "Epoch 94, Step 110, Loss: 0.0351741723716259\n",
            "Epoch 94, Step 111, Loss: 0.07211165130138397\n",
            "Epoch 94, Step 112, Loss: 0.20042556524276733\n",
            "Epoch 94, Step 113, Loss: 0.11683537065982819\n",
            "Epoch 94, Step 114, Loss: 0.07342247664928436\n",
            "Epoch 94, Step 115, Loss: 0.08811545372009277\n",
            "Epoch 94, Step 116, Loss: 0.16124708950519562\n",
            "Epoch 94, Step 117, Loss: 0.0668051466345787\n",
            "Epoch 94, Step 118, Loss: 0.03757333382964134\n",
            "Epoch 94, Step 119, Loss: 0.08404584974050522\n",
            "Epoch 94, Step 120, Loss: 0.0808807983994484\n",
            "Epoch 94, Step 121, Loss: 0.10025311261415482\n",
            "Epoch 94, Step 122, Loss: 0.10402470827102661\n",
            "Epoch 94, Step 123, Loss: 0.15816818177700043\n",
            "Epoch 94, Step 124, Loss: 0.10910293459892273\n",
            "Epoch 94, Step 125, Loss: 0.06896056979894638\n",
            "Epoch 94, Step 126, Loss: 0.028632353991270065\n",
            "Epoch 94, Step 127, Loss: 0.04411422833800316\n",
            "Epoch 94, Step 128, Loss: 0.0994936153292656\n",
            "Epoch 94, Step 129, Loss: 0.06784427911043167\n",
            "Epoch 94, Step 130, Loss: 0.10822172462940216\n",
            "Epoch 94, Step 131, Loss: 0.13683412969112396\n",
            "Epoch 94, Step 132, Loss: 0.08467113226652145\n",
            "Epoch 94, Step 133, Loss: 0.07990681380033493\n",
            "Epoch 94, Step 134, Loss: 0.07297711074352264\n",
            "Epoch 94, Step 135, Loss: 0.056862376630306244\n",
            "Epoch 94, Step 136, Loss: 0.08428595960140228\n",
            "Epoch 94, Step 137, Loss: 0.089598648250103\n",
            "Epoch 94, Step 138, Loss: 0.076947420835495\n",
            "Epoch 94, Step 139, Loss: 0.12525737285614014\n",
            "Epoch 94, Step 140, Loss: 0.09738564491271973\n",
            "Epoch 94, Step 141, Loss: 0.15411162376403809\n",
            "Epoch 94, Step 142, Loss: 0.11437293887138367\n",
            "Epoch 94, Step 143, Loss: 0.051633741706609726\n",
            "Epoch 94, Step 144, Loss: 0.13279803097248077\n",
            "Epoch 94, Step 145, Loss: 0.08214960992336273\n",
            "Epoch 94, Step 146, Loss: 0.08519807457923889\n",
            "Epoch 94, Step 147, Loss: 0.06783177703619003\n",
            "Epoch 94, Step 148, Loss: 0.09541824460029602\n",
            "Epoch 94, Step 149, Loss: 0.15591146051883698\n",
            "Epoch 94, Step 150, Loss: 0.08811850100755692\n",
            "Epoch 94, Step 151, Loss: 0.09622124582529068\n",
            "Epoch 94, Step 152, Loss: 0.049072280526161194\n",
            "Epoch 94, Step 153, Loss: 0.15992127358913422\n",
            "Epoch 94, Step 154, Loss: 0.08972503989934921\n",
            "Epoch 94, Step 155, Loss: 0.10131053626537323\n",
            "Epoch 94, Step 156, Loss: 0.15640097856521606\n",
            "Epoch 94, Step 157, Loss: 0.10203481465578079\n",
            "Epoch 94, Step 158, Loss: 0.0869636982679367\n",
            "Epoch 94, Step 159, Loss: 0.17640170454978943\n",
            "Epoch 94, Step 160, Loss: 0.16500800848007202\n",
            "Epoch 94, Step 161, Loss: 0.07891380786895752\n",
            "Epoch 94, Step 162, Loss: 0.1332758665084839\n",
            "Epoch 94, Step 163, Loss: 0.06748838722705841\n",
            "Epoch 94, Step 164, Loss: 0.07147075980901718\n",
            "Epoch 94, Step 165, Loss: 0.15920662879943848\n",
            "Epoch 94, Step 166, Loss: 0.04583030194044113\n",
            "Epoch 94, Step 167, Loss: 0.06715654581785202\n",
            "Epoch 94, Step 168, Loss: 0.0818384438753128\n",
            "Epoch 94, Step 169, Loss: 0.06277145445346832\n",
            "Epoch 94, Step 170, Loss: 0.07852975279092789\n",
            "Epoch 94, Step 171, Loss: 0.038992296904325485\n",
            "Epoch 94, Step 172, Loss: 0.059054724872112274\n",
            "Epoch 94, Step 173, Loss: 0.1702876091003418\n",
            "Epoch 94, Step 174, Loss: 0.06592348217964172\n",
            "Epoch 94, Step 175, Loss: 0.11735280603170395\n",
            "Epoch 94, Step 176, Loss: 0.09752581268548965\n",
            "Epoch 94, Step 177, Loss: 0.1474052518606186\n",
            "Epoch 94, Step 178, Loss: 0.0800505131483078\n",
            "Epoch 94, Step 179, Loss: 0.16043445467948914\n",
            "Epoch 94, Step 180, Loss: 0.08471072465181351\n",
            "Epoch 94, Step 181, Loss: 0.18767277896404266\n",
            "Epoch 94, Step 182, Loss: 0.046712059527635574\n",
            "Epoch 94, Step 183, Loss: 0.12740808725357056\n",
            "Epoch 94, Step 184, Loss: 0.0904935747385025\n",
            "Epoch 94, Step 185, Loss: 0.10841569304466248\n",
            "Epoch 94, Step 186, Loss: 0.168065145611763\n",
            "Epoch 94, Step 187, Loss: 0.10454635322093964\n",
            "Epoch 94, Step 188, Loss: 0.1407441347837448\n",
            "Epoch 94, Step 189, Loss: 0.07003473490476608\n",
            "Epoch 94, Step 190, Loss: 0.11047593504190445\n",
            "Epoch 94, Step 191, Loss: 0.17207370698451996\n",
            "Epoch 94, Step 192, Loss: 0.1052338033914566\n",
            "Epoch 94, Step 193, Loss: 0.10130176693201065\n",
            "Epoch 94, Step 194, Loss: 0.061357494443655014\n",
            "Epoch 94, Step 195, Loss: 0.044302791357040405\n",
            "Epoch 94, Step 196, Loss: 0.08765319734811783\n",
            "Epoch 94, Step 197, Loss: 0.08087158203125\n",
            "Epoch 94, Step 198, Loss: 0.0861743614077568\n",
            "Epoch 94, Step 199, Loss: 0.1169695183634758\n",
            "Epoch 94, Step 200, Loss: 0.09951455891132355\n",
            "Epoch 94, Step 201, Loss: 0.11231177300214767\n",
            "Epoch 94, Step 202, Loss: 0.12219810485839844\n",
            "Epoch 94, Step 203, Loss: 0.15981999039649963\n",
            "Epoch 94, Step 204, Loss: 0.06437619775533676\n",
            "Epoch 94, Step 205, Loss: 0.12289565801620483\n",
            "Epoch 94, Step 206, Loss: 0.06720014661550522\n",
            "Epoch 94, Step 207, Loss: 0.11803952604532242\n",
            "Epoch 94, Step 208, Loss: 0.09771230071783066\n",
            "Epoch 94, Step 209, Loss: 0.09586448967456818\n",
            "Epoch 94, Step 210, Loss: 0.1190081462264061\n",
            "Epoch 94, Step 211, Loss: 0.08946369588375092\n",
            "Epoch 94, Step 212, Loss: 0.10575610399246216\n",
            "Epoch 94, Step 213, Loss: 0.08808590471744537\n",
            "Epoch 94, Step 214, Loss: 0.1050080880522728\n",
            "Epoch 94, Step 215, Loss: 0.08759123086929321\n",
            "Epoch 94, Step 216, Loss: 0.09893374145030975\n",
            "Epoch 94, Step 217, Loss: 0.08722762018442154\n",
            "Epoch 94, Step 218, Loss: 0.08175751566886902\n",
            "Epoch 94, Step 219, Loss: 0.07091069966554642\n",
            "Epoch 94, Step 220, Loss: 0.08946839720010757\n",
            "Epoch 94, Step 221, Loss: 0.06576452404260635\n",
            "Epoch 94, Step 222, Loss: 0.17008095979690552\n",
            "Epoch 94, Step 223, Loss: 0.2214403599500656\n",
            "Epoch 94, Step 224, Loss: 0.08323264122009277\n",
            "Epoch 94, Step 225, Loss: 0.11726604402065277\n",
            "Epoch 94, Step 226, Loss: 0.11749815195798874\n",
            "Epoch 94, Step 227, Loss: 0.09862392395734787\n",
            "Epoch 94, Step 228, Loss: 0.08994917571544647\n",
            "Epoch 94, Step 229, Loss: 0.07472570985555649\n",
            "Epoch 94, Step 230, Loss: 0.06939934194087982\n",
            "Epoch 94, Step 231, Loss: 0.11444587260484695\n",
            "Epoch 94, Step 232, Loss: 0.12616881728172302\n",
            "Epoch 94, Step 233, Loss: 0.12273499369621277\n",
            "Epoch 94, Step 234, Loss: 0.1463077962398529\n",
            "Epoch 94, Step 235, Loss: 0.05433184280991554\n",
            "Epoch 94, Step 236, Loss: 0.03783411160111427\n",
            "Epoch 94, Step 237, Loss: 0.18265341222286224\n",
            "Epoch 94, Step 238, Loss: 0.10925973951816559\n",
            "Epoch 94, Step 239, Loss: 0.13021104037761688\n",
            "Epoch 94, Step 240, Loss: 0.08062821626663208\n",
            "Epoch 94, Step 241, Loss: 0.1141703724861145\n",
            "Epoch 94, Step 242, Loss: 0.07283946126699448\n",
            "Epoch 94, Step 243, Loss: 0.1011139452457428\n",
            "Epoch 94, Step 244, Loss: 0.08293959498405457\n",
            "Epoch 94, Step 245, Loss: 0.08417346328496933\n",
            "Epoch 94, Step 246, Loss: 0.12431050837039948\n",
            "Epoch 94, Step 247, Loss: 0.1027536615729332\n",
            "Epoch 94, Step 248, Loss: 0.08604562282562256\n",
            "Epoch 94, Step 249, Loss: 0.1240464448928833\n",
            "Epoch 94, Step 250, Loss: 0.06009986996650696\n",
            "Epoch 94, Step 251, Loss: 0.0644727274775505\n",
            "Epoch 94, Step 252, Loss: 0.12558670341968536\n",
            "Epoch 94, Step 253, Loss: 0.08873117715120316\n",
            "Epoch 94, Step 254, Loss: 0.0663265660405159\n",
            "Epoch 94, Step 255, Loss: 0.0764278769493103\n",
            "Epoch 94, Step 256, Loss: 0.047660648822784424\n",
            "Epoch 94, Step 257, Loss: 0.10217022895812988\n",
            "Epoch 94, Step 258, Loss: 0.026239585131406784\n",
            "Epoch 94, Step 259, Loss: 0.12072072923183441\n",
            "Epoch 94, Step 260, Loss: 0.07630157470703125\n",
            "Epoch 94, Step 261, Loss: 0.0801583006978035\n",
            "Epoch 94, Step 262, Loss: 0.14403773844242096\n",
            "Epoch 94, Step 263, Loss: 0.08508388698101044\n",
            "Epoch 94, Step 264, Loss: 0.10817349702119827\n",
            "Epoch 94, Step 265, Loss: 0.12823230028152466\n",
            "Epoch 94, Step 266, Loss: 0.07778424024581909\n",
            "Epoch 94, Step 267, Loss: 0.17881959676742554\n",
            "Epoch 94, Step 268, Loss: 0.0903601348400116\n",
            "Epoch 94, Step 269, Loss: 0.1163414791226387\n",
            "Epoch 94, Step 270, Loss: 0.05349250137805939\n",
            "Epoch 94, Step 271, Loss: 0.14231306314468384\n",
            "Epoch 94, Step 272, Loss: 0.13999199867248535\n",
            "Epoch 94, Step 273, Loss: 0.1534336656332016\n",
            "Epoch 94, Step 274, Loss: 0.12790827453136444\n",
            "Epoch 94, Step 275, Loss: 0.054247576743364334\n",
            "Epoch 94, Step 276, Loss: 0.07774209976196289\n",
            "Epoch 94, Step 277, Loss: 0.13278982043266296\n",
            "Epoch 94, Step 278, Loss: 0.07292186468839645\n",
            "Epoch 94, Step 279, Loss: 0.06417708843946457\n",
            "Epoch 94, Step 280, Loss: 0.06840744614601135\n",
            "Epoch 94, Step 281, Loss: 0.09761818498373032\n",
            "Epoch 94, Step 282, Loss: 0.0813022032380104\n",
            "Epoch 94, Step 283, Loss: 0.1826944500207901\n",
            "Epoch 94, Step 284, Loss: 0.154569610953331\n",
            "Epoch 94, Step 285, Loss: 0.15776753425598145\n",
            "Epoch 94, Step 286, Loss: 0.07679606229066849\n",
            "Epoch 94, Step 287, Loss: 0.11345761269330978\n",
            "Epoch 94, Step 288, Loss: 0.1041804850101471\n",
            "Epoch 94, Step 289, Loss: 0.023289572447538376\n",
            "Epoch 94, Step 290, Loss: 0.14373180270195007\n",
            "Epoch 94, Step 291, Loss: 0.09908472746610641\n",
            "Epoch 94, Step 292, Loss: 0.08486948162317276\n",
            "Epoch 94, Step 293, Loss: 0.13604998588562012\n",
            "Epoch 94, Step 294, Loss: 0.13675175607204437\n",
            "Epoch 94, Step 295, Loss: 0.11508321762084961\n",
            "Epoch 94, Step 296, Loss: 0.12985558807849884\n",
            "Epoch 94, Step 297, Loss: 0.12615585327148438\n",
            "Epoch 94, Step 298, Loss: 0.04761161655187607\n",
            "Epoch 94, Step 299, Loss: 0.24491667747497559\n",
            "Epoch 94, Step 300, Loss: 0.1905418336391449\n",
            "Epoch 94, Step 301, Loss: 0.07239600270986557\n",
            "Epoch 94, Step 302, Loss: 0.09416145086288452\n",
            "Epoch 94, Step 303, Loss: 0.08749992400407791\n",
            "Epoch 94, Step 304, Loss: 0.060572780668735504\n",
            "Epoch 94, Step 305, Loss: 0.07256211340427399\n",
            "Epoch 94, Step 306, Loss: 0.13155394792556763\n",
            "Epoch 94, Step 307, Loss: 0.08277573436498642\n",
            "Epoch 94, Step 308, Loss: 0.041901618242263794\n",
            "Epoch 94, Step 309, Loss: 0.11623045057058334\n",
            "Epoch 94, Step 310, Loss: 0.12879104912281036\n",
            "Epoch 94, Step 311, Loss: 0.13041260838508606\n",
            "Epoch 94, Step 312, Loss: 0.12370242178440094\n",
            "Epoch 94 end, avg train loss: 0.0996064945769767\n",
            "Epoch 94 end, avg val loss: 0.3801437371964507, accuracy: 90.38%\n",
            "Epoch 95, Step 0, Loss: 0.10173028707504272\n",
            "Epoch 95, Step 1, Loss: 0.048997048288583755\n",
            "Epoch 95, Step 2, Loss: 0.07530411332845688\n",
            "Epoch 95, Step 3, Loss: 0.07319784164428711\n",
            "Epoch 95, Step 4, Loss: 0.022101255133748055\n",
            "Epoch 95, Step 5, Loss: 0.11805161088705063\n",
            "Epoch 95, Step 6, Loss: 0.04007728025317192\n",
            "Epoch 95, Step 7, Loss: 0.1424766331911087\n",
            "Epoch 95, Step 8, Loss: 0.1343669444322586\n",
            "Epoch 95, Step 9, Loss: 0.1716223955154419\n",
            "Epoch 95, Step 10, Loss: 0.10706201195716858\n",
            "Epoch 95, Step 11, Loss: 0.08720917999744415\n",
            "Epoch 95, Step 12, Loss: 0.06408561021089554\n",
            "Epoch 95, Step 13, Loss: 0.09581244736909866\n",
            "Epoch 95, Step 14, Loss: 0.08887620270252228\n",
            "Epoch 95, Step 15, Loss: 0.059174973517656326\n",
            "Epoch 95, Step 16, Loss: 0.057453881949186325\n",
            "Epoch 95, Step 17, Loss: 0.0995187908411026\n",
            "Epoch 95, Step 18, Loss: 0.06748785078525543\n",
            "Epoch 95, Step 19, Loss: 0.18015512824058533\n",
            "Epoch 95, Step 20, Loss: 0.09069088101387024\n",
            "Epoch 95, Step 21, Loss: 0.05266132950782776\n",
            "Epoch 95, Step 22, Loss: 0.08591726422309875\n",
            "Epoch 95, Step 23, Loss: 0.08552179485559464\n",
            "Epoch 95, Step 24, Loss: 0.11287293583154678\n",
            "Epoch 95, Step 25, Loss: 0.1487032026052475\n",
            "Epoch 95, Step 26, Loss: 0.1431713104248047\n",
            "Epoch 95, Step 27, Loss: 0.05873366817831993\n",
            "Epoch 95, Step 28, Loss: 0.09208465367555618\n",
            "Epoch 95, Step 29, Loss: 0.06482800841331482\n",
            "Epoch 95, Step 30, Loss: 0.13483195006847382\n",
            "Epoch 95, Step 31, Loss: 0.0512918159365654\n",
            "Epoch 95, Step 32, Loss: 0.0748189315199852\n",
            "Epoch 95, Step 33, Loss: 0.08517588675022125\n",
            "Epoch 95, Step 34, Loss: 0.07433512806892395\n",
            "Epoch 95, Step 35, Loss: 0.09102454036474228\n",
            "Epoch 95, Step 36, Loss: 0.0897572711110115\n",
            "Epoch 95, Step 37, Loss: 0.08256266266107559\n",
            "Epoch 95, Step 38, Loss: 0.20189319550991058\n",
            "Epoch 95, Step 39, Loss: 0.1563328057527542\n",
            "Epoch 95, Step 40, Loss: 0.12266163527965546\n",
            "Epoch 95, Step 41, Loss: 0.12707693874835968\n",
            "Epoch 95, Step 42, Loss: 0.13321523368358612\n",
            "Epoch 95, Step 43, Loss: 0.09597042202949524\n",
            "Epoch 95, Step 44, Loss: 0.07876691967248917\n",
            "Epoch 95, Step 45, Loss: 0.13254472613334656\n",
            "Epoch 95, Step 46, Loss: 0.11649078130722046\n",
            "Epoch 95, Step 47, Loss: 0.17403411865234375\n",
            "Epoch 95, Step 48, Loss: 0.08375765383243561\n",
            "Epoch 95, Step 49, Loss: 0.04181315004825592\n",
            "Epoch 95, Step 50, Loss: 0.0907885953783989\n",
            "Epoch 95, Step 51, Loss: 0.05875559151172638\n",
            "Epoch 95, Step 52, Loss: 0.04923800006508827\n",
            "Epoch 95, Step 53, Loss: 0.06038779020309448\n",
            "Epoch 95, Step 54, Loss: 0.14436586201190948\n",
            "Epoch 95, Step 55, Loss: 0.187386155128479\n",
            "Epoch 95, Step 56, Loss: 0.07544159889221191\n",
            "Epoch 95, Step 57, Loss: 0.09058649092912674\n",
            "Epoch 95, Step 58, Loss: 0.1380728781223297\n",
            "Epoch 95, Step 59, Loss: 0.09872826933860779\n",
            "Epoch 95, Step 60, Loss: 0.08999589085578918\n",
            "Epoch 95, Step 61, Loss: 0.04868771880865097\n",
            "Epoch 95, Step 62, Loss: 0.12431469559669495\n",
            "Epoch 95, Step 63, Loss: 0.23689860105514526\n",
            "Epoch 95, Step 64, Loss: 0.22508175671100616\n",
            "Epoch 95, Step 65, Loss: 0.05430140346288681\n",
            "Epoch 95, Step 66, Loss: 0.14085064828395844\n",
            "Epoch 95, Step 67, Loss: 0.113148532807827\n",
            "Epoch 95, Step 68, Loss: 0.06148641183972359\n",
            "Epoch 95, Step 69, Loss: 0.07100454717874527\n",
            "Epoch 95, Step 70, Loss: 0.043337490409612656\n",
            "Epoch 95, Step 71, Loss: 0.09642098098993301\n",
            "Epoch 95, Step 72, Loss: 0.06119128689169884\n",
            "Epoch 95, Step 73, Loss: 0.0785645842552185\n",
            "Epoch 95, Step 74, Loss: 0.167291522026062\n",
            "Epoch 95, Step 75, Loss: 0.23657478392124176\n",
            "Epoch 95, Step 76, Loss: 0.08323570340871811\n",
            "Epoch 95, Step 77, Loss: 0.0704769715666771\n",
            "Epoch 95, Step 78, Loss: 0.12399395555257797\n",
            "Epoch 95, Step 79, Loss: 0.19445544481277466\n",
            "Epoch 95, Step 80, Loss: 0.0586111843585968\n",
            "Epoch 95, Step 81, Loss: 0.05363611504435539\n",
            "Epoch 95, Step 82, Loss: 0.058112893253564835\n",
            "Epoch 95, Step 83, Loss: 0.09303528815507889\n",
            "Epoch 95, Step 84, Loss: 0.07561580091714859\n",
            "Epoch 95, Step 85, Loss: 0.06327736377716064\n",
            "Epoch 95, Step 86, Loss: 0.13149456679821014\n",
            "Epoch 95, Step 87, Loss: 0.06936163455247879\n",
            "Epoch 95, Step 88, Loss: 0.22686061263084412\n",
            "Epoch 95, Step 89, Loss: 0.09074563533067703\n",
            "Epoch 95, Step 90, Loss: 0.0807005912065506\n",
            "Epoch 95, Step 91, Loss: 0.1124093234539032\n",
            "Epoch 95, Step 92, Loss: 0.10654199123382568\n",
            "Epoch 95, Step 93, Loss: 0.11103375256061554\n",
            "Epoch 95, Step 94, Loss: 0.04810989275574684\n",
            "Epoch 95, Step 95, Loss: 0.09087567031383514\n",
            "Epoch 95, Step 96, Loss: 0.0670691728591919\n",
            "Epoch 95, Step 97, Loss: 0.09750977158546448\n",
            "Epoch 95, Step 98, Loss: 0.10706088691949844\n",
            "Epoch 95, Step 99, Loss: 0.0958138182759285\n",
            "Epoch 95, Step 100, Loss: 0.17217811942100525\n",
            "Epoch 95, Step 101, Loss: 0.1582665592432022\n",
            "Epoch 95, Step 102, Loss: 0.04686996713280678\n",
            "Epoch 95, Step 103, Loss: 0.05291146785020828\n",
            "Epoch 95, Step 104, Loss: 0.13401243090629578\n",
            "Epoch 95, Step 105, Loss: 0.11246291548013687\n",
            "Epoch 95, Step 106, Loss: 0.10476913303136826\n",
            "Epoch 95, Step 107, Loss: 0.09182914346456528\n",
            "Epoch 95, Step 108, Loss: 0.09135113656520844\n",
            "Epoch 95, Step 109, Loss: 0.14795199036598206\n",
            "Epoch 95, Step 110, Loss: 0.0635940209031105\n",
            "Epoch 95, Step 111, Loss: 0.1244875118136406\n",
            "Epoch 95, Step 112, Loss: 0.06597250699996948\n",
            "Epoch 95, Step 113, Loss: 0.04848342761397362\n",
            "Epoch 95, Step 114, Loss: 0.1033739522099495\n",
            "Epoch 95, Step 115, Loss: 0.13844308257102966\n",
            "Epoch 95, Step 116, Loss: 0.14566904306411743\n",
            "Epoch 95, Step 117, Loss: 0.08582320809364319\n",
            "Epoch 95, Step 118, Loss: 0.16110442578792572\n",
            "Epoch 95, Step 119, Loss: 0.12004200369119644\n",
            "Epoch 95, Step 120, Loss: 0.06440576910972595\n",
            "Epoch 95, Step 121, Loss: 0.14285209774971008\n",
            "Epoch 95, Step 122, Loss: 0.07936996221542358\n",
            "Epoch 95, Step 123, Loss: 0.10569358617067337\n",
            "Epoch 95, Step 124, Loss: 0.09366076439619064\n",
            "Epoch 95, Step 125, Loss: 0.11508612334728241\n",
            "Epoch 95, Step 126, Loss: 0.09970132261514664\n",
            "Epoch 95, Step 127, Loss: 0.08564653992652893\n",
            "Epoch 95, Step 128, Loss: 0.056522078812122345\n",
            "Epoch 95, Step 129, Loss: 0.18253400921821594\n",
            "Epoch 95, Step 130, Loss: 0.0301419198513031\n",
            "Epoch 95, Step 131, Loss: 0.07801137119531631\n",
            "Epoch 95, Step 132, Loss: 0.06656700372695923\n",
            "Epoch 95, Step 133, Loss: 0.09733305126428604\n",
            "Epoch 95, Step 134, Loss: 0.10879921168088913\n",
            "Epoch 95, Step 135, Loss: 0.12822459638118744\n",
            "Epoch 95, Step 136, Loss: 0.10598444193601608\n",
            "Epoch 95, Step 137, Loss: 0.148321732878685\n",
            "Epoch 95, Step 138, Loss: 0.09919807314872742\n",
            "Epoch 95, Step 139, Loss: 0.0753212571144104\n",
            "Epoch 95, Step 140, Loss: 0.09672876447439194\n",
            "Epoch 95, Step 141, Loss: 0.12109068781137466\n",
            "Epoch 95, Step 142, Loss: 0.06538404524326324\n",
            "Epoch 95, Step 143, Loss: 0.13002166152000427\n",
            "Epoch 95, Step 144, Loss: 0.07739434391260147\n",
            "Epoch 95, Step 145, Loss: 0.09829916059970856\n",
            "Epoch 95, Step 146, Loss: 0.0932217389345169\n",
            "Epoch 95, Step 147, Loss: 0.1413649320602417\n",
            "Epoch 95, Step 148, Loss: 0.11016890406608582\n",
            "Epoch 95, Step 149, Loss: 0.13528111577033997\n",
            "Epoch 95, Step 150, Loss: 0.21711871027946472\n",
            "Epoch 95, Step 151, Loss: 0.11079360544681549\n",
            "Epoch 95, Step 152, Loss: 0.10079257935285568\n",
            "Epoch 95, Step 153, Loss: 0.0979880541563034\n",
            "Epoch 95, Step 154, Loss: 0.11495362967252731\n",
            "Epoch 95, Step 155, Loss: 0.06732165068387985\n",
            "Epoch 95, Step 156, Loss: 0.09376225620508194\n",
            "Epoch 95, Step 157, Loss: 0.05458826199173927\n",
            "Epoch 95, Step 158, Loss: 0.045362114906311035\n",
            "Epoch 95, Step 159, Loss: 0.0662502869963646\n",
            "Epoch 95, Step 160, Loss: 0.1218063160777092\n",
            "Epoch 95, Step 161, Loss: 0.08365274220705032\n",
            "Epoch 95, Step 162, Loss: 0.05465913936495781\n",
            "Epoch 95, Step 163, Loss: 0.12922252714633942\n",
            "Epoch 95, Step 164, Loss: 0.06444491446018219\n",
            "Epoch 95, Step 165, Loss: 0.12640026211738586\n",
            "Epoch 95, Step 166, Loss: 0.13756732642650604\n",
            "Epoch 95, Step 167, Loss: 0.11256077885627747\n",
            "Epoch 95, Step 168, Loss: 0.16039013862609863\n",
            "Epoch 95, Step 169, Loss: 0.07092011719942093\n",
            "Epoch 95, Step 170, Loss: 0.1264520287513733\n",
            "Epoch 95, Step 171, Loss: 0.14660295844078064\n",
            "Epoch 95, Step 172, Loss: 0.07573460042476654\n",
            "Epoch 95, Step 173, Loss: 0.10801713168621063\n",
            "Epoch 95, Step 174, Loss: 0.17078474164009094\n",
            "Epoch 95, Step 175, Loss: 0.10007184743881226\n",
            "Epoch 95, Step 176, Loss: 0.13541442155838013\n",
            "Epoch 95, Step 177, Loss: 0.0547611303627491\n",
            "Epoch 95, Step 178, Loss: 0.15634667873382568\n",
            "Epoch 95, Step 179, Loss: 0.1186104342341423\n",
            "Epoch 95, Step 180, Loss: 0.09835060685873032\n",
            "Epoch 95, Step 181, Loss: 0.13713058829307556\n",
            "Epoch 95, Step 182, Loss: 0.15603303909301758\n",
            "Epoch 95, Step 183, Loss: 0.07796770334243774\n",
            "Epoch 95, Step 184, Loss: 0.0936010554432869\n",
            "Epoch 95, Step 185, Loss: 0.11078193038702011\n",
            "Epoch 95, Step 186, Loss: 0.08653013408184052\n",
            "Epoch 95, Step 187, Loss: 0.07712668925523758\n",
            "Epoch 95, Step 188, Loss: 0.11581908911466599\n",
            "Epoch 95, Step 189, Loss: 0.042032741010189056\n",
            "Epoch 95, Step 190, Loss: 0.14809362590312958\n",
            "Epoch 95, Step 191, Loss: 0.09180199354887009\n",
            "Epoch 95, Step 192, Loss: 0.0657268837094307\n",
            "Epoch 95, Step 193, Loss: 0.11904171854257584\n",
            "Epoch 95, Step 194, Loss: 0.09924831986427307\n",
            "Epoch 95, Step 195, Loss: 0.1032874584197998\n",
            "Epoch 95, Step 196, Loss: 0.07550898939371109\n",
            "Epoch 95, Step 197, Loss: 0.04980150982737541\n",
            "Epoch 95, Step 198, Loss: 0.20903126895427704\n",
            "Epoch 95, Step 199, Loss: 0.11349675059318542\n",
            "Epoch 95, Step 200, Loss: 0.14151382446289062\n",
            "Epoch 95, Step 201, Loss: 0.1086212620139122\n",
            "Epoch 95, Step 202, Loss: 0.11087336391210556\n",
            "Epoch 95, Step 203, Loss: 0.10461074858903885\n",
            "Epoch 95, Step 204, Loss: 0.12995631992816925\n",
            "Epoch 95, Step 205, Loss: 0.1883118599653244\n",
            "Epoch 95, Step 206, Loss: 0.08302532881498337\n",
            "Epoch 95, Step 207, Loss: 0.08486654609441757\n",
            "Epoch 95, Step 208, Loss: 0.0977950319647789\n",
            "Epoch 95, Step 209, Loss: 0.049931157380342484\n",
            "Epoch 95, Step 210, Loss: 0.08576855808496475\n",
            "Epoch 95, Step 211, Loss: 0.11417869478464127\n",
            "Epoch 95, Step 212, Loss: 0.07118097692728043\n",
            "Epoch 95, Step 213, Loss: 0.14468376338481903\n",
            "Epoch 95, Step 214, Loss: 0.16539104282855988\n",
            "Epoch 95, Step 215, Loss: 0.040676407516002655\n",
            "Epoch 95, Step 216, Loss: 0.09039874374866486\n",
            "Epoch 95, Step 217, Loss: 0.06560788303613663\n",
            "Epoch 95, Step 218, Loss: 0.08394306153059006\n",
            "Epoch 95, Step 219, Loss: 0.05533251538872719\n",
            "Epoch 95, Step 220, Loss: 0.08473537862300873\n",
            "Epoch 95, Step 221, Loss: 0.06339884549379349\n",
            "Epoch 95, Step 222, Loss: 0.18213272094726562\n",
            "Epoch 95, Step 223, Loss: 0.09710399806499481\n",
            "Epoch 95, Step 224, Loss: 0.06887482106685638\n",
            "Epoch 95, Step 225, Loss: 0.09398047626018524\n",
            "Epoch 95, Step 226, Loss: 0.053146809339523315\n",
            "Epoch 95, Step 227, Loss: 0.13538621366024017\n",
            "Epoch 95, Step 228, Loss: 0.05100494623184204\n",
            "Epoch 95, Step 229, Loss: 0.07894739508628845\n",
            "Epoch 95, Step 230, Loss: 0.05187828466296196\n",
            "Epoch 95, Step 231, Loss: 0.07042736560106277\n",
            "Epoch 95, Step 232, Loss: 0.12245924770832062\n",
            "Epoch 95, Step 233, Loss: 0.11627161502838135\n",
            "Epoch 95, Step 234, Loss: 0.08022809773683548\n",
            "Epoch 95, Step 235, Loss: 0.1587827205657959\n",
            "Epoch 95, Step 236, Loss: 0.108570396900177\n",
            "Epoch 95, Step 237, Loss: 0.11198368668556213\n",
            "Epoch 95, Step 238, Loss: 0.14315363764762878\n",
            "Epoch 95, Step 239, Loss: 0.10625942051410675\n",
            "Epoch 95, Step 240, Loss: 0.08490820229053497\n",
            "Epoch 95, Step 241, Loss: 0.12366203218698502\n",
            "Epoch 95, Step 242, Loss: 0.08361027389764786\n",
            "Epoch 95, Step 243, Loss: 0.11780402809381485\n",
            "Epoch 95, Step 244, Loss: 0.10327167809009552\n",
            "Epoch 95, Step 245, Loss: 0.08303271979093552\n",
            "Epoch 95, Step 246, Loss: 0.09452494233846664\n",
            "Epoch 95, Step 247, Loss: 0.07026435434818268\n",
            "Epoch 95, Step 248, Loss: 0.10550285130739212\n",
            "Epoch 95, Step 249, Loss: 0.12305238842964172\n",
            "Epoch 95, Step 250, Loss: 0.11465172469615936\n",
            "Epoch 95, Step 251, Loss: 0.09009435027837753\n",
            "Epoch 95, Step 252, Loss: 0.07131369411945343\n",
            "Epoch 95, Step 253, Loss: 0.10789155960083008\n",
            "Epoch 95, Step 254, Loss: 0.09610143303871155\n",
            "Epoch 95, Step 255, Loss: 0.09704452753067017\n",
            "Epoch 95, Step 256, Loss: 0.07972072809934616\n",
            "Epoch 95, Step 257, Loss: 0.06965833157300949\n",
            "Epoch 95, Step 258, Loss: 0.13138672709465027\n",
            "Epoch 95, Step 259, Loss: 0.09262469410896301\n",
            "Epoch 95, Step 260, Loss: 0.08510764688253403\n",
            "Epoch 95, Step 261, Loss: 0.11369526386260986\n",
            "Epoch 95, Step 262, Loss: 0.11850450932979584\n",
            "Epoch 95, Step 263, Loss: 0.07200315594673157\n",
            "Epoch 95, Step 264, Loss: 0.05955693870782852\n",
            "Epoch 95, Step 265, Loss: 0.11546753346920013\n",
            "Epoch 95, Step 266, Loss: 0.09832081943750381\n",
            "Epoch 95, Step 267, Loss: 0.047782592475414276\n",
            "Epoch 95, Step 268, Loss: 0.04752155765891075\n",
            "Epoch 95, Step 269, Loss: 0.19449517130851746\n",
            "Epoch 95, Step 270, Loss: 0.07886794209480286\n",
            "Epoch 95, Step 271, Loss: 0.15983663499355316\n",
            "Epoch 95, Step 272, Loss: 0.09558875858783722\n",
            "Epoch 95, Step 273, Loss: 0.20954902470111847\n",
            "Epoch 95, Step 274, Loss: 0.10357296466827393\n",
            "Epoch 95, Step 275, Loss: 0.13233865797519684\n",
            "Epoch 95, Step 276, Loss: 0.14436720311641693\n",
            "Epoch 95, Step 277, Loss: 0.061638135462999344\n",
            "Epoch 95, Step 278, Loss: 0.07675208151340485\n",
            "Epoch 95, Step 279, Loss: 0.1886146515607834\n",
            "Epoch 95, Step 280, Loss: 0.10886163264513016\n",
            "Epoch 95, Step 281, Loss: 0.17392754554748535\n",
            "Epoch 95, Step 282, Loss: 0.14216658473014832\n",
            "Epoch 95, Step 283, Loss: 0.06260255724191666\n",
            "Epoch 95, Step 284, Loss: 0.06154073402285576\n",
            "Epoch 95, Step 285, Loss: 0.09585211426019669\n",
            "Epoch 95, Step 286, Loss: 0.1496978998184204\n",
            "Epoch 95, Step 287, Loss: 0.1370490938425064\n",
            "Epoch 95, Step 288, Loss: 0.15757158398628235\n",
            "Epoch 95, Step 289, Loss: 0.05742932856082916\n",
            "Epoch 95, Step 290, Loss: 0.10412663221359253\n",
            "Epoch 95, Step 291, Loss: 0.11305049061775208\n",
            "Epoch 95, Step 292, Loss: 0.05520924553275108\n",
            "Epoch 95, Step 293, Loss: 0.1181330755352974\n",
            "Epoch 95, Step 294, Loss: 0.10214482247829437\n",
            "Epoch 95, Step 295, Loss: 0.14145688712596893\n",
            "Epoch 95, Step 296, Loss: 0.10250768065452576\n",
            "Epoch 95, Step 297, Loss: 0.11377860605716705\n",
            "Epoch 95, Step 298, Loss: 0.09774453938007355\n",
            "Epoch 95, Step 299, Loss: 0.045374754816293716\n",
            "Epoch 95, Step 300, Loss: 0.16917994618415833\n",
            "Epoch 95, Step 301, Loss: 0.07261316478252411\n",
            "Epoch 95, Step 302, Loss: 0.05891057848930359\n",
            "Epoch 95, Step 303, Loss: 0.16714873909950256\n",
            "Epoch 95, Step 304, Loss: 0.12796100974082947\n",
            "Epoch 95, Step 305, Loss: 0.10327623039484024\n",
            "Epoch 95, Step 306, Loss: 0.06038009002804756\n",
            "Epoch 95, Step 307, Loss: 0.054743289947509766\n",
            "Epoch 95, Step 308, Loss: 0.21270805597305298\n",
            "Epoch 95, Step 309, Loss: 0.07791914790868759\n",
            "Epoch 95, Step 310, Loss: 0.07876919955015182\n",
            "Epoch 95, Step 311, Loss: 0.11887295544147491\n",
            "Epoch 95, Step 312, Loss: 0.060871563851833344\n",
            "Epoch 95 end, avg train loss: 0.10290609475689384\n",
            "Epoch 95 end, avg val loss: 0.36523795390834657, accuracy: 90.76%\n",
            "Epoch 96, Step 0, Loss: 0.14016059041023254\n",
            "Epoch 96, Step 1, Loss: 0.050073616206645966\n",
            "Epoch 96, Step 2, Loss: 0.09800468385219574\n",
            "Epoch 96, Step 3, Loss: 0.13665388524532318\n",
            "Epoch 96, Step 4, Loss: 0.09580747038125992\n",
            "Epoch 96, Step 5, Loss: 0.0841120108962059\n",
            "Epoch 96, Step 6, Loss: 0.13631094992160797\n",
            "Epoch 96, Step 7, Loss: 0.12610740959644318\n",
            "Epoch 96, Step 8, Loss: 0.16604644060134888\n",
            "Epoch 96, Step 9, Loss: 0.1356220543384552\n",
            "Epoch 96, Step 10, Loss: 0.02387586608529091\n",
            "Epoch 96, Step 11, Loss: 0.08464591950178146\n",
            "Epoch 96, Step 12, Loss: 0.08880491554737091\n",
            "Epoch 96, Step 13, Loss: 0.13645130395889282\n",
            "Epoch 96, Step 14, Loss: 0.04147176444530487\n",
            "Epoch 96, Step 15, Loss: 0.07839804142713547\n",
            "Epoch 96, Step 16, Loss: 0.09815230965614319\n",
            "Epoch 96, Step 17, Loss: 0.09543323516845703\n",
            "Epoch 96, Step 18, Loss: 0.0613640621304512\n",
            "Epoch 96, Step 19, Loss: 0.10058965533971786\n",
            "Epoch 96, Step 20, Loss: 0.12291412800550461\n",
            "Epoch 96, Step 21, Loss: 0.11642064899206161\n",
            "Epoch 96, Step 22, Loss: 0.055721696466207504\n",
            "Epoch 96, Step 23, Loss: 0.04737167805433273\n",
            "Epoch 96, Step 24, Loss: 0.07183830440044403\n",
            "Epoch 96, Step 25, Loss: 0.1345721036195755\n",
            "Epoch 96, Step 26, Loss: 0.08877331018447876\n",
            "Epoch 96, Step 27, Loss: 0.10598982870578766\n",
            "Epoch 96, Step 28, Loss: 0.09600852429866791\n",
            "Epoch 96, Step 29, Loss: 0.07547061145305634\n",
            "Epoch 96, Step 30, Loss: 0.08401951193809509\n",
            "Epoch 96, Step 31, Loss: 0.056699953973293304\n",
            "Epoch 96, Step 32, Loss: 0.08979266881942749\n",
            "Epoch 96, Step 33, Loss: 0.07114789634943008\n",
            "Epoch 96, Step 34, Loss: 0.10088028758764267\n",
            "Epoch 96, Step 35, Loss: 0.08773628622293472\n",
            "Epoch 96, Step 36, Loss: 0.2542416453361511\n",
            "Epoch 96, Step 37, Loss: 0.11934944242238998\n",
            "Epoch 96, Step 38, Loss: 0.05431215465068817\n",
            "Epoch 96, Step 39, Loss: 0.06637579947710037\n",
            "Epoch 96, Step 40, Loss: 0.09227468818426132\n",
            "Epoch 96, Step 41, Loss: 0.11857388913631439\n",
            "Epoch 96, Step 42, Loss: 0.09926968812942505\n",
            "Epoch 96, Step 43, Loss: 0.10966046899557114\n",
            "Epoch 96, Step 44, Loss: 0.10415437072515488\n",
            "Epoch 96, Step 45, Loss: 0.09388059377670288\n",
            "Epoch 96, Step 46, Loss: 0.11469371616840363\n",
            "Epoch 96, Step 47, Loss: 0.118376225233078\n",
            "Epoch 96, Step 48, Loss: 0.09480679035186768\n",
            "Epoch 96, Step 49, Loss: 0.07453905045986176\n",
            "Epoch 96, Step 50, Loss: 0.03217826783657074\n",
            "Epoch 96, Step 51, Loss: 0.13323178887367249\n",
            "Epoch 96, Step 52, Loss: 0.09930237382650375\n",
            "Epoch 96, Step 53, Loss: 0.09412570297718048\n",
            "Epoch 96, Step 54, Loss: 0.14184045791625977\n",
            "Epoch 96, Step 55, Loss: 0.07769019156694412\n",
            "Epoch 96, Step 56, Loss: 0.09124860912561417\n",
            "Epoch 96, Step 57, Loss: 0.1880122870206833\n",
            "Epoch 96, Step 58, Loss: 0.10723651200532913\n",
            "Epoch 96, Step 59, Loss: 0.10728498548269272\n",
            "Epoch 96, Step 60, Loss: 0.0780559703707695\n",
            "Epoch 96, Step 61, Loss: 0.17433859407901764\n",
            "Epoch 96, Step 62, Loss: 0.12046414613723755\n",
            "Epoch 96, Step 63, Loss: 0.0807134285569191\n",
            "Epoch 96, Step 64, Loss: 0.12906961143016815\n",
            "Epoch 96, Step 65, Loss: 0.09805473685264587\n",
            "Epoch 96, Step 66, Loss: 0.09175343066453934\n",
            "Epoch 96, Step 67, Loss: 0.0894964337348938\n",
            "Epoch 96, Step 68, Loss: 0.10895294696092606\n",
            "Epoch 96, Step 69, Loss: 0.13143447041511536\n",
            "Epoch 96, Step 70, Loss: 0.10579372942447662\n",
            "Epoch 96, Step 71, Loss: 0.1644037663936615\n",
            "Epoch 96, Step 72, Loss: 0.025505542755126953\n",
            "Epoch 96, Step 73, Loss: 0.07661085575819016\n",
            "Epoch 96, Step 74, Loss: 0.1163230687379837\n",
            "Epoch 96, Step 75, Loss: 0.08700218796730042\n",
            "Epoch 96, Step 76, Loss: 0.05943240225315094\n",
            "Epoch 96, Step 77, Loss: 0.05554885417222977\n",
            "Epoch 96, Step 78, Loss: 0.14181961119174957\n",
            "Epoch 96, Step 79, Loss: 0.11252354085445404\n",
            "Epoch 96, Step 80, Loss: 0.10223709046840668\n",
            "Epoch 96, Step 81, Loss: 0.09925736486911774\n",
            "Epoch 96, Step 82, Loss: 0.17932181060314178\n",
            "Epoch 96, Step 83, Loss: 0.13351184129714966\n",
            "Epoch 96, Step 84, Loss: 0.11007586866617203\n",
            "Epoch 96, Step 85, Loss: 0.07699649780988693\n",
            "Epoch 96, Step 86, Loss: 0.14420382678508759\n",
            "Epoch 96, Step 87, Loss: 0.08901749551296234\n",
            "Epoch 96, Step 88, Loss: 0.1810801923274994\n",
            "Epoch 96, Step 89, Loss: 0.05480558052659035\n",
            "Epoch 96, Step 90, Loss: 0.11053533107042313\n",
            "Epoch 96, Step 91, Loss: 0.10945823043584824\n",
            "Epoch 96, Step 92, Loss: 0.054815519601106644\n",
            "Epoch 96, Step 93, Loss: 0.06379518657922745\n",
            "Epoch 96, Step 94, Loss: 0.19588027894496918\n",
            "Epoch 96, Step 95, Loss: 0.12078413367271423\n",
            "Epoch 96, Step 96, Loss: 0.1329590231180191\n",
            "Epoch 96, Step 97, Loss: 0.06445616483688354\n",
            "Epoch 96, Step 98, Loss: 0.11536061763763428\n",
            "Epoch 96, Step 99, Loss: 0.0681714415550232\n",
            "Epoch 96, Step 100, Loss: 0.055320825427770615\n",
            "Epoch 96, Step 101, Loss: 0.12914784252643585\n",
            "Epoch 96, Step 102, Loss: 0.10125963389873505\n",
            "Epoch 96, Step 103, Loss: 0.1381787359714508\n",
            "Epoch 96, Step 104, Loss: 0.11128702759742737\n",
            "Epoch 96, Step 105, Loss: 0.06159267947077751\n",
            "Epoch 96, Step 106, Loss: 0.16398096084594727\n",
            "Epoch 96, Step 107, Loss: 0.16367721557617188\n",
            "Epoch 96, Step 108, Loss: 0.11928736418485641\n",
            "Epoch 96, Step 109, Loss: 0.06075471639633179\n",
            "Epoch 96, Step 110, Loss: 0.12203482538461685\n",
            "Epoch 96, Step 111, Loss: 0.15342071652412415\n",
            "Epoch 96, Step 112, Loss: 0.04317818209528923\n",
            "Epoch 96, Step 113, Loss: 0.14039233326911926\n",
            "Epoch 96, Step 114, Loss: 0.22365126013755798\n",
            "Epoch 96, Step 115, Loss: 0.050972647964954376\n",
            "Epoch 96, Step 116, Loss: 0.1471310406923294\n",
            "Epoch 96, Step 117, Loss: 0.10487119108438492\n",
            "Epoch 96, Step 118, Loss: 0.1112685352563858\n",
            "Epoch 96, Step 119, Loss: 0.07571517676115036\n",
            "Epoch 96, Step 120, Loss: 0.02166983112692833\n",
            "Epoch 96, Step 121, Loss: 0.1019839271903038\n",
            "Epoch 96, Step 122, Loss: 0.12786853313446045\n",
            "Epoch 96, Step 123, Loss: 0.08289302885532379\n",
            "Epoch 96, Step 124, Loss: 0.13580092787742615\n",
            "Epoch 96, Step 125, Loss: 0.16035284101963043\n",
            "Epoch 96, Step 126, Loss: 0.17764347791671753\n",
            "Epoch 96, Step 127, Loss: 0.06463533639907837\n",
            "Epoch 96, Step 128, Loss: 0.15316461026668549\n",
            "Epoch 96, Step 129, Loss: 0.11818239837884903\n",
            "Epoch 96, Step 130, Loss: 0.0788319930434227\n",
            "Epoch 96, Step 131, Loss: 0.04563099145889282\n",
            "Epoch 96, Step 132, Loss: 0.11987147480249405\n",
            "Epoch 96, Step 133, Loss: 0.06709486246109009\n",
            "Epoch 96, Step 134, Loss: 0.14047890901565552\n",
            "Epoch 96, Step 135, Loss: 0.1461534947156906\n",
            "Epoch 96, Step 136, Loss: 0.12316206097602844\n",
            "Epoch 96, Step 137, Loss: 0.09227195382118225\n",
            "Epoch 96, Step 138, Loss: 0.1010127067565918\n",
            "Epoch 96, Step 139, Loss: 0.029689062386751175\n",
            "Epoch 96, Step 140, Loss: 0.11782270669937134\n",
            "Epoch 96, Step 141, Loss: 0.043844081461429596\n",
            "Epoch 96, Step 142, Loss: 0.08688882738351822\n",
            "Epoch 96, Step 143, Loss: 0.035215552896261215\n",
            "Epoch 96, Step 144, Loss: 0.0919867753982544\n",
            "Epoch 96, Step 145, Loss: 0.10323139280080795\n",
            "Epoch 96, Step 146, Loss: 0.17435690760612488\n",
            "Epoch 96, Step 147, Loss: 0.1213126927614212\n",
            "Epoch 96, Step 148, Loss: 0.05147640034556389\n",
            "Epoch 96, Step 149, Loss: 0.17108236253261566\n",
            "Epoch 96, Step 150, Loss: 0.10725514590740204\n",
            "Epoch 96, Step 151, Loss: 0.08091338723897934\n",
            "Epoch 96, Step 152, Loss: 0.19031600654125214\n",
            "Epoch 96, Step 153, Loss: 0.1190129891037941\n",
            "Epoch 96, Step 154, Loss: 0.11654052138328552\n",
            "Epoch 96, Step 155, Loss: 0.13249048590660095\n",
            "Epoch 96, Step 156, Loss: 0.09618743509054184\n",
            "Epoch 96, Step 157, Loss: 0.09826086461544037\n",
            "Epoch 96, Step 158, Loss: 0.10910876095294952\n",
            "Epoch 96, Step 159, Loss: 0.08405585587024689\n",
            "Epoch 96, Step 160, Loss: 0.07389712333679199\n",
            "Epoch 96, Step 161, Loss: 0.06964774429798126\n",
            "Epoch 96, Step 162, Loss: 0.13544459640979767\n",
            "Epoch 96, Step 163, Loss: 0.19057369232177734\n",
            "Epoch 96, Step 164, Loss: 0.13959214091300964\n",
            "Epoch 96, Step 165, Loss: 0.1461915522813797\n",
            "Epoch 96, Step 166, Loss: 0.09697394073009491\n",
            "Epoch 96, Step 167, Loss: 0.05930047854781151\n",
            "Epoch 96, Step 168, Loss: 0.17472025752067566\n",
            "Epoch 96, Step 169, Loss: 0.06939204037189484\n",
            "Epoch 96, Step 170, Loss: 0.11292336136102676\n",
            "Epoch 96, Step 171, Loss: 0.10481172800064087\n",
            "Epoch 96, Step 172, Loss: 0.13446199893951416\n",
            "Epoch 96, Step 173, Loss: 0.07378130406141281\n",
            "Epoch 96, Step 174, Loss: 0.05374530702829361\n",
            "Epoch 96, Step 175, Loss: 0.0795719251036644\n",
            "Epoch 96, Step 176, Loss: 0.09682869911193848\n",
            "Epoch 96, Step 177, Loss: 0.07188458740711212\n",
            "Epoch 96, Step 178, Loss: 0.17372940480709076\n",
            "Epoch 96, Step 179, Loss: 0.08231804519891739\n",
            "Epoch 96, Step 180, Loss: 0.07800040394067764\n",
            "Epoch 96, Step 181, Loss: 0.1502135843038559\n",
            "Epoch 96, Step 182, Loss: 0.11238457262516022\n",
            "Epoch 96, Step 183, Loss: 0.06712476164102554\n",
            "Epoch 96, Step 184, Loss: 0.07860764861106873\n",
            "Epoch 96, Step 185, Loss: 0.09328379482030869\n",
            "Epoch 96, Step 186, Loss: 0.12840749323368073\n",
            "Epoch 96, Step 187, Loss: 0.1154293566942215\n",
            "Epoch 96, Step 188, Loss: 0.07159668207168579\n",
            "Epoch 96, Step 189, Loss: 0.04961328208446503\n",
            "Epoch 96, Step 190, Loss: 0.0551702044904232\n",
            "Epoch 96, Step 191, Loss: 0.0902491956949234\n",
            "Epoch 96, Step 192, Loss: 0.1375843584537506\n",
            "Epoch 96, Step 193, Loss: 0.11592563986778259\n",
            "Epoch 96, Step 194, Loss: 0.1335226148366928\n",
            "Epoch 96, Step 195, Loss: 0.11821281909942627\n",
            "Epoch 96, Step 196, Loss: 0.04021569713950157\n",
            "Epoch 96, Step 197, Loss: 0.09144364297389984\n",
            "Epoch 96, Step 198, Loss: 0.10078269243240356\n",
            "Epoch 96, Step 199, Loss: 0.22104744613170624\n",
            "Epoch 96, Step 200, Loss: 0.07379528135061264\n",
            "Epoch 96, Step 201, Loss: 0.09054004400968552\n",
            "Epoch 96, Step 202, Loss: 0.06201138347387314\n",
            "Epoch 96, Step 203, Loss: 0.15026399493217468\n",
            "Epoch 96, Step 204, Loss: 0.12242519855499268\n",
            "Epoch 96, Step 205, Loss: 0.07493101060390472\n",
            "Epoch 96, Step 206, Loss: 0.099223792552948\n",
            "Epoch 96, Step 207, Loss: 0.08602212369441986\n",
            "Epoch 96, Step 208, Loss: 0.08300544321537018\n",
            "Epoch 96, Step 209, Loss: 0.09492899477481842\n",
            "Epoch 96, Step 210, Loss: 0.07889341562986374\n",
            "Epoch 96, Step 211, Loss: 0.1235802099108696\n",
            "Epoch 96, Step 212, Loss: 0.04551621153950691\n",
            "Epoch 96, Step 213, Loss: 0.07614923268556595\n",
            "Epoch 96, Step 214, Loss: 0.09521833807229996\n",
            "Epoch 96, Step 215, Loss: 0.1097312644124031\n",
            "Epoch 96, Step 216, Loss: 0.0672866627573967\n",
            "Epoch 96, Step 217, Loss: 0.07369086146354675\n",
            "Epoch 96, Step 218, Loss: 0.07101503014564514\n",
            "Epoch 96, Step 219, Loss: 0.11678251624107361\n",
            "Epoch 96, Step 220, Loss: 0.09246234595775604\n",
            "Epoch 96, Step 221, Loss: 0.22184936702251434\n",
            "Epoch 96, Step 222, Loss: 0.12943020462989807\n",
            "Epoch 96, Step 223, Loss: 0.11101241409778595\n",
            "Epoch 96, Step 224, Loss: 0.08466876298189163\n",
            "Epoch 96, Step 225, Loss: 0.07753364741802216\n",
            "Epoch 96, Step 226, Loss: 0.13514961302280426\n",
            "Epoch 96, Step 227, Loss: 0.15630421042442322\n",
            "Epoch 96, Step 228, Loss: 0.09381706267595291\n",
            "Epoch 96, Step 229, Loss: 0.06823427230119705\n",
            "Epoch 96, Step 230, Loss: 0.05606504902243614\n",
            "Epoch 96, Step 231, Loss: 0.07599268853664398\n",
            "Epoch 96, Step 232, Loss: 0.07066366076469421\n",
            "Epoch 96, Step 233, Loss: 0.10127943754196167\n",
            "Epoch 96, Step 234, Loss: 0.11081250011920929\n",
            "Epoch 96, Step 235, Loss: 0.0999484583735466\n",
            "Epoch 96, Step 236, Loss: 0.06248820573091507\n",
            "Epoch 96, Step 237, Loss: 0.09616035968065262\n",
            "Epoch 96, Step 238, Loss: 0.06627186387777328\n",
            "Epoch 96, Step 239, Loss: 0.07145212590694427\n",
            "Epoch 96, Step 240, Loss: 0.10908427089452744\n",
            "Epoch 96, Step 241, Loss: 0.09887709468603134\n",
            "Epoch 96, Step 242, Loss: 0.11980418860912323\n",
            "Epoch 96, Step 243, Loss: 0.05747503414750099\n",
            "Epoch 96, Step 244, Loss: 0.11113515496253967\n",
            "Epoch 96, Step 245, Loss: 0.1460065096616745\n",
            "Epoch 96, Step 246, Loss: 0.11999091506004333\n",
            "Epoch 96, Step 247, Loss: 0.1387602984905243\n",
            "Epoch 96, Step 248, Loss: 0.15297995507717133\n",
            "Epoch 96, Step 249, Loss: 0.10171002894639969\n",
            "Epoch 96, Step 250, Loss: 0.09264128655195236\n",
            "Epoch 96, Step 251, Loss: 0.05780365690588951\n",
            "Epoch 96, Step 252, Loss: 0.07379935681819916\n",
            "Epoch 96, Step 253, Loss: 0.03714995086193085\n",
            "Epoch 96, Step 254, Loss: 0.07888653129339218\n",
            "Epoch 96, Step 255, Loss: 0.07283531874418259\n",
            "Epoch 96, Step 256, Loss: 0.08167070150375366\n",
            "Epoch 96, Step 257, Loss: 0.11358869820833206\n",
            "Epoch 96, Step 258, Loss: 0.1289154440164566\n",
            "Epoch 96, Step 259, Loss: 0.0802728608250618\n",
            "Epoch 96, Step 260, Loss: 0.1181524321436882\n",
            "Epoch 96, Step 261, Loss: 0.06698202341794968\n",
            "Epoch 96, Step 262, Loss: 0.049156706780195236\n",
            "Epoch 96, Step 263, Loss: 0.09627538919448853\n",
            "Epoch 96, Step 264, Loss: 0.16659019887447357\n",
            "Epoch 96, Step 265, Loss: 0.06465359777212143\n",
            "Epoch 96, Step 266, Loss: 0.04148552194237709\n",
            "Epoch 96, Step 267, Loss: 0.049058035016059875\n",
            "Epoch 96, Step 268, Loss: 0.1646379828453064\n",
            "Epoch 96, Step 269, Loss: 0.10540346801280975\n",
            "Epoch 96, Step 270, Loss: 0.09161844104528427\n",
            "Epoch 96, Step 271, Loss: 0.1260066032409668\n",
            "Epoch 96, Step 272, Loss: 0.0947246253490448\n",
            "Epoch 96, Step 273, Loss: 0.14397424459457397\n",
            "Epoch 96, Step 274, Loss: 0.12314344942569733\n",
            "Epoch 96, Step 275, Loss: 0.10731382668018341\n",
            "Epoch 96, Step 276, Loss: 0.14205636084079742\n",
            "Epoch 96, Step 277, Loss: 0.08879908174276352\n",
            "Epoch 96, Step 278, Loss: 0.08258633315563202\n",
            "Epoch 96, Step 279, Loss: 0.07027237117290497\n",
            "Epoch 96, Step 280, Loss: 0.14551113545894623\n",
            "Epoch 96, Step 281, Loss: 0.07663075625896454\n",
            "Epoch 96, Step 282, Loss: 0.08666731417179108\n",
            "Epoch 96, Step 283, Loss: 0.15958377718925476\n",
            "Epoch 96, Step 284, Loss: 0.09632676094770432\n",
            "Epoch 96, Step 285, Loss: 0.16312088072299957\n",
            "Epoch 96, Step 286, Loss: 0.0409231074154377\n",
            "Epoch 96, Step 287, Loss: 0.08294481784105301\n",
            "Epoch 96, Step 288, Loss: 0.048020780086517334\n",
            "Epoch 96, Step 289, Loss: 0.05317011475563049\n",
            "Epoch 96, Step 290, Loss: 0.11715710163116455\n",
            "Epoch 96, Step 291, Loss: 0.055104028433561325\n",
            "Epoch 96, Step 292, Loss: 0.06778628379106522\n",
            "Epoch 96, Step 293, Loss: 0.12825116515159607\n",
            "Epoch 96, Step 294, Loss: 0.07812871038913727\n",
            "Epoch 96, Step 295, Loss: 0.08839503675699234\n",
            "Epoch 96, Step 296, Loss: 0.09631137549877167\n",
            "Epoch 96, Step 297, Loss: 0.09108005464076996\n",
            "Epoch 96, Step 298, Loss: 0.1900600790977478\n",
            "Epoch 96, Step 299, Loss: 0.039380159229040146\n",
            "Epoch 96, Step 300, Loss: 0.11523403227329254\n",
            "Epoch 96, Step 301, Loss: 0.08769159018993378\n",
            "Epoch 96, Step 302, Loss: 0.04484694078564644\n",
            "Epoch 96, Step 303, Loss: 0.06655988842248917\n",
            "Epoch 96, Step 304, Loss: 0.06907595694065094\n",
            "Epoch 96, Step 305, Loss: 0.09068571776151657\n",
            "Epoch 96, Step 306, Loss: 0.07584798336029053\n",
            "Epoch 96, Step 307, Loss: 0.09828247129917145\n",
            "Epoch 96, Step 308, Loss: 0.11553284525871277\n",
            "Epoch 96, Step 309, Loss: 0.034610096365213394\n",
            "Epoch 96, Step 310, Loss: 0.13392625749111176\n",
            "Epoch 96, Step 311, Loss: 0.105952188372612\n",
            "Epoch 96, Step 312, Loss: 0.048394545912742615\n",
            "Epoch 96 end, avg train loss: 0.10109803072227456\n",
            "Epoch 96 end, avg val loss: 0.3786783691118412, accuracy: 90.70%\n",
            "Epoch 97, Step 0, Loss: 0.07646492123603821\n",
            "Epoch 97, Step 1, Loss: 0.10257481783628464\n",
            "Epoch 97, Step 2, Loss: 0.09212855994701385\n",
            "Epoch 97, Step 3, Loss: 0.07789791375398636\n",
            "Epoch 97, Step 4, Loss: 0.1542557328939438\n",
            "Epoch 97, Step 5, Loss: 0.09944029897451401\n",
            "Epoch 97, Step 6, Loss: 0.0513962097465992\n",
            "Epoch 97, Step 7, Loss: 0.14767341315746307\n",
            "Epoch 97, Step 8, Loss: 0.08414513617753983\n",
            "Epoch 97, Step 9, Loss: 0.11404500901699066\n",
            "Epoch 97, Step 10, Loss: 0.09052230417728424\n",
            "Epoch 97, Step 11, Loss: 0.067111074924469\n",
            "Epoch 97, Step 12, Loss: 0.12056013196706772\n",
            "Epoch 97, Step 13, Loss: 0.07846427708864212\n",
            "Epoch 97, Step 14, Loss: 0.10073273628950119\n",
            "Epoch 97, Step 15, Loss: 0.04922745004296303\n",
            "Epoch 97, Step 16, Loss: 0.11291362345218658\n",
            "Epoch 97, Step 17, Loss: 0.0597388818860054\n",
            "Epoch 97, Step 18, Loss: 0.09420688450336456\n",
            "Epoch 97, Step 19, Loss: 0.16914193332195282\n",
            "Epoch 97, Step 20, Loss: 0.09323255717754364\n",
            "Epoch 97, Step 21, Loss: 0.0817088931798935\n",
            "Epoch 97, Step 22, Loss: 0.047567252069711685\n",
            "Epoch 97, Step 23, Loss: 0.04275369644165039\n",
            "Epoch 97, Step 24, Loss: 0.13562315702438354\n",
            "Epoch 97, Step 25, Loss: 0.19098851084709167\n",
            "Epoch 97, Step 26, Loss: 0.06614096462726593\n",
            "Epoch 97, Step 27, Loss: 0.06375212967395782\n",
            "Epoch 97, Step 28, Loss: 0.12075391411781311\n",
            "Epoch 97, Step 29, Loss: 0.0626504123210907\n",
            "Epoch 97, Step 30, Loss: 0.0468728169798851\n",
            "Epoch 97, Step 31, Loss: 0.0561259500682354\n",
            "Epoch 97, Step 32, Loss: 0.11404602229595184\n",
            "Epoch 97, Step 33, Loss: 0.08622239530086517\n",
            "Epoch 97, Step 34, Loss: 0.0835525169968605\n",
            "Epoch 97, Step 35, Loss: 0.11946210265159607\n",
            "Epoch 97, Step 36, Loss: 0.10791146010160446\n",
            "Epoch 97, Step 37, Loss: 0.0772646814584732\n",
            "Epoch 97, Step 38, Loss: 0.0793846920132637\n",
            "Epoch 97, Step 39, Loss: 0.051865171641111374\n",
            "Epoch 97, Step 40, Loss: 0.04611941799521446\n",
            "Epoch 97, Step 41, Loss: 0.061895985156297684\n",
            "Epoch 97, Step 42, Loss: 0.10244716703891754\n",
            "Epoch 97, Step 43, Loss: 0.07447052747011185\n",
            "Epoch 97, Step 44, Loss: 0.06565704196691513\n",
            "Epoch 97, Step 45, Loss: 0.07855729758739471\n",
            "Epoch 97, Step 46, Loss: 0.05545172840356827\n",
            "Epoch 97, Step 47, Loss: 0.07926123589277267\n",
            "Epoch 97, Step 48, Loss: 0.06309182196855545\n",
            "Epoch 97, Step 49, Loss: 0.10596360266208649\n",
            "Epoch 97, Step 50, Loss: 0.06870056688785553\n",
            "Epoch 97, Step 51, Loss: 0.09515607357025146\n",
            "Epoch 97, Step 52, Loss: 0.17115598917007446\n",
            "Epoch 97, Step 53, Loss: 0.06310126185417175\n",
            "Epoch 97, Step 54, Loss: 0.12694405019283295\n",
            "Epoch 97, Step 55, Loss: 0.0987132266163826\n",
            "Epoch 97, Step 56, Loss: 0.03918197378516197\n",
            "Epoch 97, Step 57, Loss: 0.12775035202503204\n",
            "Epoch 97, Step 58, Loss: 0.07363748550415039\n",
            "Epoch 97, Step 59, Loss: 0.08829492330551147\n",
            "Epoch 97, Step 60, Loss: 0.10350527614355087\n",
            "Epoch 97, Step 61, Loss: 0.060487985610961914\n",
            "Epoch 97, Step 62, Loss: 0.08428642153739929\n",
            "Epoch 97, Step 63, Loss: 0.11924568563699722\n",
            "Epoch 97, Step 64, Loss: 0.0648638904094696\n",
            "Epoch 97, Step 65, Loss: 0.053188033401966095\n",
            "Epoch 97, Step 66, Loss: 0.11414668709039688\n",
            "Epoch 97, Step 67, Loss: 0.05649014562368393\n",
            "Epoch 97, Step 68, Loss: 0.12528087198734283\n",
            "Epoch 97, Step 69, Loss: 0.04077789559960365\n",
            "Epoch 97, Step 70, Loss: 0.026279117912054062\n",
            "Epoch 97, Step 71, Loss: 0.09333167970180511\n",
            "Epoch 97, Step 72, Loss: 0.10250528901815414\n",
            "Epoch 97, Step 73, Loss: 0.12435663491487503\n",
            "Epoch 97, Step 74, Loss: 0.06144845858216286\n",
            "Epoch 97, Step 75, Loss: 0.14036017656326294\n",
            "Epoch 97, Step 76, Loss: 0.11887175589799881\n",
            "Epoch 97, Step 77, Loss: 0.08503684401512146\n",
            "Epoch 97, Step 78, Loss: 0.15089809894561768\n",
            "Epoch 97, Step 79, Loss: 0.07375074923038483\n",
            "Epoch 97, Step 80, Loss: 0.11119133979082108\n",
            "Epoch 97, Step 81, Loss: 0.09260503202676773\n",
            "Epoch 97, Step 82, Loss: 0.07928747683763504\n",
            "Epoch 97, Step 83, Loss: 0.12068626284599304\n",
            "Epoch 97, Step 84, Loss: 0.054513152688741684\n",
            "Epoch 97, Step 85, Loss: 0.04027734696865082\n",
            "Epoch 97, Step 86, Loss: 0.0895325094461441\n",
            "Epoch 97, Step 87, Loss: 0.050937335938215256\n",
            "Epoch 97, Step 88, Loss: 0.05071752890944481\n",
            "Epoch 97, Step 89, Loss: 0.08668285608291626\n",
            "Epoch 97, Step 90, Loss: 0.1318671703338623\n",
            "Epoch 97, Step 91, Loss: 0.14571355283260345\n",
            "Epoch 97, Step 92, Loss: 0.08491940051317215\n",
            "Epoch 97, Step 93, Loss: 0.07830890268087387\n",
            "Epoch 97, Step 94, Loss: 0.08613140136003494\n",
            "Epoch 97, Step 95, Loss: 0.12469540536403656\n",
            "Epoch 97, Step 96, Loss: 0.07144636660814285\n",
            "Epoch 97, Step 97, Loss: 0.07308036834001541\n",
            "Epoch 97, Step 98, Loss: 0.13886986672878265\n",
            "Epoch 97, Step 99, Loss: 0.07981459051370621\n",
            "Epoch 97, Step 100, Loss: 0.08950026333332062\n",
            "Epoch 97, Step 101, Loss: 0.1490069478750229\n",
            "Epoch 97, Step 102, Loss: 0.0783277377486229\n",
            "Epoch 97, Step 103, Loss: 0.06075522303581238\n",
            "Epoch 97, Step 104, Loss: 0.06529673933982849\n",
            "Epoch 97, Step 105, Loss: 0.12928330898284912\n",
            "Epoch 97, Step 106, Loss: 0.13185276091098785\n",
            "Epoch 97, Step 107, Loss: 0.13846245408058167\n",
            "Epoch 97, Step 108, Loss: 0.05665310099720955\n",
            "Epoch 97, Step 109, Loss: 0.21538777649402618\n",
            "Epoch 97, Step 110, Loss: 0.06397981196641922\n",
            "Epoch 97, Step 111, Loss: 0.04598362743854523\n",
            "Epoch 97, Step 112, Loss: 0.12919661402702332\n",
            "Epoch 97, Step 113, Loss: 0.11995482444763184\n",
            "Epoch 97, Step 114, Loss: 0.11829909682273865\n",
            "Epoch 97, Step 115, Loss: 0.13838452100753784\n",
            "Epoch 97, Step 116, Loss: 0.07731248438358307\n",
            "Epoch 97, Step 117, Loss: 0.084503673017025\n",
            "Epoch 97, Step 118, Loss: 0.14765894412994385\n",
            "Epoch 97, Step 119, Loss: 0.1693757027387619\n",
            "Epoch 97, Step 120, Loss: 0.1590403914451599\n",
            "Epoch 97, Step 121, Loss: 0.19017471373081207\n",
            "Epoch 97, Step 122, Loss: 0.07430553436279297\n",
            "Epoch 97, Step 123, Loss: 0.044636938720941544\n",
            "Epoch 97, Step 124, Loss: 0.10266324877738953\n",
            "Epoch 97, Step 125, Loss: 0.03743254020810127\n",
            "Epoch 97, Step 126, Loss: 0.10555006563663483\n",
            "Epoch 97, Step 127, Loss: 0.108269602060318\n",
            "Epoch 97, Step 128, Loss: 0.09795302897691727\n",
            "Epoch 97, Step 129, Loss: 0.06568335741758347\n",
            "Epoch 97, Step 130, Loss: 0.19089728593826294\n",
            "Epoch 97, Step 131, Loss: 0.04589461535215378\n",
            "Epoch 97, Step 132, Loss: 0.1436178982257843\n",
            "Epoch 97, Step 133, Loss: 0.108678437769413\n",
            "Epoch 97, Step 134, Loss: 0.14432576298713684\n",
            "Epoch 97, Step 135, Loss: 0.06395809352397919\n",
            "Epoch 97, Step 136, Loss: 0.06475534290075302\n",
            "Epoch 97, Step 137, Loss: 0.07473728805780411\n",
            "Epoch 97, Step 138, Loss: 0.14421376585960388\n",
            "Epoch 97, Step 139, Loss: 0.058050233870744705\n",
            "Epoch 97, Step 140, Loss: 0.07334000617265701\n",
            "Epoch 97, Step 141, Loss: 0.07330703735351562\n",
            "Epoch 97, Step 142, Loss: 0.0566394217312336\n",
            "Epoch 97, Step 143, Loss: 0.10021153092384338\n",
            "Epoch 97, Step 144, Loss: 0.15174101293087006\n",
            "Epoch 97, Step 145, Loss: 0.12047819793224335\n",
            "Epoch 97, Step 146, Loss: 0.12680800259113312\n",
            "Epoch 97, Step 147, Loss: 0.10169483721256256\n",
            "Epoch 97, Step 148, Loss: 0.08707013726234436\n",
            "Epoch 97, Step 149, Loss: 0.0663970410823822\n",
            "Epoch 97, Step 150, Loss: 0.11137429624795914\n",
            "Epoch 97, Step 151, Loss: 0.1729196012020111\n",
            "Epoch 97, Step 152, Loss: 0.10646454244852066\n",
            "Epoch 97, Step 153, Loss: 0.081496961414814\n",
            "Epoch 97, Step 154, Loss: 0.04066335782408714\n",
            "Epoch 97, Step 155, Loss: 0.08826447278261185\n",
            "Epoch 97, Step 156, Loss: 0.1284347027540207\n",
            "Epoch 97, Step 157, Loss: 0.05597613751888275\n",
            "Epoch 97, Step 158, Loss: 0.08933920413255692\n",
            "Epoch 97, Step 159, Loss: 0.06572743505239487\n",
            "Epoch 97, Step 160, Loss: 0.06704813241958618\n",
            "Epoch 97, Step 161, Loss: 0.08985891193151474\n",
            "Epoch 97, Step 162, Loss: 0.14556171000003815\n",
            "Epoch 97, Step 163, Loss: 0.08753868192434311\n",
            "Epoch 97, Step 164, Loss: 0.09376009553670883\n",
            "Epoch 97, Step 165, Loss: 0.09065106511116028\n",
            "Epoch 97, Step 166, Loss: 0.08380452543497086\n",
            "Epoch 97, Step 167, Loss: 0.12875890731811523\n",
            "Epoch 97, Step 168, Loss: 0.07332354784011841\n",
            "Epoch 97, Step 169, Loss: 0.09842715412378311\n",
            "Epoch 97, Step 170, Loss: 0.07638134807348251\n",
            "Epoch 97, Step 171, Loss: 0.1253310590982437\n",
            "Epoch 97, Step 172, Loss: 0.09254976361989975\n",
            "Epoch 97, Step 173, Loss: 0.16804687678813934\n",
            "Epoch 97, Step 174, Loss: 0.08265716582536697\n",
            "Epoch 97, Step 175, Loss: 0.16093933582305908\n",
            "Epoch 97, Step 176, Loss: 0.10693557560443878\n",
            "Epoch 97, Step 177, Loss: 0.13458289206027985\n",
            "Epoch 97, Step 178, Loss: 0.06870304048061371\n",
            "Epoch 97, Step 179, Loss: 0.10394037514925003\n",
            "Epoch 97, Step 180, Loss: 0.11010243743658066\n",
            "Epoch 97, Step 181, Loss: 0.08412590622901917\n",
            "Epoch 97, Step 182, Loss: 0.08347628265619278\n",
            "Epoch 97, Step 183, Loss: 0.0623524934053421\n",
            "Epoch 97, Step 184, Loss: 0.17123982310295105\n",
            "Epoch 97, Step 185, Loss: 0.08711162209510803\n",
            "Epoch 97, Step 186, Loss: 0.08215221762657166\n",
            "Epoch 97, Step 187, Loss: 0.1152973547577858\n",
            "Epoch 97, Step 188, Loss: 0.12263756990432739\n",
            "Epoch 97, Step 189, Loss: 0.07083261758089066\n",
            "Epoch 97, Step 190, Loss: 0.11906835436820984\n",
            "Epoch 97, Step 191, Loss: 0.04557894170284271\n",
            "Epoch 97, Step 192, Loss: 0.079386867582798\n",
            "Epoch 97, Step 193, Loss: 0.10110263526439667\n",
            "Epoch 97, Step 194, Loss: 0.10161192715167999\n",
            "Epoch 97, Step 195, Loss: 0.05559985712170601\n",
            "Epoch 97, Step 196, Loss: 0.16732478141784668\n",
            "Epoch 97, Step 197, Loss: 0.13526111841201782\n",
            "Epoch 97, Step 198, Loss: 0.1268727034330368\n",
            "Epoch 97, Step 199, Loss: 0.19804102182388306\n",
            "Epoch 97, Step 200, Loss: 0.06308721750974655\n",
            "Epoch 97, Step 201, Loss: 0.12187044322490692\n",
            "Epoch 97, Step 202, Loss: 0.10201241075992584\n",
            "Epoch 97, Step 203, Loss: 0.04289422184228897\n",
            "Epoch 97, Step 204, Loss: 0.06584303081035614\n",
            "Epoch 97, Step 205, Loss: 0.0984099730849266\n",
            "Epoch 97, Step 206, Loss: 0.10615119338035583\n",
            "Epoch 97, Step 207, Loss: 0.12554068863391876\n",
            "Epoch 97, Step 208, Loss: 0.1199798434972763\n",
            "Epoch 97, Step 209, Loss: 0.0781349316239357\n",
            "Epoch 97, Step 210, Loss: 0.07941165566444397\n",
            "Epoch 97, Step 211, Loss: 0.0974978432059288\n",
            "Epoch 97, Step 212, Loss: 0.08202631026506424\n",
            "Epoch 97, Step 213, Loss: 0.13101530075073242\n",
            "Epoch 97, Step 214, Loss: 0.11784229427576065\n",
            "Epoch 97, Step 215, Loss: 0.07959049940109253\n",
            "Epoch 97, Step 216, Loss: 0.1315707564353943\n",
            "Epoch 97, Step 217, Loss: 0.07101485878229141\n",
            "Epoch 97, Step 218, Loss: 0.1873505711555481\n",
            "Epoch 97, Step 219, Loss: 0.07146230340003967\n",
            "Epoch 97, Step 220, Loss: 0.1441773772239685\n",
            "Epoch 97, Step 221, Loss: 0.06090404465794563\n",
            "Epoch 97, Step 222, Loss: 0.10294601321220398\n",
            "Epoch 97, Step 223, Loss: 0.05172795429825783\n",
            "Epoch 97, Step 224, Loss: 0.09100036323070526\n",
            "Epoch 97, Step 225, Loss: 0.05663790926337242\n",
            "Epoch 97, Step 226, Loss: 0.0867597758769989\n",
            "Epoch 97, Step 227, Loss: 0.12129870057106018\n",
            "Epoch 97, Step 228, Loss: 0.10401009768247604\n",
            "Epoch 97, Step 229, Loss: 0.11108500510454178\n",
            "Epoch 97, Step 230, Loss: 0.07554440200328827\n",
            "Epoch 97, Step 231, Loss: 0.036547716706991196\n",
            "Epoch 97, Step 232, Loss: 0.1738879233598709\n",
            "Epoch 97, Step 233, Loss: 0.061401452869176865\n",
            "Epoch 97, Step 234, Loss: 0.16069209575653076\n",
            "Epoch 97, Step 235, Loss: 0.07954476773738861\n",
            "Epoch 97, Step 236, Loss: 0.06251464039087296\n",
            "Epoch 97, Step 237, Loss: 0.10547398030757904\n",
            "Epoch 97, Step 238, Loss: 0.09057027846574783\n",
            "Epoch 97, Step 239, Loss: 0.15149086713790894\n",
            "Epoch 97, Step 240, Loss: 0.1631193906068802\n",
            "Epoch 97, Step 241, Loss: 0.08113464713096619\n",
            "Epoch 97, Step 242, Loss: 0.04400479793548584\n",
            "Epoch 97, Step 243, Loss: 0.16565591096878052\n",
            "Epoch 97, Step 244, Loss: 0.09759315848350525\n",
            "Epoch 97, Step 245, Loss: 0.07291237264871597\n",
            "Epoch 97, Step 246, Loss: 0.15344415605068207\n",
            "Epoch 97, Step 247, Loss: 0.0339038223028183\n",
            "Epoch 97, Step 248, Loss: 0.14449483156204224\n",
            "Epoch 97, Step 249, Loss: 0.06776349991559982\n",
            "Epoch 97, Step 250, Loss: 0.16034291684627533\n",
            "Epoch 97, Step 251, Loss: 0.10288216173648834\n",
            "Epoch 97, Step 252, Loss: 0.08075419068336487\n",
            "Epoch 97, Step 253, Loss: 0.08862830698490143\n",
            "Epoch 97, Step 254, Loss: 0.12948058545589447\n",
            "Epoch 97, Step 255, Loss: 0.06828659027814865\n",
            "Epoch 97, Step 256, Loss: 0.09751410782337189\n",
            "Epoch 97, Step 257, Loss: 0.08060376346111298\n",
            "Epoch 97, Step 258, Loss: 0.08066532760858536\n",
            "Epoch 97, Step 259, Loss: 0.24854297935962677\n",
            "Epoch 97, Step 260, Loss: 0.09419341385364532\n",
            "Epoch 97, Step 261, Loss: 0.0961233377456665\n",
            "Epoch 97, Step 262, Loss: 0.08118302375078201\n",
            "Epoch 97, Step 263, Loss: 0.11007716506719589\n",
            "Epoch 97, Step 264, Loss: 0.043157294392585754\n",
            "Epoch 97, Step 265, Loss: 0.1849791258573532\n",
            "Epoch 97, Step 266, Loss: 0.15587449073791504\n",
            "Epoch 97, Step 267, Loss: 0.07390321791172028\n",
            "Epoch 97, Step 268, Loss: 0.13459952175617218\n",
            "Epoch 97, Step 269, Loss: 0.05252697691321373\n",
            "Epoch 97, Step 270, Loss: 0.10767162591218948\n",
            "Epoch 97, Step 271, Loss: 0.19246582686901093\n",
            "Epoch 97, Step 272, Loss: 0.10644534230232239\n",
            "Epoch 97, Step 273, Loss: 0.09124612808227539\n",
            "Epoch 97, Step 274, Loss: 0.1347639262676239\n",
            "Epoch 97, Step 275, Loss: 0.08141284435987473\n",
            "Epoch 97, Step 276, Loss: 0.0905635878443718\n",
            "Epoch 97, Step 277, Loss: 0.1354583352804184\n",
            "Epoch 97, Step 278, Loss: 0.08326395601034164\n",
            "Epoch 97, Step 279, Loss: 0.055361852049827576\n",
            "Epoch 97, Step 280, Loss: 0.08065948635339737\n",
            "Epoch 97, Step 281, Loss: 0.1107180267572403\n",
            "Epoch 97, Step 282, Loss: 0.05702159181237221\n",
            "Epoch 97, Step 283, Loss: 0.07414907962083817\n",
            "Epoch 97, Step 284, Loss: 0.10418649017810822\n",
            "Epoch 97, Step 285, Loss: 0.15213924646377563\n",
            "Epoch 97, Step 286, Loss: 0.13634459674358368\n",
            "Epoch 97, Step 287, Loss: 0.10848306119441986\n",
            "Epoch 97, Step 288, Loss: 0.15935596823692322\n",
            "Epoch 97, Step 289, Loss: 0.08112520724534988\n",
            "Epoch 97, Step 290, Loss: 0.13969747722148895\n",
            "Epoch 97, Step 291, Loss: 0.1106654554605484\n",
            "Epoch 97, Step 292, Loss: 0.08244279772043228\n",
            "Epoch 97, Step 293, Loss: 0.059195853769779205\n",
            "Epoch 97, Step 294, Loss: 0.14686575531959534\n",
            "Epoch 97, Step 295, Loss: 0.06574222445487976\n",
            "Epoch 97, Step 296, Loss: 0.10102423280477524\n",
            "Epoch 97, Step 297, Loss: 0.08409371972084045\n",
            "Epoch 97, Step 298, Loss: 0.08146071434020996\n",
            "Epoch 97, Step 299, Loss: 0.15073980391025543\n",
            "Epoch 97, Step 300, Loss: 0.1073632761836052\n",
            "Epoch 97, Step 301, Loss: 0.0574684739112854\n",
            "Epoch 97, Step 302, Loss: 0.05221742019057274\n",
            "Epoch 97, Step 303, Loss: 0.14057587087154388\n",
            "Epoch 97, Step 304, Loss: 0.17172473669052124\n",
            "Epoch 97, Step 305, Loss: 0.05837476998567581\n",
            "Epoch 97, Step 306, Loss: 0.11305923014879227\n",
            "Epoch 97, Step 307, Loss: 0.12747085094451904\n",
            "Epoch 97, Step 308, Loss: 0.051778096705675125\n",
            "Epoch 97, Step 309, Loss: 0.054614607244729996\n",
            "Epoch 97, Step 310, Loss: 0.10507405549287796\n",
            "Epoch 97, Step 311, Loss: 0.10355520993471146\n",
            "Epoch 97, Step 312, Loss: 0.02353210374712944\n",
            "Epoch 97 end, avg train loss: 0.0980755271860205\n",
            "Epoch 97 end, avg val loss: 0.3815730102522346, accuracy: 90.69%\n",
            "Epoch 98, Step 0, Loss: 0.10637347400188446\n",
            "Epoch 98, Step 1, Loss: 0.05105641856789589\n",
            "Epoch 98, Step 2, Loss: 0.11049383878707886\n",
            "Epoch 98, Step 3, Loss: 0.12651509046554565\n",
            "Epoch 98, Step 4, Loss: 0.04757018759846687\n",
            "Epoch 98, Step 5, Loss: 0.053778454661369324\n",
            "Epoch 98, Step 6, Loss: 0.10777125507593155\n",
            "Epoch 98, Step 7, Loss: 0.08460571616888046\n",
            "Epoch 98, Step 8, Loss: 0.10893379151821136\n",
            "Epoch 98, Step 9, Loss: 0.17028117179870605\n",
            "Epoch 98, Step 10, Loss: 0.0570293590426445\n",
            "Epoch 98, Step 11, Loss: 0.10890726000070572\n",
            "Epoch 98, Step 12, Loss: 0.102416031062603\n",
            "Epoch 98, Step 13, Loss: 0.10552886128425598\n",
            "Epoch 98, Step 14, Loss: 0.08928408473730087\n",
            "Epoch 98, Step 15, Loss: 0.1405392438173294\n",
            "Epoch 98, Step 16, Loss: 0.04031703248620033\n",
            "Epoch 98, Step 17, Loss: 0.05215850844979286\n",
            "Epoch 98, Step 18, Loss: 0.06975047290325165\n",
            "Epoch 98, Step 19, Loss: 0.08418471366167068\n",
            "Epoch 98, Step 20, Loss: 0.057057350873947144\n",
            "Epoch 98, Step 21, Loss: 0.09135804325342178\n",
            "Epoch 98, Step 22, Loss: 0.033256642520427704\n",
            "Epoch 98, Step 23, Loss: 0.0878714770078659\n",
            "Epoch 98, Step 24, Loss: 0.11540712416172028\n",
            "Epoch 98, Step 25, Loss: 0.05847442150115967\n",
            "Epoch 98, Step 26, Loss: 0.09192947298288345\n",
            "Epoch 98, Step 27, Loss: 0.0515817254781723\n",
            "Epoch 98, Step 28, Loss: 0.18896903097629547\n",
            "Epoch 98, Step 29, Loss: 0.05975096672773361\n",
            "Epoch 98, Step 30, Loss: 0.107295922935009\n",
            "Epoch 98, Step 31, Loss: 0.02675405703485012\n",
            "Epoch 98, Step 32, Loss: 0.12537473440170288\n",
            "Epoch 98, Step 33, Loss: 0.08628782629966736\n",
            "Epoch 98, Step 34, Loss: 0.1380862295627594\n",
            "Epoch 98, Step 35, Loss: 0.06862694025039673\n",
            "Epoch 98, Step 36, Loss: 0.10264434665441513\n",
            "Epoch 98, Step 37, Loss: 0.12129461765289307\n",
            "Epoch 98, Step 38, Loss: 0.08004125952720642\n",
            "Epoch 98, Step 39, Loss: 0.06249813735485077\n",
            "Epoch 98, Step 40, Loss: 0.09991449117660522\n",
            "Epoch 98, Step 41, Loss: 0.08413092792034149\n",
            "Epoch 98, Step 42, Loss: 0.11254098266363144\n",
            "Epoch 98, Step 43, Loss: 0.06485147774219513\n",
            "Epoch 98, Step 44, Loss: 0.08938426524400711\n",
            "Epoch 98, Step 45, Loss: 0.07796436548233032\n",
            "Epoch 98, Step 46, Loss: 0.033488769084215164\n",
            "Epoch 98, Step 47, Loss: 0.0827140212059021\n",
            "Epoch 98, Step 48, Loss: 0.08106327056884766\n",
            "Epoch 98, Step 49, Loss: 0.06322147697210312\n",
            "Epoch 98, Step 50, Loss: 0.11238345503807068\n",
            "Epoch 98, Step 51, Loss: 0.03794600069522858\n",
            "Epoch 98, Step 52, Loss: 0.18450334668159485\n",
            "Epoch 98, Step 53, Loss: 0.0631403774023056\n",
            "Epoch 98, Step 54, Loss: 0.03185974434018135\n",
            "Epoch 98, Step 55, Loss: 0.04972049221396446\n",
            "Epoch 98, Step 56, Loss: 0.11852524429559708\n",
            "Epoch 98, Step 57, Loss: 0.09423963725566864\n",
            "Epoch 98, Step 58, Loss: 0.07831433415412903\n",
            "Epoch 98, Step 59, Loss: 0.0950055941939354\n",
            "Epoch 98, Step 60, Loss: 0.13725492358207703\n",
            "Epoch 98, Step 61, Loss: 0.1415465921163559\n",
            "Epoch 98, Step 62, Loss: 0.09760318696498871\n",
            "Epoch 98, Step 63, Loss: 0.09025952219963074\n",
            "Epoch 98, Step 64, Loss: 0.08141662180423737\n",
            "Epoch 98, Step 65, Loss: 0.06827473640441895\n",
            "Epoch 98, Step 66, Loss: 0.10545620322227478\n",
            "Epoch 98, Step 67, Loss: 0.0767076164484024\n",
            "Epoch 98, Step 68, Loss: 0.03250614181160927\n",
            "Epoch 98, Step 69, Loss: 0.08026355504989624\n",
            "Epoch 98, Step 70, Loss: 0.1346239447593689\n",
            "Epoch 98, Step 71, Loss: 0.13185802102088928\n",
            "Epoch 98, Step 72, Loss: 0.18919670581817627\n",
            "Epoch 98, Step 73, Loss: 0.09512567520141602\n",
            "Epoch 98, Step 74, Loss: 0.04032420367002487\n",
            "Epoch 98, Step 75, Loss: 0.1039857268333435\n",
            "Epoch 98, Step 76, Loss: 0.07968995720148087\n",
            "Epoch 98, Step 77, Loss: 0.0639069527387619\n",
            "Epoch 98, Step 78, Loss: 0.1251647174358368\n",
            "Epoch 98, Step 79, Loss: 0.07787831872701645\n",
            "Epoch 98, Step 80, Loss: 0.09998616576194763\n",
            "Epoch 98, Step 81, Loss: 0.11982914060354233\n",
            "Epoch 98, Step 82, Loss: 0.09962815046310425\n",
            "Epoch 98, Step 83, Loss: 0.07168745994567871\n",
            "Epoch 98, Step 84, Loss: 0.04356485977768898\n",
            "Epoch 98, Step 85, Loss: 0.1225016787648201\n",
            "Epoch 98, Step 86, Loss: 0.12409815192222595\n",
            "Epoch 98, Step 87, Loss: 0.06114109605550766\n",
            "Epoch 98, Step 88, Loss: 0.13121670484542847\n",
            "Epoch 98, Step 89, Loss: 0.14588119089603424\n",
            "Epoch 98, Step 90, Loss: 0.17825832962989807\n",
            "Epoch 98, Step 91, Loss: 0.13212920725345612\n",
            "Epoch 98, Step 92, Loss: 0.12800799310207367\n",
            "Epoch 98, Step 93, Loss: 0.0765131413936615\n",
            "Epoch 98, Step 94, Loss: 0.2031184434890747\n",
            "Epoch 98, Step 95, Loss: 0.060199182480573654\n",
            "Epoch 98, Step 96, Loss: 0.05399245396256447\n",
            "Epoch 98, Step 97, Loss: 0.11350982636213303\n",
            "Epoch 98, Step 98, Loss: 0.06186528503894806\n",
            "Epoch 98, Step 99, Loss: 0.06970495730638504\n",
            "Epoch 98, Step 100, Loss: 0.12974615395069122\n",
            "Epoch 98, Step 101, Loss: 0.10891443490982056\n",
            "Epoch 98, Step 102, Loss: 0.13199327886104584\n",
            "Epoch 98, Step 103, Loss: 0.13323310017585754\n",
            "Epoch 98, Step 104, Loss: 0.08488649129867554\n",
            "Epoch 98, Step 105, Loss: 0.06203746050596237\n",
            "Epoch 98, Step 106, Loss: 0.17516615986824036\n",
            "Epoch 98, Step 107, Loss: 0.20405985414981842\n",
            "Epoch 98, Step 108, Loss: 0.14473387598991394\n",
            "Epoch 98, Step 109, Loss: 0.11499996483325958\n",
            "Epoch 98, Step 110, Loss: 0.08515746891498566\n",
            "Epoch 98, Step 111, Loss: 0.09516265243291855\n",
            "Epoch 98, Step 112, Loss: 0.10416694730520248\n",
            "Epoch 98, Step 113, Loss: 0.13169991970062256\n",
            "Epoch 98, Step 114, Loss: 0.03918277472257614\n",
            "Epoch 98, Step 115, Loss: 0.08179245889186859\n",
            "Epoch 98, Step 116, Loss: 0.1624450832605362\n",
            "Epoch 98, Step 117, Loss: 0.07453198730945587\n",
            "Epoch 98, Step 118, Loss: 0.10168490558862686\n",
            "Epoch 98, Step 119, Loss: 0.1846352517604828\n",
            "Epoch 98, Step 120, Loss: 0.0614456944167614\n",
            "Epoch 98, Step 121, Loss: 0.13040074706077576\n",
            "Epoch 98, Step 122, Loss: 0.09442903101444244\n",
            "Epoch 98, Step 123, Loss: 0.08674546331167221\n",
            "Epoch 98, Step 124, Loss: 0.04524650052189827\n",
            "Epoch 98, Step 125, Loss: 0.05481701344251633\n",
            "Epoch 98, Step 126, Loss: 0.1679130643606186\n",
            "Epoch 98, Step 127, Loss: 0.0696711540222168\n",
            "Epoch 98, Step 128, Loss: 0.028668101876974106\n",
            "Epoch 98, Step 129, Loss: 0.10940557718276978\n",
            "Epoch 98, Step 130, Loss: 0.09386586397886276\n",
            "Epoch 98, Step 131, Loss: 0.06846071779727936\n",
            "Epoch 98, Step 132, Loss: 0.08522186428308487\n",
            "Epoch 98, Step 133, Loss: 0.10172713547945023\n",
            "Epoch 98, Step 134, Loss: 0.02329270727932453\n",
            "Epoch 98, Step 135, Loss: 0.102582186460495\n",
            "Epoch 98, Step 136, Loss: 0.06359303742647171\n",
            "Epoch 98, Step 137, Loss: 0.06128252297639847\n",
            "Epoch 98, Step 138, Loss: 0.14056283235549927\n",
            "Epoch 98, Step 139, Loss: 0.07332351803779602\n",
            "Epoch 98, Step 140, Loss: 0.0912286564707756\n",
            "Epoch 98, Step 141, Loss: 0.09275199472904205\n",
            "Epoch 98, Step 142, Loss: 0.036591991782188416\n",
            "Epoch 98, Step 143, Loss: 0.07661258429288864\n",
            "Epoch 98, Step 144, Loss: 0.11947499960660934\n",
            "Epoch 98, Step 145, Loss: 0.10349004715681076\n",
            "Epoch 98, Step 146, Loss: 0.0353839136660099\n",
            "Epoch 98, Step 147, Loss: 0.10979650169610977\n",
            "Epoch 98, Step 148, Loss: 0.13975179195404053\n",
            "Epoch 98, Step 149, Loss: 0.18753598630428314\n",
            "Epoch 98, Step 150, Loss: 0.049156684428453445\n",
            "Epoch 98, Step 151, Loss: 0.11322692036628723\n",
            "Epoch 98, Step 152, Loss: 0.08745033293962479\n",
            "Epoch 98, Step 153, Loss: 0.05397113040089607\n",
            "Epoch 98, Step 154, Loss: 0.1248854398727417\n",
            "Epoch 98, Step 155, Loss: 0.10111990571022034\n",
            "Epoch 98, Step 156, Loss: 0.07900037616491318\n",
            "Epoch 98, Step 157, Loss: 0.05768100172281265\n",
            "Epoch 98, Step 158, Loss: 0.1254708468914032\n",
            "Epoch 98, Step 159, Loss: 0.12891751527786255\n",
            "Epoch 98, Step 160, Loss: 0.043736159801483154\n",
            "Epoch 98, Step 161, Loss: 0.07946063578128815\n",
            "Epoch 98, Step 162, Loss: 0.10329096019268036\n",
            "Epoch 98, Step 163, Loss: 0.054744232445955276\n",
            "Epoch 98, Step 164, Loss: 0.13080358505249023\n",
            "Epoch 98, Step 165, Loss: 0.0960654467344284\n",
            "Epoch 98, Step 166, Loss: 0.08484496921300888\n",
            "Epoch 98, Step 167, Loss: 0.09334678947925568\n",
            "Epoch 98, Step 168, Loss: 0.08407733589410782\n",
            "Epoch 98, Step 169, Loss: 0.10310931503772736\n",
            "Epoch 98, Step 170, Loss: 0.09295964241027832\n",
            "Epoch 98, Step 171, Loss: 0.1029757410287857\n",
            "Epoch 98, Step 172, Loss: 0.07105779647827148\n",
            "Epoch 98, Step 173, Loss: 0.05434277653694153\n",
            "Epoch 98, Step 174, Loss: 0.05531180649995804\n",
            "Epoch 98, Step 175, Loss: 0.05668962001800537\n",
            "Epoch 98, Step 176, Loss: 0.08689182251691818\n",
            "Epoch 98, Step 177, Loss: 0.07973231375217438\n",
            "Epoch 98, Step 178, Loss: 0.14293219149112701\n",
            "Epoch 98, Step 179, Loss: 0.13137635588645935\n",
            "Epoch 98, Step 180, Loss: 0.0743449479341507\n",
            "Epoch 98, Step 181, Loss: 0.13196033239364624\n",
            "Epoch 98, Step 182, Loss: 0.05980890244245529\n",
            "Epoch 98, Step 183, Loss: 0.05904366448521614\n",
            "Epoch 98, Step 184, Loss: 0.14705997705459595\n",
            "Epoch 98, Step 185, Loss: 0.07672294974327087\n",
            "Epoch 98, Step 186, Loss: 0.060179367661476135\n",
            "Epoch 98, Step 187, Loss: 0.07276517152786255\n",
            "Epoch 98, Step 188, Loss: 0.13070206344127655\n",
            "Epoch 98, Step 189, Loss: 0.14158937335014343\n",
            "Epoch 98, Step 190, Loss: 0.2063223272562027\n",
            "Epoch 98, Step 191, Loss: 0.0882924348115921\n",
            "Epoch 98, Step 192, Loss: 0.05069783329963684\n",
            "Epoch 98, Step 193, Loss: 0.08243778347969055\n",
            "Epoch 98, Step 194, Loss: 0.16921742260456085\n",
            "Epoch 98, Step 195, Loss: 0.09154897928237915\n",
            "Epoch 98, Step 196, Loss: 0.06297319382429123\n",
            "Epoch 98, Step 197, Loss: 0.0991305559873581\n",
            "Epoch 98, Step 198, Loss: 0.08778579533100128\n",
            "Epoch 98, Step 199, Loss: 0.08757299929857254\n",
            "Epoch 98, Step 200, Loss: 0.08706136047840118\n",
            "Epoch 98, Step 201, Loss: 0.0856880471110344\n",
            "Epoch 98, Step 202, Loss: 0.08360795676708221\n",
            "Epoch 98, Step 203, Loss: 0.14545734226703644\n",
            "Epoch 98, Step 204, Loss: 0.1610151082277298\n",
            "Epoch 98, Step 205, Loss: 0.10578551888465881\n",
            "Epoch 98, Step 206, Loss: 0.08868525922298431\n",
            "Epoch 98, Step 207, Loss: 0.11923320591449738\n",
            "Epoch 98, Step 208, Loss: 0.06500260531902313\n",
            "Epoch 98, Step 209, Loss: 0.11942123621702194\n",
            "Epoch 98, Step 210, Loss: 0.06744858622550964\n",
            "Epoch 98, Step 211, Loss: 0.13834533095359802\n",
            "Epoch 98, Step 212, Loss: 0.10165318101644516\n",
            "Epoch 98, Step 213, Loss: 0.09351161867380142\n",
            "Epoch 98, Step 214, Loss: 0.03266456350684166\n",
            "Epoch 98, Step 215, Loss: 0.03818145766854286\n",
            "Epoch 98, Step 216, Loss: 0.12681593000888824\n",
            "Epoch 98, Step 217, Loss: 0.04234008863568306\n",
            "Epoch 98, Step 218, Loss: 0.17849497497081757\n",
            "Epoch 98, Step 219, Loss: 0.15740041434764862\n",
            "Epoch 98, Step 220, Loss: 0.13787297904491425\n",
            "Epoch 98, Step 221, Loss: 0.18765568733215332\n",
            "Epoch 98, Step 222, Loss: 0.10998940467834473\n",
            "Epoch 98, Step 223, Loss: 0.07847889512777328\n",
            "Epoch 98, Step 224, Loss: 0.13028736412525177\n",
            "Epoch 98, Step 225, Loss: 0.06165824830532074\n",
            "Epoch 98, Step 226, Loss: 0.1045791506767273\n",
            "Epoch 98, Step 227, Loss: 0.11259827762842178\n",
            "Epoch 98, Step 228, Loss: 0.15599048137664795\n",
            "Epoch 98, Step 229, Loss: 0.07603532820940018\n",
            "Epoch 98, Step 230, Loss: 0.1585768610239029\n",
            "Epoch 98, Step 231, Loss: 0.10040702670812607\n",
            "Epoch 98, Step 232, Loss: 0.10820962488651276\n",
            "Epoch 98, Step 233, Loss: 0.11388983577489853\n",
            "Epoch 98, Step 234, Loss: 0.09921912848949432\n",
            "Epoch 98, Step 235, Loss: 0.16244299709796906\n",
            "Epoch 98, Step 236, Loss: 0.04615270346403122\n",
            "Epoch 98, Step 237, Loss: 0.07213995605707169\n",
            "Epoch 98, Step 238, Loss: 0.22434166073799133\n",
            "Epoch 98, Step 239, Loss: 0.08885341882705688\n",
            "Epoch 98, Step 240, Loss: 0.16950766742229462\n",
            "Epoch 98, Step 241, Loss: 0.0750989019870758\n",
            "Epoch 98, Step 242, Loss: 0.0707862600684166\n",
            "Epoch 98, Step 243, Loss: 0.11357064545154572\n",
            "Epoch 98, Step 244, Loss: 0.10491063445806503\n",
            "Epoch 98, Step 245, Loss: 0.15450020134449005\n",
            "Epoch 98, Step 246, Loss: 0.08717451989650726\n",
            "Epoch 98, Step 247, Loss: 0.06904447823762894\n",
            "Epoch 98, Step 248, Loss: 0.16314783692359924\n",
            "Epoch 98, Step 249, Loss: 0.11765467375516891\n",
            "Epoch 98, Step 250, Loss: 0.027753934264183044\n",
            "Epoch 98, Step 251, Loss: 0.13188226521015167\n",
            "Epoch 98, Step 252, Loss: 0.1928354948759079\n",
            "Epoch 98, Step 253, Loss: 0.11333753168582916\n",
            "Epoch 98, Step 254, Loss: 0.11719908565282822\n",
            "Epoch 98, Step 255, Loss: 0.166870579123497\n",
            "Epoch 98, Step 256, Loss: 0.11049962788820267\n",
            "Epoch 98, Step 257, Loss: 0.06544768065214157\n",
            "Epoch 98, Step 258, Loss: 0.13009075820446014\n",
            "Epoch 98, Step 259, Loss: 0.053494151681661606\n",
            "Epoch 98, Step 260, Loss: 0.12824372947216034\n",
            "Epoch 98, Step 261, Loss: 0.09814305603504181\n",
            "Epoch 98, Step 262, Loss: 0.05730970948934555\n",
            "Epoch 98, Step 263, Loss: 0.07484079897403717\n",
            "Epoch 98, Step 264, Loss: 0.13397660851478577\n",
            "Epoch 98, Step 265, Loss: 0.12024544179439545\n",
            "Epoch 98, Step 266, Loss: 0.05342738702893257\n",
            "Epoch 98, Step 267, Loss: 0.080980084836483\n",
            "Epoch 98, Step 268, Loss: 0.2209203541278839\n",
            "Epoch 98, Step 269, Loss: 0.11531204730272293\n",
            "Epoch 98, Step 270, Loss: 0.10813508182764053\n",
            "Epoch 98, Step 271, Loss: 0.1218942329287529\n",
            "Epoch 98, Step 272, Loss: 0.08563082665205002\n",
            "Epoch 98, Step 273, Loss: 0.061296701431274414\n",
            "Epoch 98, Step 274, Loss: 0.05793450027704239\n",
            "Epoch 98, Step 275, Loss: 0.08755166083574295\n",
            "Epoch 98, Step 276, Loss: 0.09100090712308884\n",
            "Epoch 98, Step 277, Loss: 0.10973674058914185\n",
            "Epoch 98, Step 278, Loss: 0.09475409984588623\n",
            "Epoch 98, Step 279, Loss: 0.08338256180286407\n",
            "Epoch 98, Step 280, Loss: 0.07163110375404358\n",
            "Epoch 98, Step 281, Loss: 0.08307767659425735\n",
            "Epoch 98, Step 282, Loss: 0.08808557689189911\n",
            "Epoch 98, Step 283, Loss: 0.05398130789399147\n",
            "Epoch 98, Step 284, Loss: 0.09773413091897964\n",
            "Epoch 98, Step 285, Loss: 0.03703058883547783\n",
            "Epoch 98, Step 286, Loss: 0.1237780898809433\n",
            "Epoch 98, Step 287, Loss: 0.05356919765472412\n",
            "Epoch 98, Step 288, Loss: 0.2299961894750595\n",
            "Epoch 98, Step 289, Loss: 0.15747353434562683\n",
            "Epoch 98, Step 290, Loss: 0.07344582676887512\n",
            "Epoch 98, Step 291, Loss: 0.08387307822704315\n",
            "Epoch 98, Step 292, Loss: 0.06278850883245468\n",
            "Epoch 98, Step 293, Loss: 0.08799254149198532\n",
            "Epoch 98, Step 294, Loss: 0.11561639606952667\n",
            "Epoch 98, Step 295, Loss: 0.08772611618041992\n",
            "Epoch 98, Step 296, Loss: 0.08896064013242722\n",
            "Epoch 98, Step 297, Loss: 0.07049079239368439\n",
            "Epoch 98, Step 298, Loss: 0.12728439271450043\n",
            "Epoch 98, Step 299, Loss: 0.05525381118059158\n",
            "Epoch 98, Step 300, Loss: 0.12820732593536377\n",
            "Epoch 98, Step 301, Loss: 0.08293744921684265\n",
            "Epoch 98, Step 302, Loss: 0.09204020351171494\n",
            "Epoch 98, Step 303, Loss: 0.10340457409620285\n",
            "Epoch 98, Step 304, Loss: 0.1234205886721611\n",
            "Epoch 98, Step 305, Loss: 0.1028689295053482\n",
            "Epoch 98, Step 306, Loss: 0.09764938056468964\n",
            "Epoch 98, Step 307, Loss: 0.07992968708276749\n",
            "Epoch 98, Step 308, Loss: 0.06067107990384102\n",
            "Epoch 98, Step 309, Loss: 0.10990090668201447\n",
            "Epoch 98, Step 310, Loss: 0.12491673976182938\n",
            "Epoch 98, Step 311, Loss: 0.24579109251499176\n",
            "Epoch 98, Step 312, Loss: 0.22793732583522797\n",
            "Epoch 98 end, avg train loss: 0.09887320932726891\n",
            "Epoch 98 end, avg val loss: 0.3704033444563363, accuracy: 90.64%\n",
            "Epoch 99, Step 0, Loss: 0.09111295640468597\n",
            "Epoch 99, Step 1, Loss: 0.05798211693763733\n",
            "Epoch 99, Step 2, Loss: 0.02555512823164463\n",
            "Epoch 99, Step 3, Loss: 0.07413755357265472\n",
            "Epoch 99, Step 4, Loss: 0.17036135494709015\n",
            "Epoch 99, Step 5, Loss: 0.11563463509082794\n",
            "Epoch 99, Step 6, Loss: 0.14113813638687134\n",
            "Epoch 99, Step 7, Loss: 0.03112773783504963\n",
            "Epoch 99, Step 8, Loss: 0.060177747160196304\n",
            "Epoch 99, Step 9, Loss: 0.06594745814800262\n",
            "Epoch 99, Step 10, Loss: 0.05239034444093704\n",
            "Epoch 99, Step 11, Loss: 0.11771075427532196\n",
            "Epoch 99, Step 12, Loss: 0.056124527007341385\n",
            "Epoch 99, Step 13, Loss: 0.139764204621315\n",
            "Epoch 99, Step 14, Loss: 0.10796504467725754\n",
            "Epoch 99, Step 15, Loss: 0.02544032409787178\n",
            "Epoch 99, Step 16, Loss: 0.1112261414527893\n",
            "Epoch 99, Step 17, Loss: 0.04359467700123787\n",
            "Epoch 99, Step 18, Loss: 0.10037816315889359\n",
            "Epoch 99, Step 19, Loss: 0.08460472524166107\n",
            "Epoch 99, Step 20, Loss: 0.06360840797424316\n",
            "Epoch 99, Step 21, Loss: 0.15965153276920319\n",
            "Epoch 99, Step 22, Loss: 0.12385590374469757\n",
            "Epoch 99, Step 23, Loss: 0.08898410201072693\n",
            "Epoch 99, Step 24, Loss: 0.14218980073928833\n",
            "Epoch 99, Step 25, Loss: 0.09759996086359024\n",
            "Epoch 99, Step 26, Loss: 0.13218285143375397\n",
            "Epoch 99, Step 27, Loss: 0.06980709731578827\n",
            "Epoch 99, Step 28, Loss: 0.10047636181116104\n",
            "Epoch 99, Step 29, Loss: 0.039618488401174545\n",
            "Epoch 99, Step 30, Loss: 0.09367208182811737\n",
            "Epoch 99, Step 31, Loss: 0.04089163616299629\n",
            "Epoch 99, Step 32, Loss: 0.0796356052160263\n",
            "Epoch 99, Step 33, Loss: 0.15794622898101807\n",
            "Epoch 99, Step 34, Loss: 0.11807435005903244\n",
            "Epoch 99, Step 35, Loss: 0.05126166343688965\n",
            "Epoch 99, Step 36, Loss: 0.07676874846220016\n",
            "Epoch 99, Step 37, Loss: 0.08885984867811203\n",
            "Epoch 99, Step 38, Loss: 0.06316985934972763\n",
            "Epoch 99, Step 39, Loss: 0.0693921446800232\n",
            "Epoch 99, Step 40, Loss: 0.05756857246160507\n",
            "Epoch 99, Step 41, Loss: 0.06286400556564331\n",
            "Epoch 99, Step 42, Loss: 0.07473910599946976\n",
            "Epoch 99, Step 43, Loss: 0.13243505358695984\n",
            "Epoch 99, Step 44, Loss: 0.07910989969968796\n",
            "Epoch 99, Step 45, Loss: 0.05678027123212814\n",
            "Epoch 99, Step 46, Loss: 0.11723721027374268\n",
            "Epoch 99, Step 47, Loss: 0.07800035923719406\n",
            "Epoch 99, Step 48, Loss: 0.05908694863319397\n",
            "Epoch 99, Step 49, Loss: 0.11614543944597244\n",
            "Epoch 99, Step 50, Loss: 0.14165490865707397\n",
            "Epoch 99, Step 51, Loss: 0.1376170516014099\n",
            "Epoch 99, Step 52, Loss: 0.0805293619632721\n",
            "Epoch 99, Step 53, Loss: 0.08544878661632538\n",
            "Epoch 99, Step 54, Loss: 0.06528792530298233\n",
            "Epoch 99, Step 55, Loss: 0.1147499531507492\n",
            "Epoch 99, Step 56, Loss: 0.10552720725536346\n",
            "Epoch 99, Step 57, Loss: 0.06880762428045273\n",
            "Epoch 99, Step 58, Loss: 0.10812059044837952\n",
            "Epoch 99, Step 59, Loss: 0.1328442245721817\n",
            "Epoch 99, Step 60, Loss: 0.21800605952739716\n",
            "Epoch 99, Step 61, Loss: 0.12470462918281555\n",
            "Epoch 99, Step 62, Loss: 0.061720769852399826\n",
            "Epoch 99, Step 63, Loss: 0.0896073654294014\n",
            "Epoch 99, Step 64, Loss: 0.1502874791622162\n",
            "Epoch 99, Step 65, Loss: 0.12960776686668396\n",
            "Epoch 99, Step 66, Loss: 0.14368827641010284\n",
            "Epoch 99, Step 67, Loss: 0.13584724068641663\n",
            "Epoch 99, Step 68, Loss: 0.17146624624729156\n",
            "Epoch 99, Step 69, Loss: 0.02945920079946518\n",
            "Epoch 99, Step 70, Loss: 0.18142639100551605\n",
            "Epoch 99, Step 71, Loss: 0.09463176131248474\n",
            "Epoch 99, Step 72, Loss: 0.056197673082351685\n",
            "Epoch 99, Step 73, Loss: 0.07442360371351242\n",
            "Epoch 99, Step 74, Loss: 0.07562152296304703\n",
            "Epoch 99, Step 75, Loss: 0.08197839558124542\n",
            "Epoch 99, Step 76, Loss: 0.08393622934818268\n",
            "Epoch 99, Step 77, Loss: 0.12808601558208466\n",
            "Epoch 99, Step 78, Loss: 0.0838245078921318\n",
            "Epoch 99, Step 79, Loss: 0.09073904156684875\n",
            "Epoch 99, Step 80, Loss: 0.11676731705665588\n",
            "Epoch 99, Step 81, Loss: 0.14880084991455078\n",
            "Epoch 99, Step 82, Loss: 0.10464045405387878\n",
            "Epoch 99, Step 83, Loss: 0.09093217551708221\n",
            "Epoch 99, Step 84, Loss: 0.09082397073507309\n",
            "Epoch 99, Step 85, Loss: 0.03282461687922478\n",
            "Epoch 99, Step 86, Loss: 0.09979189187288284\n",
            "Epoch 99, Step 87, Loss: 0.12487298250198364\n",
            "Epoch 99, Step 88, Loss: 0.10824547708034515\n",
            "Epoch 99, Step 89, Loss: 0.08172958344221115\n",
            "Epoch 99, Step 90, Loss: 0.1456443816423416\n",
            "Epoch 99, Step 91, Loss: 0.06953143328428268\n",
            "Epoch 99, Step 92, Loss: 0.06278403103351593\n",
            "Epoch 99, Step 93, Loss: 0.10665331780910492\n",
            "Epoch 99, Step 94, Loss: 0.11934671550989151\n",
            "Epoch 99, Step 95, Loss: 0.20915435254573822\n",
            "Epoch 99, Step 96, Loss: 0.037661947309970856\n",
            "Epoch 99, Step 97, Loss: 0.06920845806598663\n",
            "Epoch 99, Step 98, Loss: 0.12821617722511292\n",
            "Epoch 99, Step 99, Loss: 0.08642864227294922\n",
            "Epoch 99, Step 100, Loss: 0.07812634855508804\n",
            "Epoch 99, Step 101, Loss: 0.20372013747692108\n",
            "Epoch 99, Step 102, Loss: 0.08939895778894424\n",
            "Epoch 99, Step 103, Loss: 0.05988427624106407\n",
            "Epoch 99, Step 104, Loss: 0.05671036243438721\n",
            "Epoch 99, Step 105, Loss: 0.04585918039083481\n",
            "Epoch 99, Step 106, Loss: 0.0615193247795105\n",
            "Epoch 99, Step 107, Loss: 0.11072372645139694\n",
            "Epoch 99, Step 108, Loss: 0.12008269131183624\n",
            "Epoch 99, Step 109, Loss: 0.14119753241539001\n",
            "Epoch 99, Step 110, Loss: 0.1392628252506256\n",
            "Epoch 99, Step 111, Loss: 0.13842126727104187\n",
            "Epoch 99, Step 112, Loss: 0.09286817908287048\n",
            "Epoch 99, Step 113, Loss: 0.04832394793629646\n",
            "Epoch 99, Step 114, Loss: 0.10439862310886383\n",
            "Epoch 99, Step 115, Loss: 0.07100272178649902\n",
            "Epoch 99, Step 116, Loss: 0.0364362932741642\n",
            "Epoch 99, Step 117, Loss: 0.14037230610847473\n",
            "Epoch 99, Step 118, Loss: 0.10404233634471893\n",
            "Epoch 99, Step 119, Loss: 0.11078712344169617\n",
            "Epoch 99, Step 120, Loss: 0.10368041694164276\n",
            "Epoch 99, Step 121, Loss: 0.09082765877246857\n",
            "Epoch 99, Step 122, Loss: 0.15748845040798187\n",
            "Epoch 99, Step 123, Loss: 0.09454777836799622\n",
            "Epoch 99, Step 124, Loss: 0.10554146021604538\n",
            "Epoch 99, Step 125, Loss: 0.14578701555728912\n",
            "Epoch 99, Step 126, Loss: 0.05520455539226532\n",
            "Epoch 99, Step 127, Loss: 0.060524407774209976\n",
            "Epoch 99, Step 128, Loss: 0.08046122640371323\n",
            "Epoch 99, Step 129, Loss: 0.11301429569721222\n",
            "Epoch 99, Step 130, Loss: 0.1603376567363739\n",
            "Epoch 99, Step 131, Loss: 0.06835897266864777\n",
            "Epoch 99, Step 132, Loss: 0.14026987552642822\n",
            "Epoch 99, Step 133, Loss: 0.09387873113155365\n",
            "Epoch 99, Step 134, Loss: 0.06384046375751495\n",
            "Epoch 99, Step 135, Loss: 0.12465859949588776\n",
            "Epoch 99, Step 136, Loss: 0.13371405005455017\n",
            "Epoch 99, Step 137, Loss: 0.05737099051475525\n",
            "Epoch 99, Step 138, Loss: 0.06462045758962631\n",
            "Epoch 99, Step 139, Loss: 0.029826080426573753\n",
            "Epoch 99, Step 140, Loss: 0.08532433956861496\n",
            "Epoch 99, Step 141, Loss: 0.13476808369159698\n",
            "Epoch 99, Step 142, Loss: 0.06056235730648041\n",
            "Epoch 99, Step 143, Loss: 0.07780227065086365\n",
            "Epoch 99, Step 144, Loss: 0.07780668139457703\n",
            "Epoch 99, Step 145, Loss: 0.14685222506523132\n",
            "Epoch 99, Step 146, Loss: 0.11715713888406754\n",
            "Epoch 99, Step 147, Loss: 0.0702878013253212\n",
            "Epoch 99, Step 148, Loss: 0.05120068043470383\n",
            "Epoch 99, Step 149, Loss: 0.08831319957971573\n",
            "Epoch 99, Step 150, Loss: 0.06289107352495193\n",
            "Epoch 99, Step 151, Loss: 0.07086151838302612\n",
            "Epoch 99, Step 152, Loss: 0.1443464457988739\n",
            "Epoch 99, Step 153, Loss: 0.10429324954748154\n",
            "Epoch 99, Step 154, Loss: 0.10672043263912201\n",
            "Epoch 99, Step 155, Loss: 0.1414330005645752\n",
            "Epoch 99, Step 156, Loss: 0.06963200122117996\n",
            "Epoch 99, Step 157, Loss: 0.06675653904676437\n",
            "Epoch 99, Step 158, Loss: 0.041867438703775406\n",
            "Epoch 99, Step 159, Loss: 0.07339002937078476\n",
            "Epoch 99, Step 160, Loss: 0.08327723294496536\n",
            "Epoch 99, Step 161, Loss: 0.06794481724500656\n",
            "Epoch 99, Step 162, Loss: 0.14074404537677765\n",
            "Epoch 99, Step 163, Loss: 0.07287686318159103\n",
            "Epoch 99, Step 164, Loss: 0.05437946692109108\n",
            "Epoch 99, Step 165, Loss: 0.09186558425426483\n",
            "Epoch 99, Step 166, Loss: 0.08443105965852737\n",
            "Epoch 99, Step 167, Loss: 0.08940672129392624\n",
            "Epoch 99, Step 168, Loss: 0.14326342940330505\n",
            "Epoch 99, Step 169, Loss: 0.18117283284664154\n",
            "Epoch 99, Step 170, Loss: 0.09462500363588333\n",
            "Epoch 99, Step 171, Loss: 0.06442369520664215\n",
            "Epoch 99, Step 172, Loss: 0.15085558593273163\n",
            "Epoch 99, Step 173, Loss: 0.05388954281806946\n",
            "Epoch 99, Step 174, Loss: 0.08050517737865448\n",
            "Epoch 99, Step 175, Loss: 0.11235164105892181\n",
            "Epoch 99, Step 176, Loss: 0.05903803929686546\n",
            "Epoch 99, Step 177, Loss: 0.1471901834011078\n",
            "Epoch 99, Step 178, Loss: 0.20963546633720398\n",
            "Epoch 99, Step 179, Loss: 0.10964132100343704\n",
            "Epoch 99, Step 180, Loss: 0.13821426033973694\n",
            "Epoch 99, Step 181, Loss: 0.08914785087108612\n",
            "Epoch 99, Step 182, Loss: 0.09462739527225494\n",
            "Epoch 99, Step 183, Loss: 0.1703553944826126\n",
            "Epoch 99, Step 184, Loss: 0.13778427243232727\n",
            "Epoch 99, Step 185, Loss: 0.13439390063285828\n",
            "Epoch 99, Step 186, Loss: 0.1174226701259613\n",
            "Epoch 99, Step 187, Loss: 0.09427628666162491\n",
            "Epoch 99, Step 188, Loss: 0.08353809267282486\n",
            "Epoch 99, Step 189, Loss: 0.13514482975006104\n",
            "Epoch 99, Step 190, Loss: 0.08890726417303085\n",
            "Epoch 99, Step 191, Loss: 0.1401415914297104\n",
            "Epoch 99, Step 192, Loss: 0.11218087375164032\n",
            "Epoch 99, Step 193, Loss: 0.1048661470413208\n",
            "Epoch 99, Step 194, Loss: 0.10333111882209778\n",
            "Epoch 99, Step 195, Loss: 0.13641516864299774\n",
            "Epoch 99, Step 196, Loss: 0.0902150496840477\n",
            "Epoch 99, Step 197, Loss: 0.15308260917663574\n",
            "Epoch 99, Step 198, Loss: 0.1363457590341568\n",
            "Epoch 99, Step 199, Loss: 0.13692083954811096\n",
            "Epoch 99, Step 200, Loss: 0.1335354596376419\n",
            "Epoch 99, Step 201, Loss: 0.12094777077436447\n",
            "Epoch 99, Step 202, Loss: 0.07808231562376022\n",
            "Epoch 99, Step 203, Loss: 0.1346532255411148\n",
            "Epoch 99, Step 204, Loss: 0.0568721741437912\n",
            "Epoch 99, Step 205, Loss: 0.2093801200389862\n",
            "Epoch 99, Step 206, Loss: 0.10320207476615906\n",
            "Epoch 99, Step 207, Loss: 0.10981275141239166\n",
            "Epoch 99, Step 208, Loss: 0.14648351073265076\n",
            "Epoch 99, Step 209, Loss: 0.09943263232707977\n",
            "Epoch 99, Step 210, Loss: 0.06895960122346878\n",
            "Epoch 99, Step 211, Loss: 0.03581498935818672\n",
            "Epoch 99, Step 212, Loss: 0.09651641547679901\n",
            "Epoch 99, Step 213, Loss: 0.1985177993774414\n",
            "Epoch 99, Step 214, Loss: 0.065013587474823\n",
            "Epoch 99, Step 215, Loss: 0.08708978444337845\n",
            "Epoch 99, Step 216, Loss: 0.10587777942419052\n",
            "Epoch 99, Step 217, Loss: 0.11219874024391174\n",
            "Epoch 99, Step 218, Loss: 0.08586867898702621\n",
            "Epoch 99, Step 219, Loss: 0.10054699331521988\n",
            "Epoch 99, Step 220, Loss: 0.13309843838214874\n",
            "Epoch 99, Step 221, Loss: 0.1409260630607605\n",
            "Epoch 99, Step 222, Loss: 0.038514576852321625\n",
            "Epoch 99, Step 223, Loss: 0.05894799903035164\n",
            "Epoch 99, Step 224, Loss: 0.12093699723482132\n",
            "Epoch 99, Step 225, Loss: 0.09689300507307053\n",
            "Epoch 99, Step 226, Loss: 0.06833930313587189\n",
            "Epoch 99, Step 227, Loss: 0.10922768712043762\n",
            "Epoch 99, Step 228, Loss: 0.042729947715997696\n",
            "Epoch 99, Step 229, Loss: 0.20559871196746826\n",
            "Epoch 99, Step 230, Loss: 0.08465593308210373\n",
            "Epoch 99, Step 231, Loss: 0.04554079473018646\n",
            "Epoch 99, Step 232, Loss: 0.15492552518844604\n",
            "Epoch 99, Step 233, Loss: 0.0710863471031189\n",
            "Epoch 99, Step 234, Loss: 0.10340894013643265\n",
            "Epoch 99, Step 235, Loss: 0.12566548585891724\n",
            "Epoch 99, Step 236, Loss: 0.07652342319488525\n",
            "Epoch 99, Step 237, Loss: 0.08075154572725296\n",
            "Epoch 99, Step 238, Loss: 0.12169190496206284\n",
            "Epoch 99, Step 239, Loss: 0.09008263051509857\n",
            "Epoch 99, Step 240, Loss: 0.08297724276781082\n",
            "Epoch 99, Step 241, Loss: 0.08699724823236465\n",
            "Epoch 99, Step 242, Loss: 0.14672400057315826\n",
            "Epoch 99, Step 243, Loss: 0.10437607020139694\n",
            "Epoch 99, Step 244, Loss: 0.07384663075208664\n",
            "Epoch 99, Step 245, Loss: 0.04793829843401909\n",
            "Epoch 99, Step 246, Loss: 0.09227847307920456\n",
            "Epoch 99, Step 247, Loss: 0.08761344105005264\n",
            "Epoch 99, Step 248, Loss: 0.09337768703699112\n",
            "Epoch 99, Step 249, Loss: 0.07718954235315323\n",
            "Epoch 99, Step 250, Loss: 0.034158363938331604\n",
            "Epoch 99, Step 251, Loss: 0.09455641359090805\n",
            "Epoch 99, Step 252, Loss: 0.15159907937049866\n",
            "Epoch 99, Step 253, Loss: 0.12456568330526352\n",
            "Epoch 99, Step 254, Loss: 0.03209380805492401\n",
            "Epoch 99, Step 255, Loss: 0.09807881712913513\n",
            "Epoch 99, Step 256, Loss: 0.07887619733810425\n",
            "Epoch 99, Step 257, Loss: 0.09285681694746017\n",
            "Epoch 99, Step 258, Loss: 0.10691969841718674\n",
            "Epoch 99, Step 259, Loss: 0.08538060635328293\n",
            "Epoch 99, Step 260, Loss: 0.03316335380077362\n",
            "Epoch 99, Step 261, Loss: 0.10496946424245834\n",
            "Epoch 99, Step 262, Loss: 0.07437845319509506\n",
            "Epoch 99, Step 263, Loss: 0.04237990826368332\n",
            "Epoch 99, Step 264, Loss: 0.09708920866250992\n",
            "Epoch 99, Step 265, Loss: 0.12076199054718018\n",
            "Epoch 99, Step 266, Loss: 0.13168159127235413\n",
            "Epoch 99, Step 267, Loss: 0.14479978382587433\n",
            "Epoch 99, Step 268, Loss: 0.051635514944791794\n",
            "Epoch 99, Step 269, Loss: 0.06439117342233658\n",
            "Epoch 99, Step 270, Loss: 0.09565569460391998\n",
            "Epoch 99, Step 271, Loss: 0.05439766123890877\n",
            "Epoch 99, Step 272, Loss: 0.04639764130115509\n",
            "Epoch 99, Step 273, Loss: 0.09603996574878693\n",
            "Epoch 99, Step 274, Loss: 0.14419543743133545\n",
            "Epoch 99, Step 275, Loss: 0.0543813593685627\n",
            "Epoch 99, Step 276, Loss: 0.09052915126085281\n",
            "Epoch 99, Step 277, Loss: 0.0814986601471901\n",
            "Epoch 99, Step 278, Loss: 0.10944324731826782\n",
            "Epoch 99, Step 279, Loss: 0.13138405978679657\n",
            "Epoch 99, Step 280, Loss: 0.13656975328922272\n",
            "Epoch 99, Step 281, Loss: 0.09695159643888474\n",
            "Epoch 99, Step 282, Loss: 0.06805498898029327\n",
            "Epoch 99, Step 283, Loss: 0.04832344502210617\n",
            "Epoch 99, Step 284, Loss: 0.08827802538871765\n",
            "Epoch 99, Step 285, Loss: 0.07092522084712982\n",
            "Epoch 99, Step 286, Loss: 0.08101212233304977\n",
            "Epoch 99, Step 287, Loss: 0.09535869210958481\n",
            "Epoch 99, Step 288, Loss: 0.07863879203796387\n",
            "Epoch 99, Step 289, Loss: 0.16167867183685303\n",
            "Epoch 99, Step 290, Loss: 0.11668954789638519\n",
            "Epoch 99, Step 291, Loss: 0.08481959998607635\n",
            "Epoch 99, Step 292, Loss: 0.05984685197472572\n",
            "Epoch 99, Step 293, Loss: 0.0786321833729744\n",
            "Epoch 99, Step 294, Loss: 0.07853453606367111\n",
            "Epoch 99, Step 295, Loss: 0.09745684266090393\n",
            "Epoch 99, Step 296, Loss: 0.09061577916145325\n",
            "Epoch 99, Step 297, Loss: 0.09020209312438965\n",
            "Epoch 99, Step 298, Loss: 0.05167971923947334\n",
            "Epoch 99, Step 299, Loss: 0.09854298084974289\n",
            "Epoch 99, Step 300, Loss: 0.13648217916488647\n",
            "Epoch 99, Step 301, Loss: 0.08532992005348206\n",
            "Epoch 99, Step 302, Loss: 0.06737437844276428\n",
            "Epoch 99, Step 303, Loss: 0.0746375322341919\n",
            "Epoch 99, Step 304, Loss: 0.04777221009135246\n",
            "Epoch 99, Step 305, Loss: 0.06920669972896576\n",
            "Epoch 99, Step 306, Loss: 0.12273110449314117\n",
            "Epoch 99, Step 307, Loss: 0.14792723953723907\n",
            "Epoch 99, Step 308, Loss: 0.0928640067577362\n",
            "Epoch 99, Step 309, Loss: 0.060081250965595245\n",
            "Epoch 99, Step 310, Loss: 0.11930824816226959\n",
            "Epoch 99, Step 311, Loss: 0.09104731678962708\n",
            "Epoch 99, Step 312, Loss: 0.11775370687246323\n",
            "Epoch 99 end, avg train loss: 0.09746753064374002\n",
            "Epoch 99 end, avg val loss: 0.376713761508677, accuracy: 90.66%\n",
            "Epoch 100, Step 0, Loss: 0.04796779155731201\n",
            "Epoch 100, Step 1, Loss: 0.04014832526445389\n",
            "Epoch 100, Step 2, Loss: 0.04962955415248871\n",
            "Epoch 100, Step 3, Loss: 0.03910459205508232\n",
            "Epoch 100, Step 4, Loss: 0.053871531039476395\n",
            "Epoch 100, Step 5, Loss: 0.05806232988834381\n",
            "Epoch 100, Step 6, Loss: 0.11726535856723785\n",
            "Epoch 100, Step 7, Loss: 0.08823666721582413\n",
            "Epoch 100, Step 8, Loss: 0.11727096885442734\n",
            "Epoch 100, Step 9, Loss: 0.030555522069334984\n",
            "Epoch 100, Step 10, Loss: 0.12374859303236008\n",
            "Epoch 100, Step 11, Loss: 0.13557250797748566\n",
            "Epoch 100, Step 12, Loss: 0.13349372148513794\n",
            "Epoch 100, Step 13, Loss: 0.11951877921819687\n",
            "Epoch 100, Step 14, Loss: 0.13396312296390533\n",
            "Epoch 100, Step 15, Loss: 0.07138486951589584\n",
            "Epoch 100, Step 16, Loss: 0.13677163422107697\n",
            "Epoch 100, Step 17, Loss: 0.09856684505939484\n",
            "Epoch 100, Step 18, Loss: 0.05158428102731705\n",
            "Epoch 100, Step 19, Loss: 0.029191797599196434\n",
            "Epoch 100, Step 20, Loss: 0.10669556260108948\n",
            "Epoch 100, Step 21, Loss: 0.05215141549706459\n",
            "Epoch 100, Step 22, Loss: 0.08888980746269226\n",
            "Epoch 100, Step 23, Loss: 0.11706169694662094\n",
            "Epoch 100, Step 24, Loss: 0.05439120903611183\n",
            "Epoch 100, Step 25, Loss: 0.06886987388134003\n",
            "Epoch 100, Step 26, Loss: 0.10288052260875702\n",
            "Epoch 100, Step 27, Loss: 0.11671820282936096\n",
            "Epoch 100, Step 28, Loss: 0.060840629041194916\n",
            "Epoch 100, Step 29, Loss: 0.12728601694107056\n",
            "Epoch 100, Step 30, Loss: 0.0680941566824913\n",
            "Epoch 100, Step 31, Loss: 0.16920918226242065\n",
            "Epoch 100, Step 32, Loss: 0.105997733771801\n",
            "Epoch 100, Step 33, Loss: 0.0696326419711113\n",
            "Epoch 100, Step 34, Loss: 0.12410895526409149\n",
            "Epoch 100, Step 35, Loss: 0.08793407678604126\n",
            "Epoch 100, Step 36, Loss: 0.07042702287435532\n",
            "Epoch 100, Step 37, Loss: 0.08020062744617462\n",
            "Epoch 100, Step 38, Loss: 0.15639708936214447\n",
            "Epoch 100, Step 39, Loss: 0.10028323531150818\n",
            "Epoch 100, Step 40, Loss: 0.06873587518930435\n",
            "Epoch 100, Step 41, Loss: 0.11303693056106567\n",
            "Epoch 100, Step 42, Loss: 0.09163299202919006\n",
            "Epoch 100, Step 43, Loss: 0.017757464200258255\n",
            "Epoch 100, Step 44, Loss: 0.1930796205997467\n",
            "Epoch 100, Step 45, Loss: 0.07182355225086212\n",
            "Epoch 100, Step 46, Loss: 0.03897201269865036\n",
            "Epoch 100, Step 47, Loss: 0.1467924267053604\n",
            "Epoch 100, Step 48, Loss: 0.07680706679821014\n",
            "Epoch 100, Step 49, Loss: 0.10404158383607864\n",
            "Epoch 100, Step 50, Loss: 0.11317283660173416\n",
            "Epoch 100, Step 51, Loss: 0.12404754757881165\n",
            "Epoch 100, Step 52, Loss: 0.10698030889034271\n",
            "Epoch 100, Step 53, Loss: 0.10372226685285568\n",
            "Epoch 100, Step 54, Loss: 0.07368394732475281\n",
            "Epoch 100, Step 55, Loss: 0.10379244387149811\n",
            "Epoch 100, Step 56, Loss: 0.05224216729402542\n",
            "Epoch 100, Step 57, Loss: 0.17016442120075226\n",
            "Epoch 100, Step 58, Loss: 0.15937626361846924\n",
            "Epoch 100, Step 59, Loss: 0.06691309809684753\n",
            "Epoch 100, Step 60, Loss: 0.051330216228961945\n",
            "Epoch 100, Step 61, Loss: 0.10368651896715164\n",
            "Epoch 100, Step 62, Loss: 0.12543566524982452\n",
            "Epoch 100, Step 63, Loss: 0.06460835784673691\n",
            "Epoch 100, Step 64, Loss: 0.1868583709001541\n",
            "Epoch 100, Step 65, Loss: 0.08389504253864288\n",
            "Epoch 100, Step 66, Loss: 0.076454758644104\n",
            "Epoch 100, Step 67, Loss: 0.05564039945602417\n",
            "Epoch 100, Step 68, Loss: 0.06782722473144531\n",
            "Epoch 100, Step 69, Loss: 0.10296181589365005\n",
            "Epoch 100, Step 70, Loss: 0.09410671889781952\n",
            "Epoch 100, Step 71, Loss: 0.06457970291376114\n",
            "Epoch 100, Step 72, Loss: 0.0610678493976593\n",
            "Epoch 100, Step 73, Loss: 0.05607359856367111\n",
            "Epoch 100, Step 74, Loss: 0.04965372011065483\n",
            "Epoch 100, Step 75, Loss: 0.14663457870483398\n",
            "Epoch 100, Step 76, Loss: 0.10351753234863281\n",
            "Epoch 100, Step 77, Loss: 0.05475366488099098\n",
            "Epoch 100, Step 78, Loss: 0.04263995215296745\n",
            "Epoch 100, Step 79, Loss: 0.06531036645174026\n",
            "Epoch 100, Step 80, Loss: 0.08943767100572586\n",
            "Epoch 100, Step 81, Loss: 0.08968412131071091\n",
            "Epoch 100, Step 82, Loss: 0.10873729735612869\n",
            "Epoch 100, Step 83, Loss: 0.06260404735803604\n",
            "Epoch 100, Step 84, Loss: 0.07250998169183731\n",
            "Epoch 100, Step 85, Loss: 0.12786328792572021\n",
            "Epoch 100, Step 86, Loss: 0.0877569168806076\n",
            "Epoch 100, Step 87, Loss: 0.1327609419822693\n",
            "Epoch 100, Step 88, Loss: 0.07152304798364639\n",
            "Epoch 100, Step 89, Loss: 0.06501476466655731\n",
            "Epoch 100, Step 90, Loss: 0.0682687908411026\n",
            "Epoch 100, Step 91, Loss: 0.098002590239048\n",
            "Epoch 100, Step 92, Loss: 0.10130854696035385\n",
            "Epoch 100, Step 93, Loss: 0.07197637856006622\n",
            "Epoch 100, Step 94, Loss: 0.03695852681994438\n",
            "Epoch 100, Step 95, Loss: 0.07751776278018951\n",
            "Epoch 100, Step 96, Loss: 0.07658296823501587\n",
            "Epoch 100, Step 97, Loss: 0.07007768750190735\n",
            "Epoch 100, Step 98, Loss: 0.07895385473966599\n",
            "Epoch 100, Step 99, Loss: 0.06875085085630417\n",
            "Epoch 100, Step 100, Loss: 0.09951411187648773\n",
            "Epoch 100, Step 101, Loss: 0.10075100511312485\n",
            "Epoch 100, Step 102, Loss: 0.10874757170677185\n",
            "Epoch 100, Step 103, Loss: 0.08622272312641144\n",
            "Epoch 100, Step 104, Loss: 0.06377283483743668\n",
            "Epoch 100, Step 105, Loss: 0.14100398123264313\n",
            "Epoch 100, Step 106, Loss: 0.08968961983919144\n",
            "Epoch 100, Step 107, Loss: 0.08029074966907501\n",
            "Epoch 100, Step 108, Loss: 0.06618257611989975\n",
            "Epoch 100, Step 109, Loss: 0.08945958316326141\n",
            "Epoch 100, Step 110, Loss: 0.10882905125617981\n",
            "Epoch 100, Step 111, Loss: 0.05760043114423752\n",
            "Epoch 100, Step 112, Loss: 0.17138728499412537\n",
            "Epoch 100, Step 113, Loss: 0.08471514284610748\n",
            "Epoch 100, Step 114, Loss: 0.1409761905670166\n",
            "Epoch 100, Step 115, Loss: 0.031247423961758614\n",
            "Epoch 100, Step 116, Loss: 0.0898006334900856\n",
            "Epoch 100, Step 117, Loss: 0.17099548876285553\n",
            "Epoch 100, Step 118, Loss: 0.053514864295721054\n",
            "Epoch 100, Step 119, Loss: 0.058415401726961136\n",
            "Epoch 100, Step 120, Loss: 0.07451328635215759\n",
            "Epoch 100, Step 121, Loss: 0.06607356667518616\n",
            "Epoch 100, Step 122, Loss: 0.06039663031697273\n",
            "Epoch 100, Step 123, Loss: 0.04547446221113205\n",
            "Epoch 100, Step 124, Loss: 0.0763762965798378\n",
            "Epoch 100, Step 125, Loss: 0.08936360478401184\n",
            "Epoch 100, Step 126, Loss: 0.07847399264574051\n",
            "Epoch 100, Step 127, Loss: 0.0679890364408493\n",
            "Epoch 100, Step 128, Loss: 0.06614992767572403\n",
            "Epoch 100, Step 129, Loss: 0.18475645780563354\n",
            "Epoch 100, Step 130, Loss: 0.11288518458604813\n",
            "Epoch 100, Step 131, Loss: 0.044735029339790344\n",
            "Epoch 100, Step 132, Loss: 0.09998990595340729\n",
            "Epoch 100, Step 133, Loss: 0.0661633163690567\n",
            "Epoch 100, Step 134, Loss: 0.11999326944351196\n",
            "Epoch 100, Step 135, Loss: 0.04409103840589523\n",
            "Epoch 100, Step 136, Loss: 0.05409473925828934\n",
            "Epoch 100, Step 137, Loss: 0.10109899938106537\n",
            "Epoch 100, Step 138, Loss: 0.09644443541765213\n",
            "Epoch 100, Step 139, Loss: 0.1479271948337555\n",
            "Epoch 100, Step 140, Loss: 0.08167446404695511\n",
            "Epoch 100, Step 141, Loss: 0.08855240046977997\n",
            "Epoch 100, Step 142, Loss: 0.041754983365535736\n",
            "Epoch 100, Step 143, Loss: 0.0400792732834816\n",
            "Epoch 100, Step 144, Loss: 0.07548799365758896\n",
            "Epoch 100, Step 145, Loss: 0.11274603009223938\n",
            "Epoch 100, Step 146, Loss: 0.07143077254295349\n",
            "Epoch 100, Step 147, Loss: 0.10867806524038315\n",
            "Epoch 100, Step 148, Loss: 0.09917255491018295\n",
            "Epoch 100, Step 149, Loss: 0.06134448200464249\n",
            "Epoch 100, Step 150, Loss: 0.09256266802549362\n",
            "Epoch 100, Step 151, Loss: 0.05215714871883392\n",
            "Epoch 100, Step 152, Loss: 0.06447353959083557\n",
            "Epoch 100, Step 153, Loss: 0.04157142713665962\n",
            "Epoch 100, Step 154, Loss: 0.08526928722858429\n",
            "Epoch 100, Step 155, Loss: 0.09919542819261551\n",
            "Epoch 100, Step 156, Loss: 0.09142852574586868\n",
            "Epoch 100, Step 157, Loss: 0.10303448140621185\n",
            "Epoch 100, Step 158, Loss: 0.11758328974246979\n",
            "Epoch 100, Step 159, Loss: 0.12401078641414642\n",
            "Epoch 100, Step 160, Loss: 0.18747468292713165\n",
            "Epoch 100, Step 161, Loss: 0.07699158787727356\n",
            "Epoch 100, Step 162, Loss: 0.09027174860239029\n",
            "Epoch 100, Step 163, Loss: 0.1368783563375473\n",
            "Epoch 100, Step 164, Loss: 0.07049842178821564\n",
            "Epoch 100, Step 165, Loss: 0.051108334213495255\n",
            "Epoch 100, Step 166, Loss: 0.07820507884025574\n",
            "Epoch 100, Step 167, Loss: 0.10615874081850052\n",
            "Epoch 100, Step 168, Loss: 0.053106870502233505\n",
            "Epoch 100, Step 169, Loss: 0.0747886672616005\n",
            "Epoch 100, Step 170, Loss: 0.07084858417510986\n",
            "Epoch 100, Step 171, Loss: 0.052742164582014084\n",
            "Epoch 100, Step 172, Loss: 0.07304008305072784\n",
            "Epoch 100, Step 173, Loss: 0.08527006208896637\n",
            "Epoch 100, Step 174, Loss: 0.08774327486753464\n",
            "Epoch 100, Step 175, Loss: 0.14591102302074432\n",
            "Epoch 100, Step 176, Loss: 0.06533002108335495\n",
            "Epoch 100, Step 177, Loss: 0.08605565875768661\n",
            "Epoch 100, Step 178, Loss: 0.0458822138607502\n",
            "Epoch 100, Step 179, Loss: 0.055917661637067795\n",
            "Epoch 100, Step 180, Loss: 0.164497971534729\n",
            "Epoch 100, Step 181, Loss: 0.12201058119535446\n",
            "Epoch 100, Step 182, Loss: 0.057870637625455856\n",
            "Epoch 100, Step 183, Loss: 0.07547656446695328\n",
            "Epoch 100, Step 184, Loss: 0.08475472778081894\n",
            "Epoch 100, Step 185, Loss: 0.06271739304065704\n",
            "Epoch 100, Step 186, Loss: 0.0840485543012619\n",
            "Epoch 100, Step 187, Loss: 0.081311896443367\n",
            "Epoch 100, Step 188, Loss: 0.08297006785869598\n",
            "Epoch 100, Step 189, Loss: 0.16029292345046997\n",
            "Epoch 100, Step 190, Loss: 0.09596490859985352\n",
            "Epoch 100, Step 191, Loss: 0.06329485028982162\n",
            "Epoch 100, Step 192, Loss: 0.05406515672802925\n",
            "Epoch 100, Step 193, Loss: 0.053663406521081924\n",
            "Epoch 100, Step 194, Loss: 0.041483763605356216\n",
            "Epoch 100, Step 195, Loss: 0.10565344244241714\n",
            "Epoch 100, Step 196, Loss: 0.12670281529426575\n",
            "Epoch 100, Step 197, Loss: 0.09543595463037491\n",
            "Epoch 100, Step 198, Loss: 0.15199412405490875\n",
            "Epoch 100, Step 199, Loss: 0.051148608326911926\n",
            "Epoch 100, Step 200, Loss: 0.06755948811769485\n",
            "Epoch 100, Step 201, Loss: 0.05845252424478531\n",
            "Epoch 100, Step 202, Loss: 0.12450046837329865\n",
            "Epoch 100, Step 203, Loss: 0.06463075429201126\n",
            "Epoch 100, Step 204, Loss: 0.08528772741556168\n",
            "Epoch 100, Step 205, Loss: 0.15403015911579132\n",
            "Epoch 100, Step 206, Loss: 0.052984390407800674\n",
            "Epoch 100, Step 207, Loss: 0.10677434504032135\n",
            "Epoch 100, Step 208, Loss: 0.06497007608413696\n",
            "Epoch 100, Step 209, Loss: 0.20091287791728973\n",
            "Epoch 100, Step 210, Loss: 0.09909457713365555\n",
            "Epoch 100, Step 211, Loss: 0.15856459736824036\n",
            "Epoch 100, Step 212, Loss: 0.0805780217051506\n",
            "Epoch 100, Step 213, Loss: 0.12179736793041229\n",
            "Epoch 100, Step 214, Loss: 0.2021525502204895\n",
            "Epoch 100, Step 215, Loss: 0.05967136472463608\n",
            "Epoch 100, Step 216, Loss: 0.11770142614841461\n",
            "Epoch 100, Step 217, Loss: 0.12218256294727325\n",
            "Epoch 100, Step 218, Loss: 0.053036730736494064\n",
            "Epoch 100, Step 219, Loss: 0.09758812189102173\n",
            "Epoch 100, Step 220, Loss: 0.1145230233669281\n",
            "Epoch 100, Step 221, Loss: 0.06300407648086548\n",
            "Epoch 100, Step 222, Loss: 0.07262525707483292\n",
            "Epoch 100, Step 223, Loss: 0.10856297612190247\n",
            "Epoch 100, Step 224, Loss: 0.06582473963499069\n",
            "Epoch 100, Step 225, Loss: 0.07824636995792389\n",
            "Epoch 100, Step 226, Loss: 0.09663504362106323\n",
            "Epoch 100, Step 227, Loss: 0.08512575924396515\n",
            "Epoch 100, Step 228, Loss: 0.10500176250934601\n",
            "Epoch 100, Step 229, Loss: 0.06587798148393631\n",
            "Epoch 100, Step 230, Loss: 0.08838722854852676\n",
            "Epoch 100, Step 231, Loss: 0.024340758100152016\n",
            "Epoch 100, Step 232, Loss: 0.03709924966096878\n",
            "Epoch 100, Step 233, Loss: 0.10370441526174545\n",
            "Epoch 100, Step 234, Loss: 0.0984266847372055\n",
            "Epoch 100, Step 235, Loss: 0.039329931139945984\n",
            "Epoch 100, Step 236, Loss: 0.11283295601606369\n",
            "Epoch 100, Step 237, Loss: 0.09747913479804993\n",
            "Epoch 100, Step 238, Loss: 0.12390710413455963\n",
            "Epoch 100, Step 239, Loss: 0.07997124642133713\n",
            "Epoch 100, Step 240, Loss: 0.14042484760284424\n",
            "Epoch 100, Step 241, Loss: 0.08105893433094025\n",
            "Epoch 100, Step 242, Loss: 0.1178407073020935\n",
            "Epoch 100, Step 243, Loss: 0.08700229972600937\n",
            "Epoch 100, Step 244, Loss: 0.03700952231884003\n",
            "Epoch 100, Step 245, Loss: 0.1278245896100998\n",
            "Epoch 100, Step 246, Loss: 0.07179956138134003\n",
            "Epoch 100, Step 247, Loss: 0.09845437854528427\n",
            "Epoch 100, Step 248, Loss: 0.05421986058354378\n",
            "Epoch 100, Step 249, Loss: 0.06832204014062881\n",
            "Epoch 100, Step 250, Loss: 0.13496513664722443\n",
            "Epoch 100, Step 251, Loss: 0.08652874827384949\n",
            "Epoch 100, Step 252, Loss: 0.08589261770248413\n",
            "Epoch 100, Step 253, Loss: 0.11570821702480316\n",
            "Epoch 100, Step 254, Loss: 0.12424501031637192\n",
            "Epoch 100, Step 255, Loss: 0.1372816562652588\n",
            "Epoch 100, Step 256, Loss: 0.13789725303649902\n",
            "Epoch 100, Step 257, Loss: 0.06694091856479645\n",
            "Epoch 100, Step 258, Loss: 0.04567337781190872\n",
            "Epoch 100, Step 259, Loss: 0.061680857092142105\n",
            "Epoch 100, Step 260, Loss: 0.11535502970218658\n",
            "Epoch 100, Step 261, Loss: 0.1479354202747345\n",
            "Epoch 100, Step 262, Loss: 0.15759392082691193\n",
            "Epoch 100, Step 263, Loss: 0.03706798702478409\n",
            "Epoch 100, Step 264, Loss: 0.1327180564403534\n",
            "Epoch 100, Step 265, Loss: 0.11414185911417007\n",
            "Epoch 100, Step 266, Loss: 0.09805542230606079\n",
            "Epoch 100, Step 267, Loss: 0.15798239409923553\n",
            "Epoch 100, Step 268, Loss: 0.12143650650978088\n",
            "Epoch 100, Step 269, Loss: 0.03685488551855087\n",
            "Epoch 100, Step 270, Loss: 0.08562216907739639\n",
            "Epoch 100, Step 271, Loss: 0.09228000789880753\n",
            "Epoch 100, Step 272, Loss: 0.1728992760181427\n",
            "Epoch 100, Step 273, Loss: 0.12071197479963303\n",
            "Epoch 100, Step 274, Loss: 0.05667334049940109\n",
            "Epoch 100, Step 275, Loss: 0.046378057450056076\n",
            "Epoch 100, Step 276, Loss: 0.09031771123409271\n",
            "Epoch 100, Step 277, Loss: 0.05381723865866661\n",
            "Epoch 100, Step 278, Loss: 0.05907938629388809\n",
            "Epoch 100, Step 279, Loss: 0.054913945496082306\n",
            "Epoch 100, Step 280, Loss: 0.11047705262899399\n",
            "Epoch 100, Step 281, Loss: 0.07488134503364563\n",
            "Epoch 100, Step 282, Loss: 0.09018560498952866\n",
            "Epoch 100, Step 283, Loss: 0.0853608027100563\n",
            "Epoch 100, Step 284, Loss: 0.06558958441019058\n",
            "Epoch 100, Step 285, Loss: 0.14644163846969604\n",
            "Epoch 100, Step 286, Loss: 0.1149255707859993\n",
            "Epoch 100, Step 287, Loss: 0.13214845955371857\n",
            "Epoch 100, Step 288, Loss: 0.0473322868347168\n",
            "Epoch 100, Step 289, Loss: 0.06569300591945648\n",
            "Epoch 100, Step 290, Loss: 0.061657682061195374\n",
            "Epoch 100, Step 291, Loss: 0.11114362627267838\n",
            "Epoch 100, Step 292, Loss: 0.1572021245956421\n",
            "Epoch 100, Step 293, Loss: 0.049131736159324646\n",
            "Epoch 100, Step 294, Loss: 0.08697731047868729\n",
            "Epoch 100, Step 295, Loss: 0.11275742948055267\n",
            "Epoch 100, Step 296, Loss: 0.057754337787628174\n",
            "Epoch 100, Step 297, Loss: 0.08624830096960068\n",
            "Epoch 100, Step 298, Loss: 0.04529943689703941\n",
            "Epoch 100, Step 299, Loss: 0.14566200971603394\n",
            "Epoch 100, Step 300, Loss: 0.1493600457906723\n",
            "Epoch 100, Step 301, Loss: 0.06513026356697083\n",
            "Epoch 100, Step 302, Loss: 0.10828939825296402\n",
            "Epoch 100, Step 303, Loss: 0.09864378720521927\n",
            "Epoch 100, Step 304, Loss: 0.08718512207269669\n",
            "Epoch 100, Step 305, Loss: 0.09549751877784729\n",
            "Epoch 100, Step 306, Loss: 0.036611780524253845\n",
            "Epoch 100, Step 307, Loss: 0.04807664826512337\n",
            "Epoch 100, Step 308, Loss: 0.03206182271242142\n",
            "Epoch 100, Step 309, Loss: 0.17250104248523712\n",
            "Epoch 100, Step 310, Loss: 0.13168667256832123\n",
            "Epoch 100, Step 311, Loss: 0.16411782801151276\n",
            "Epoch 100, Step 312, Loss: 0.13010761141777039\n",
            "Epoch 100 end, avg train loss: 0.09079263718745198\n",
            "Epoch 100 end, avg val loss: 0.37197706793417235, accuracy: 90.76%\n",
            "GPU memory allocated: 0.24 GB\n",
            "GPU memory reserved: 0.56 GB\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "adaIpsModel = ResNetWithDropout().to(device)\n",
        "adaIpsOptimizer = AdaIPS_S(adaIpsModel.parameters())\n",
        "train(adaIpsModel, adaIpsOptimizer, epochs=epochs)\n",
        "adaIpsModel = adaIpsModel.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHiNJ80x24Oz"
      },
      "outputs": [],
      "source": [
        "# adamModel = PretrainedResNet18().to(device)\n",
        "# adamOptimizer = torch.optim.Adam(adamModel.parameters(), lr=1e-3)\n",
        "# train(adamModel, adamOptimizer, epochs=epochs)\n",
        "# adamModel = adamModel.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mqweVqXS24Oz"
      },
      "outputs": [],
      "source": [
        "# ipsModel = PretrainedResNet18().to(device)\n",
        "# ipsOptimizer = IPS(ipsModel.parameters(), lower_bound=0, T=epochs * (len(trainset)/batch_size))\n",
        "# train(model=ipsModel, optimizer=ipsOptimizer, epochs=epochs)\n",
        "# ipsModel = ipsModel.cpu()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LZfWE11624O0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "cff44764-3ba3-42f8-e3b8-31c4b1584a63"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "\n",
        "models = [\n",
        "    (adaIpsModel, 'adaips optimizer'),\n",
        "    # (adamModel, 'adam optimizer'),\n",
        "    # (adaIpsModel_, 'adaips per param optimizer'),\n",
        "\n",
        "    # (ipsModel, 'ips optimizer')\n",
        "\n",
        "]\n",
        "exclude = []\n",
        "focus = [adaIpsModel]\n",
        "interval = 1\n",
        "for model, optimizer in models:\n",
        "    if model in exclude: continue\n",
        "    plt.plot(\n",
        "        [i for i, loss in enumerate(model.t_losses) if i % interval == 0],\n",
        "        [loss for i, loss in enumerate(model.t_losses) if i % interval == 0],\n",
        "        label=\"training loss\",\n",
        "        alpha=0.5 if model not in focus else 1\n",
        "    )\n",
        "    plt.plot(\n",
        "        [i for i, loss in enumerate(model.v_losses) if i % interval == 0],\n",
        "        [loss for i, loss in enumerate(model.v_losses) if i % interval == 0],\n",
        "        label=\"validation loss\",\n",
        "        alpha=0.5 if model not in focus else 1\n",
        "    )\n",
        "plt.title(\"Training loss CIFAR-10 dataset\")\n",
        "plt.xlabel('epochs')\n",
        "plt.ylabel('loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for model, opt in models:\n",
        "    torch.save(model, f\"{opt.split(' ')[0]}_cifar_vanilla.pth\")"
      ],
      "metadata": {
        "id": "suaIdJjR7O6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test(adaIpsModel, testloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PhmXgrpbI-KB",
        "outputId": "8664967c-c840-4306-b528-4df0e0fce1c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9061\n",
            "Precision: 0.9067\n",
            "Recall: 0.9061\n",
            "F1-score: 0.9059\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}